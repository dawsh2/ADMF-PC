{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Trading System Analysis\n",
    "\n",
    "This notebook provides complete analysis of the full trading system including signals, portfolio, and execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Signal generation analysis\n",
    "- Trade execution analysis\n",
    "- Portfolio performance metrics\n",
    "- Risk analysis\n",
    "- Execution cost analysis\n",
    "- Position and fill analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "\n",
    "# Analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_slippage = True\n",
    "analyze_intraday_patterns = True\n",
    "market_timezone = \"America/New_York\"\n",
    "\n",
    "# Performance thresholds\n",
    "min_sharpe_ratio = 1.0\n",
    "max_acceptable_drawdown = 0.20  # 20%\n",
    "min_win_rate = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Convert run_dir to Path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata and Traces"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load run metadata\nmetadata_path = run_dir / 'metadata.json'\nif metadata_path.exists():\n    with open(metadata_path, 'r') as f:\n        metadata = json.load(f)\n    \n    print(f\"✅ Run metadata loaded\")\n    print(f\"   Total bars: {metadata.get('total_bars', 'N/A')}\")\n    print(f\"   Total signals: {metadata.get('total_signals', 'N/A')}\")\n    print(f\"   Total orders: {metadata.get('total_orders', 'N/A')}\")\n    print(f\"   Total fills: {metadata.get('total_fills', 'N/A')}\")\n    print(f\"   Total positions: {metadata.get('total_positions', 'N/A')}\")\n    \n    # Get global traces path\n    global_traces_path = Path(metadata.get('global_traces_path', '/Users/daws/ADMF-PC/traces'))\n    print(f\"\\n📁 Global traces path: {global_traces_path}\")\nelse:\n    print(\"❌ No metadata.json found\")\n    metadata = {}\n    global_traces_path = Path('/Users/daws/ADMF-PC/traces')\n\n# Check what traces are available in global store\nstore_path = global_traces_path / 'store'\nhas_global_signals = store_path.exists() and any(store_path.glob('*.parquet'))\n\n# For backward compatibility, also check run directory\ntraces_dir = run_dir / 'traces'\nhas_local_signals = (traces_dir / 'signals').exists() if traces_dir.exists() else False\nhas_portfolio = (traces_dir / 'portfolio').exists() if traces_dir.exists() else False\nhas_execution = (traces_dir / 'execution').exists() if traces_dir.exists() else False\n\nprint(f\"\\n📊 Available traces:\")\nprint(f\"   Global signals: {'✅' if has_global_signals else '❌'}\")\nprint(f\"   Local signals: {'✅' if has_local_signals else '❌'}\")\nprint(f\"   Portfolio: {'✅' if has_portfolio else '❌'}\")\nprint(f\"   Execution: {'✅' if has_execution else '❌'}\")\n\n# Determine trace location\nuse_global_store = has_global_signals and not has_local_signals\nis_full_system = metadata.get('total_orders', 0) > 0 or has_portfolio or has_execution"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load market data\n",
    "market_data = None\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Try different possible locations\n",
    "        data_paths = [\n",
    "            run_dir / f'data/{symbol}_{timeframe}.csv',\n",
    "            run_dir / f'{symbol}_{timeframe}.csv',\n",
    "            run_dir.parent / f'data/{symbol}_{timeframe}.csv',\n",
    "            Path(f'/Users/daws/ADMF-PC/data/{symbol}_{timeframe}.csv')\n",
    "        ]\n",
    "        \n",
    "        for data_path in data_paths:\n",
    "            if data_path.exists():\n",
    "                market_data = pd.read_csv(data_path)\n",
    "                market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
    "                market_data = market_data.sort_values('timestamp')\n",
    "                \n",
    "                # Add derived fields\n",
    "                market_data['returns'] = market_data['close'].pct_change()\n",
    "                market_data['log_returns'] = np.log(market_data['close'] / market_data['close'].shift(1))\n",
    "                market_data['hour'] = market_data['timestamp'].dt.hour\n",
    "                market_data['minute'] = market_data['timestamp'].dt.minute\n",
    "                market_data['day_of_week'] = market_data['timestamp'].dt.dayofweek\n",
    "                \n",
    "                print(f\"✅ Loaded market data from: {data_path}\")\n",
    "                print(f\"   Date range: {market_data['timestamp'].min()} to {market_data['timestamp'].max()}\")\n",
    "                print(f\"   Total bars: {len(market_data)}\")\n",
    "                break\n",
    "        \n",
    "        if market_data is not None:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol}: {e}\")\n",
    "\n",
    "if market_data is None:\n",
    "    print(\"❌ Could not load market data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load and analyze signals if available\nif has_global_signals or has_local_signals:\n    print(\"\\n📊 SIGNAL ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Load strategy index\n    strategy_index_path = run_dir / 'strategy_index.parquet'\n    if strategy_index_path.exists():\n        strategy_index = pd.read_parquet(strategy_index_path)\n        print(f\"Loaded {len(strategy_index)} strategies from run index\")\n        \n        # Show strategy distribution\n        by_type = strategy_index['strategy_type'].value_counts()\n        print(\"\\nStrategies by type:\")\n        for stype, count in by_type.items():\n            print(f\"  {stype}: {count}\")\n    else:\n        print(\"No strategy index found in run directory\")\n        strategy_index = pd.DataFrame()\n    \n    # Analyze signal patterns from global store\n    if use_global_store and len(strategy_index) > 0:\n        print(\"\\n📊 Analyzing signals from global store...\")\n        \n        # Get trace paths from metadata components\n        signal_counts = []\n        components = metadata.get('components', {})\n        \n        for comp_name, comp_data in components.items():\n            if comp_data.get('type') == 'strategy' and 'trace_path' in comp_data:\n                trace_path = Path(comp_data['trace_path'])\n                if trace_path.exists():\n                    signals = pd.read_parquet(trace_path)\n                    \n                    # Count actual signal changes (non-zero values)\n                    signal_changes = signals[signals['val'] != 0]\n                    \n                    signal_counts.append({\n                        'strategy_type': comp_data.get('strategy_type'),\n                        'strategy_hash': comp_data.get('strategy_hash'),\n                        'total_signals': len(signals),\n                        'signal_changes': len(signal_changes),\n                        'long_signals': (signal_changes['val'] > 0).sum(),\n                        'short_signals': (signal_changes['val'] < 0).sum(),\n                        'signals_per_1000_bars': len(signal_changes) / (metadata.get('total_bars', 1000) / 1000)\n                    })\n                    \n                    # Show sample signals\n                    if len(signal_changes) > 0:\n                        print(f\"\\n  Strategy: {comp_data.get('strategy_type')} ({comp_data.get('strategy_hash', '')[:8]})\")\n                        print(f\"    Signal changes: {len(signal_changes)}\")\n                        print(f\"    First signal: {signal_changes.iloc[0]['ts']} -> {signal_changes.iloc[0]['val']}\")\n                        print(f\"    Last signal: {signal_changes.iloc[-1]['ts']} -> {signal_changes.iloc[-1]['val']}\")\n        \n        if signal_counts:\n            signal_df = pd.DataFrame(signal_counts)\n            print(\"\\n📊 Signal frequency analysis:\")\n            print(signal_df.to_string(index=False))\n            \n            # Check if signals were generated but no trades\n            if metadata.get('total_signals', 0) > 0 and metadata.get('total_orders', 0) == 0:\n                print(\"\\n⚠️ WARNING: Signals were generated but no orders were created!\")\n                print(\"Possible reasons:\")\n                print(\"  - Risk constraints (stop loss/take profit) may be too tight\")\n                print(\"  - Position sizing returned 0 shares\")\n                print(\"  - Intraday constraints prevented trades\")\n                print(\"  - Check the execution logs for more details\")\n        else:\n            print(\"\\n⚠️ No signal traces found in global store\")\n            \n    # Analyze from local traces (backward compatibility)\n    elif has_local_signals:\n        print(\"\\n📊 Analyzing signals from local traces...\")\n        # Original local trace analysis code here\nelse:\n    print(\"\\n⚠️ No signal traces available\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load and analyze portfolio data\nif is_full_system:\n    print(\"\\n💼 PORTFOLIO ANALYSIS\")\n    print(\"=\" * 80)\n    \n    trades_df = pd.DataFrame()\n    portfolio_traces_found = False\n    \n    # First check for unified trades file in global store\n    if use_global_store:\n        # Look for trades files (T{hash}.parquet pattern)\n        trades_files = list(store_path.glob('T*.parquet'))\n        \n        if trades_files:\n            print(f\"Found {len(trades_files)} trades file(s) in global store\")\n            \n            # Load the most recent trades file (they have timestamps in the hash)\n            # or load all and concatenate if multiple runs\n            all_trades = []\n            for trades_file in trades_files:\n                try:\n                    df = pd.read_parquet(trades_file)\n                    print(f\"✅ Loaded {len(df)} trades from {trades_file.name}\")\n                    all_trades.append(df)\n                except Exception as e:\n                    print(f\"Error loading {trades_file}: {e}\")\n            \n            if all_trades:\n                trades_df = pd.concat(all_trades, ignore_index=True)\n                \n                # Convert timestamp columns\n                for col in ['entry_time', 'exit_time', 'entry_order_time', 'exit_order_time', \n                           'entry_fill_time', 'exit_fill_time']:\n                    if col in trades_df.columns:\n                        trades_df[col] = pd.to_datetime(trades_df[col])\n                \n                # CRITICAL: Convert price columns to numeric to handle string prices\n                price_columns = ['entry_fill_price', 'exit_fill_price', 'entry_order_price', 'exit_order_price']\n                for col in price_columns:\n                    if col in trades_df.columns:\n                        trades_df[col] = pd.to_numeric(trades_df[col], errors='coerce')\n                \n                portfolio_traces_found = True\n                print(f\"✅ Total trades loaded: {len(trades_df)}\")\n                \n                # DEBUG: Examine trades data structure\n                print(\"\\n🔍 DEBUG: Trades DataFrame Analysis\")\n                print(f\"Shape: {trades_df.shape}\")\n                print(f\"\\nColumns: {list(trades_df.columns)}\")\n                \n                # Check price fields\n                print(f\"\\n📊 Price field analysis:\")\n                print(f\"entry_fill_price non-null: {trades_df['entry_fill_price'].notna().sum()} / {len(trades_df)}\")\n                print(f\"exit_fill_price non-null: {trades_df['exit_fill_price'].notna().sum()} / {len(trades_df)}\")\n                print(f\"entry_order_price non-null: {trades_df['entry_order_price'].notna().sum()} / {len(trades_df)}\")\n                print(f\"exit_order_price non-null: {trades_df['exit_order_price'].notna().sum()} / {len(trades_df)}\")\n                \n                # Show unique values\n                print(f\"\\nentry_fill_price unique values (first 5): {trades_df['entry_fill_price'].unique()[:5]}\")\n                print(f\"exit_fill_price unique values (first 5): {trades_df['exit_fill_price'].unique()[:5]}\")\n                \n                # Show a sample trade with all fields\n                if len(trades_df) > 0:\n                    print(f\"\\n📋 Sample trade (first row):\")\n                    sample = trades_df.iloc[0]\n                    for col in trades_df.columns:\n                        print(f\"  {col}: {sample[col]}\")\n                \n                # Check for trades with complete price data\n                complete_price_trades = trades_df[\n                    trades_df['entry_fill_price'].notna() & \n                    trades_df['exit_fill_price'].notna()\n                ]\n                print(f\"\\n✅ Trades with complete price data: {len(complete_price_trades)} / {len(trades_df)}\")\n                \n                # Show trade statistics\n                if len(trades_df) > 0:\n                    print(\"\\n📊 Trade Statistics:\")\n                    print(f\"  Total trades: {len(trades_df)}\")\n                    print(f\"  Unique strategies: {trades_df['strategy_id'].nunique() if 'strategy_id' in trades_df else 'N/A'}\")\n                    \n                    if 'pnl' in trades_df:\n                        winning_trades = trades_df[trades_df['pnl'] > 0]\n                        print(f\"  Win rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n                        print(f\"  Average PnL: ${trades_df['pnl'].mean():.2f}\")\n                        print(f\"  Total PnL: ${trades_df['pnl'].sum():.2f}\")\n                    \n                    if 'duration_bars' in trades_df:\n                        print(f\"  Average duration: {trades_df['duration_bars'].mean():.1f} bars\")\n                    \n                    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n                        # Calculate returns from fill prices ONLY for trades with valid numeric prices\n                        valid_trades = trades_df[\n                            trades_df['entry_fill_price'].notna() & \n                            trades_df['exit_fill_price'].notna() &\n                            (trades_df['entry_fill_price'] > 0)\n                        ].copy()\n                        \n                        if len(valid_trades) > 0:\n                            valid_trades['return'] = (valid_trades['exit_fill_price'] - valid_trades['entry_fill_price']) / valid_trades['entry_fill_price']\n                            # Adjust for short trades\n                            short_mask = valid_trades['direction'] == 'short'\n                            valid_trades.loc[short_mask, 'return'] = -valid_trades.loc[short_mask, 'return']\n                            print(f\"  Average return: {valid_trades['return'].mean()*100:.3f}%\")\n                            \n                            # Copy return column back to main dataframe\n                            trades_df['return'] = np.nan\n                            trades_df.loc[valid_trades.index, 'return'] = valid_trades['return']\n                    \n                    # Show sample trades\n                    print(\"\\n📋 Sample trades:\")\n                    display_cols = ['trade_id', 'symbol', 'direction', 'entry_time', 'exit_time', \n                                   'entry_fill_price', 'exit_fill_price', 'pnl']\n                    display_cols = [col for col in display_cols if col in trades_df.columns]\n                    if display_cols:\n                        print(trades_df[display_cols].head(5).to_string(index=False))\n    \n    # Fallback to old format if no unified trades file\n    if not portfolio_traces_found:\n        print(\"\\n⚠️ No unified trades file found, checking for legacy portfolio traces...\")\n        \n        # Check both global and local for individual order/position files\n        orders = pd.DataFrame()\n        positions_opened = pd.DataFrame()\n        positions_closed = pd.DataFrame()\n        \n        # [Previous code for loading individual order/position files remains as fallback]\n        # ... (keeping the existing fallback code)\n        \n    # If still no trades found\n    if len(trades_df) == 0:\n        print(\"\\n⚠️ No trades found. Possible reasons:\")\n        print(\"  - Signals didn't trigger any trades due to risk constraints\")\n        print(\"  - Position sizing returned 0 shares\")\n        print(\"  - Intraday constraints prevented trades\")\n        print(\"  - Check console output for execution warnings\")\nelse:\n    print(\"\\n⚠️ No portfolio data to analyze (metadata shows 0 orders/fills/positions)\")\n    trades_df = pd.DataFrame()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load and analyze execution data\nif is_full_system and len(trades_df) > 0:\n    print(\"\\n⚡ EXECUTION ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Extract execution metrics from trades dataframe\n    print(\"📊 Analyzing execution quality from trades data...\")\n    \n    # Fill statistics\n    total_fills = len(trades_df) * 2  # Entry and exit for each trade\n    print(f\"\\nFill Statistics:\")\n    print(f\"  Total fills: {total_fills} (entry + exit fills)\")\n    \n    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n        # Filter out non-numeric values\n        entry_prices = trades_df['entry_fill_price'].dropna()\n        exit_prices = trades_df['exit_fill_price'].dropna()\n        \n        if len(entry_prices) > 0 and len(exit_prices) > 0:\n            all_fill_prices = pd.concat([entry_prices, exit_prices])\n            print(f\"  Average fill price: ${all_fill_prices.mean():.2f}\")\n            print(f\"  Fill price range: ${all_fill_prices.min():.2f} - ${all_fill_prices.max():.2f}\")\n    \n    # Slippage analysis\n    if analyze_slippage:\n        print(\"\\n💸 Slippage Analysis:\")\n        \n        # Entry slippage\n        if 'slippage_entry' in trades_df:\n            # Convert to numeric if needed\n            trades_df['slippage_entry'] = pd.to_numeric(trades_df['slippage_entry'], errors='coerce')\n            valid_slippage = trades_df[trades_df['slippage_entry'].notna() & trades_df['entry_fill_price'].notna()]\n            if len(valid_slippage) > 0:\n                entry_slippage_bps = valid_slippage['slippage_entry'] / valid_slippage['entry_fill_price'] * 10000\n                print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n        elif 'entry_order_price' in trades_df and 'entry_fill_price' in trades_df:\n            # Calculate if not pre-computed (only for valid numeric data)\n            valid_entry = trades_df[\n                trades_df['entry_order_price'].notna() & \n                trades_df['entry_fill_price'].notna() &\n                (trades_df['entry_order_price'] > 0) &\n                (trades_df['entry_fill_price'] > 0)\n            ].copy()\n            \n            if len(valid_entry) > 0:\n                valid_entry['slippage_entry'] = abs(valid_entry['entry_fill_price'] - valid_entry['entry_order_price'])\n                entry_slippage_bps = valid_entry['slippage_entry'] / valid_entry['entry_fill_price'] * 10000\n                print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n        \n        # Exit slippage\n        if 'slippage_exit' in trades_df:\n            # Convert to numeric if needed\n            trades_df['slippage_exit'] = pd.to_numeric(trades_df['slippage_exit'], errors='coerce')\n            valid_slippage = trades_df[trades_df['slippage_exit'].notna() & trades_df['exit_fill_price'].notna()]\n            if len(valid_slippage) > 0:\n                exit_slippage_bps = valid_slippage['slippage_exit'] / valid_slippage['exit_fill_price'] * 10000\n                print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n        elif 'exit_order_price' in trades_df and 'exit_fill_price' in trades_df:\n            # Calculate if not pre-computed (only for valid numeric data)\n            valid_exit = trades_df[\n                trades_df['exit_order_price'].notna() & \n                trades_df['exit_fill_price'].notna() &\n                (trades_df['exit_order_price'] > 0) &\n                (trades_df['exit_fill_price'] > 0)\n            ].copy()\n            \n            if len(valid_exit) > 0:\n                valid_exit['slippage_exit'] = abs(valid_exit['exit_fill_price'] - valid_exit['exit_order_price'])\n                exit_slippage_bps = valid_exit['slippage_exit'] / valid_exit['exit_fill_price'] * 10000\n                print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n        \n        # Total slippage cost\n        if 'slippage_entry' in trades_df and 'slippage_exit' in trades_df:\n            # Ensure numeric\n            trades_df['slippage_entry'] = pd.to_numeric(trades_df['slippage_entry'], errors='coerce')\n            trades_df['slippage_exit'] = pd.to_numeric(trades_df['slippage_exit'], errors='coerce')\n            \n            total_slippage_cost = (\n                trades_df['slippage_entry'].fillna(0) + \n                trades_df['slippage_exit'].fillna(0)\n            ).sum()\n            print(f\"  Total slippage cost: ${total_slippage_cost:.2f}\")\n    \n    # Execution timing analysis\n    print(\"\\n⏱️ Execution Timing:\")\n    if 'entry_order_time' in trades_df and 'entry_fill_time' in trades_df:\n        try:\n            # Ensure both timestamps are timezone-naive or both are timezone-aware\n            entry_order_times = pd.to_datetime(trades_df['entry_order_time']).dt.tz_localize(None)\n            entry_fill_times = pd.to_datetime(trades_df['entry_fill_time']).dt.tz_localize(None)\n            \n            # Calculate time to fill\n            time_diff = entry_fill_times - entry_order_times\n            valid_times = time_diff.dt.total_seconds().dropna()\n            \n            if len(valid_times) > 0:\n                print(f\"  Entry order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n        except Exception as e:\n            print(f\"  Entry timing calculation failed: {e}\")\n    \n    if 'exit_order_time' in trades_df and 'exit_fill_time' in trades_df:\n        try:\n            # Ensure both timestamps are timezone-naive or both are timezone-aware\n            exit_order_times = pd.to_datetime(trades_df['exit_order_time']).dt.tz_localize(None)\n            exit_fill_times = pd.to_datetime(trades_df['exit_fill_time']).dt.tz_localize(None)\n            \n            # Calculate time to fill\n            time_diff = exit_fill_times - exit_order_times\n            valid_times = time_diff.dt.total_seconds().dropna()\n            \n            if len(valid_times) > 0:\n                print(f\"  Exit order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n        except Exception as e:\n            print(f\"  Exit timing calculation failed: {e}\")\n    \n    # Commission analysis\n    if 'commission' in trades_df:\n        # Ensure numeric\n        trades_df['commission'] = pd.to_numeric(trades_df['commission'], errors='coerce')\n        print(\"\\n💰 Execution Costs:\")\n        print(f\"  Total commissions: ${trades_df['commission'].sum():.2f}\")\n        print(f\"  Average commission per trade: ${trades_df['commission'].mean():.2f}\")\n        \n        if 'pnl' in trades_df and trades_df['pnl'].sum() != 0:\n            print(f\"  Commission as % of PnL: {abs(trades_df['commission'].sum() / trades_df['pnl'].sum() * 100):.2f}%\")\n    \n    # Exit reason analysis\n    if 'exit_reason' in trades_df:\n        print(\"\\n🎯 Exit Reasons:\")\n        exit_reasons = trades_df['exit_reason'].value_counts()\n        for reason, count in exit_reasons.items():\n            print(f\"  {reason}: {count} ({count/len(trades_df)*100:.1f}%)\")\n            \nelif is_full_system:\n    print(\"\\n⚠️ No execution data to analyze (no trades found)\")\nelse:\n    print(\"\\n⚠️ Skipping execution analysis (no trades executed)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate overall performance metrics if we have trades\nif len(trades_df) > 0:\n    print(\"\\n📈 PERFORMANCE METRICS\")\n    print(\"=\" * 80)\n    \n    # Use PnL data if available\n    if 'pnl' in trades_df:\n        # Calculate equity curve from trades\n        initial_capital = 100000  # Assumed\n        trades_df = trades_df.sort_values('exit_time')\n        trades_df['cum_pnl'] = trades_df['pnl'].cumsum()\n        trades_df['equity'] = initial_capital + trades_df['cum_pnl']\n        \n        # Create equity curve dataframe\n        equity_curve = []\n        equity_curve.append({'timestamp': trades_df['entry_time'].min(), 'equity': initial_capital})\n        for _, trade in trades_df.iterrows():\n            equity_curve.append({'timestamp': trade['exit_time'], 'equity': trade['equity']})\n        \n        equity_df = pd.DataFrame(equity_curve)\n        equity_df['returns'] = equity_df['equity'].pct_change()\n        \n        # Performance metrics\n        total_return = (equity_df['equity'].iloc[-1] / initial_capital - 1)\n        \n        # Sharpe ratio (assuming daily returns)\n        if 'exit_time' in trades_df and len(trades_df) > 1:\n            daily_returns = trades_df.groupby(trades_df['exit_time'].dt.date)['pnl'].sum() / initial_capital\n            if len(daily_returns) > 1 and daily_returns.std() > 0:\n                sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n            else:\n                sharpe_ratio = 0\n        else:\n            sharpe_ratio = 0\n        \n        # Max drawdown\n        cummax = equity_df['equity'].expanding().max()\n        drawdown = (equity_df['equity'] / cummax - 1)\n        max_drawdown = drawdown.min()\n        \n        # Win/loss statistics\n        winning_trades = trades_df[trades_df['pnl'] > 0]\n        losing_trades = trades_df[trades_df['pnl'] <= 0]\n        \n        print(f\"Total Return: {total_return*100:.2f}%\")\n        print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n        print(f\"Max Drawdown: {max_drawdown*100:.2f}%\")\n        print(f\"\\nTrade Statistics:\")\n        print(f\"  Total Trades: {len(trades_df)}\")\n        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n        print(f\"  Average Win: ${winning_trades['pnl'].mean():.2f}\" if len(winning_trades) > 0 else \"  Average Win: N/A\")\n        print(f\"  Average Loss: ${losing_trades['pnl'].mean():.2f}\" if len(losing_trades) > 0 else \"  Average Loss: N/A\")\n        \n        if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0:\n            profit_factor = winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())\n            print(f\"  Profit Factor: {profit_factor:.2f}\")\n        else:\n            print(f\"  Profit Factor: N/A\")\n        \n        # Performance vs thresholds\n        print(f\"\\n🎯 Performance vs Thresholds:\")\n        print(f\"  Sharpe Ratio: {sharpe_ratio:.2f} {'✅' if sharpe_ratio >= min_sharpe_ratio else '❌'} (min: {min_sharpe_ratio})\")\n        print(f\"  Max Drawdown: {abs(max_drawdown)*100:.1f}% {'✅' if abs(max_drawdown) <= max_acceptable_drawdown else '❌'} (max: {max_acceptable_drawdown*100:.0f}%)\")\n        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}% {'✅' if len(winning_trades)/len(trades_df) >= min_win_rate else '❌'} (min: {min_win_rate*100:.0f}%)\")\n        \n        # Plot equity curve\n        plt.figure(figsize=(12, 6))\n        plt.plot(equity_df['timestamp'], equity_df['equity'])\n        plt.title('Portfolio Equity Curve')\n        plt.xlabel('Date')\n        plt.ylabel('Equity ($)')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n        \n        # Plot drawdown\n        plt.figure(figsize=(12, 4))\n        plt.fill_between(equity_df['timestamp'], drawdown * 100, 0, alpha=0.3, color='red')\n        plt.title('Portfolio Drawdown')\n        plt.xlabel('Date')\n        plt.ylabel('Drawdown (%)')\n        plt.grid(True, alpha=0.3)\n        plt.show()\n    else:\n        print(\"⚠️ No PnL data available in trades\")\nelse:\n    print(\"\\n⚠️ No trades available for performance analysis\")\n    print(\"\\nPossible reasons why no trades were executed:\")\n    print(\"1. Risk parameters (stop loss: 0.075%, take profit: 0.1%) may be too tight\")\n    print(\"2. Position sizing returned 0 shares\")\n    print(\"3. Intraday constraints prevented trades\") \n    print(\"4. Signals were generated but didn't meet execution criteria\")\n    print(\"\\nCheck the signal analysis section above to confirm signals were generated.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intraday Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze intraday patterns if requested\n",
    "if analyze_intraday_patterns and len(trades_df) > 0:\n",
    "    print(\"\\n⏰ INTRADAY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract hour of entry and exit\n",
    "    trades_df['entry_hour'] = trades_df['entry_time'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time'].dt.hour\n",
    "    trades_df['entry_day'] = trades_df['entry_time'].dt.dayofweek\n",
    "    \n",
    "    # Performance by hour of day\n",
    "    hourly_performance = trades_df.groupby('entry_hour').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Win rate by hour\n",
    "    hourly_win_rate = trades_df.groupby('entry_hour').apply(\n",
    "        lambda x: (x['pnl'] > 0).mean() * 100\n",
    "    )\n",
    "    \n",
    "    # Performance by day of week\n",
    "    daily_performance = trades_df.groupby('entry_day').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Trades by hour\n",
    "    ax = axes[0, 0]\n",
    "    hourly_performance['pnl']['count'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Number of Trades by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Trade Count')\n",
    "    \n",
    "    # Win rate by hour\n",
    "    ax = axes[0, 1]\n",
    "    hourly_win_rate.plot(kind='bar', ax=ax, color='green')\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Win Rate by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Win Rate (%)')\n",
    "    \n",
    "    # Average PnL by hour\n",
    "    ax = axes[1, 0]\n",
    "    hourly_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='blue')\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    # Performance by day of week\n",
    "    ax = axes[1, 1]\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    daily_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='purple')\n",
    "    ax.set_xticklabels(days[:len(daily_performance)], rotation=0)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Day of Week')\n",
    "    ax.set_xlabel('Day of Week')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best and worst times\n",
    "    print(\"\\n🕐 Best Trading Hours:\")\n",
    "    best_hours = hourly_performance['pnl']['mean'].nlargest(3)\n",
    "    for hour, avg_pnl in best_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")\n",
    "    \n",
    "    print(\"\\n🕐 Worst Trading Hours:\")\n",
    "    worst_hours = hourly_performance['pnl']['mean'].nsmallest(3)\n",
    "    for hour, avg_pnl in worst_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Comprehensive risk analysis\nif len(trades_df) > 0:\n    print(\"\\n⚠️ RISK ANALYSIS\")\n    print(\"=\" * 80)\n    \n    # Trade duration analysis\n    if 'duration' in trades_df.columns:\n        print(\"Trade Duration Statistics:\")\n        print(f\"  Average: {trades_df['duration'].mean():.1f} minutes\")\n        print(f\"  Median: {trades_df['duration'].median():.1f} minutes\")\n        print(f\"  Shortest: {trades_df['duration'].min():.1f} minutes\")\n        print(f\"  Longest: {trades_df['duration'].max():.1f} minutes\")\n    elif 'duration_bars' in trades_df.columns:\n        print(\"Trade Duration Statistics (in bars):\")\n        print(f\"  Average: {trades_df['duration_bars'].mean():.1f} bars\")\n        print(f\"  Median: {trades_df['duration_bars'].median():.1f} bars\")\n        print(f\"  Shortest: {trades_df['duration_bars'].min():.0f} bars\")\n        print(f\"  Longest: {trades_df['duration_bars'].max():.0f} bars\")\n    elif 'duration_time' in trades_df.columns:\n        print(\"Trade Duration Statistics:\")\n        print(f\"  Duration times available in 'duration_time' column\")\n    \n    # Consecutive wins/losses\n    if 'pnl' in trades_df.columns:\n        trades_df['is_win'] = trades_df['pnl'] > 0\n        trades_df['streak'] = (trades_df['is_win'] != trades_df['is_win'].shift()).cumsum()\n        \n        win_streaks = trades_df[trades_df['is_win']].groupby('streak').size()\n        loss_streaks = trades_df[~trades_df['is_win']].groupby('streak').size()\n        \n        print(f\"\\nStreak Analysis:\")\n        print(f\"  Max consecutive wins: {win_streaks.max() if len(win_streaks) > 0 else 0}\")\n        print(f\"  Max consecutive losses: {loss_streaks.max() if len(loss_streaks) > 0 else 0}\")\n        print(f\"  Average win streak: {win_streaks.mean():.1f}\" if len(win_streaks) > 0 else \"  Average win streak: N/A\")\n        print(f\"  Average loss streak: {loss_streaks.mean():.1f}\" if len(loss_streaks) > 0 else \"  Average loss streak: N/A\")\n    \n    # Risk-adjusted returns\n    if 'return' in trades_df.columns and trades_df['return'].std() > 0:\n        information_ratio = trades_df['return'].mean() / trades_df['return'].std()\n        print(f\"\\nRisk-Adjusted Metrics:\")\n        print(f\"  Information Ratio: {information_ratio:.3f}\")\n        print(f\"  Return/Risk: {trades_df['return'].mean() / trades_df['return'].std():.3f}\")\n    \n    # Value at Risk (VaR)\n    if 'pnl' in trades_df.columns:\n        var_95 = np.percentile(trades_df['pnl'], 5)\n        var_99 = np.percentile(trades_df['pnl'], 1)\n        \n        print(f\"\\nValue at Risk (VaR):\")\n        print(f\"  95% VaR: ${var_95:.2f}\")\n        print(f\"  99% VaR: ${var_99:.2f}\")\n        \n        # Plot PnL distribution\n        plt.figure(figsize=(10, 6))\n        plt.hist(trades_df['pnl'], bins=50, alpha=0.7, edgecolor='black')\n        plt.axvline(0, color='red', linestyle='--', alpha=0.5, label='Breakeven')\n        plt.axvline(trades_df['pnl'].mean(), color='green', linestyle='--', alpha=0.5, label='Mean PnL')\n        plt.axvline(var_95, color='orange', linestyle='--', alpha=0.5, label='95% VaR')\n        plt.xlabel('PnL ($)')\n        plt.ylabel('Frequency')\n        plt.title('PnL Distribution')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.show()\n    \n    # Show what columns are available for further analysis\n    print(f\"\\nAvailable trade data columns: {list(trades_df.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Generate summary and recommendations\nprint(\"\\n📋 SUMMARY AND RECOMMENDATIONS\")\nprint(\"=\" * 80)\n\nsummary = {\n    'run_info': {\n        'run_id': run_dir.name,\n        'config_name': config_name,\n        'analysis_timestamp': datetime.now().isoformat(),\n        'is_full_system': is_full_system\n    },\n    'data_summary': {\n        'total_bars': metadata.get('total_bars', 0),\n        'total_signals': metadata.get('total_signals', 0),\n        'total_orders': metadata.get('total_orders', 0),\n        'total_fills': metadata.get('total_fills', 0),\n        'total_positions': metadata.get('total_positions', 0)\n    },\n    'performance_summary': {},\n    'risk_summary': {},\n    'recommendations': []\n}\n\nif len(trades_df) > 0:\n    # Performance summary\n    summary['performance_summary'] = {\n        'total_trades': len(trades_df),\n        'total_return': float(total_return) if 'total_return' in locals() else 0,\n        'sharpe_ratio': float(sharpe_ratio) if 'sharpe_ratio' in locals() else 0,\n        'max_drawdown': float(max_drawdown) if 'max_drawdown' in locals() else 0,\n        'win_rate': float(len(winning_trades)/len(trades_df)) if 'winning_trades' in locals() else 0,\n        'profit_factor': float(winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())) if 'winning_trades' in locals() and 'losing_trades' in locals() and len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else 0\n    }\n    \n    # Risk summary\n    risk_summary = {}\n    if 'var_95' in locals():\n        risk_summary['var_95'] = float(var_95)\n    if 'var_99' in locals():\n        risk_summary['var_99'] = float(var_99)\n    if 'loss_streaks' in locals() and len(loss_streaks) > 0:\n        risk_summary['max_consecutive_losses'] = int(loss_streaks.max())\n    \n    # Add trade duration based on available columns\n    if 'duration' in trades_df.columns:\n        risk_summary['avg_trade_duration_minutes'] = float(trades_df['duration'].mean())\n    elif 'duration_bars' in trades_df.columns:\n        risk_summary['avg_trade_duration_bars'] = float(trades_df['duration_bars'].mean())\n    \n    summary['risk_summary'] = risk_summary\n    \n    # Generate recommendations\n    if 'sharpe_ratio' in locals() and sharpe_ratio < min_sharpe_ratio:\n        summary['recommendations'].append({\n            'type': 'performance',\n            'severity': 'high',\n            'message': f'Sharpe ratio ({sharpe_ratio:.2f}) below minimum threshold ({min_sharpe_ratio}). Consider parameter optimization.'\n        })\n    \n    if 'max_drawdown' in locals() and abs(max_drawdown) > max_acceptable_drawdown:\n        summary['recommendations'].append({\n            'type': 'risk',\n            'severity': 'high',\n            'message': f'Maximum drawdown ({abs(max_drawdown)*100:.1f}%) exceeds acceptable limit ({max_acceptable_drawdown*100:.0f}%). Implement stricter risk controls.'\n        })\n    \n    if 'winning_trades' in locals() and len(winning_trades)/len(trades_df) < min_win_rate:\n        summary['recommendations'].append({\n            'type': 'performance',\n            'severity': 'medium',\n            'message': f'Win rate ({len(winning_trades)/len(trades_df)*100:.1f}%) below minimum ({min_win_rate*100:.0f}%). Review entry criteria.'\n        })\n    \n    # Execution-specific recommendations\n    if 'fills' in locals() and 'slippage_bps' in fills.columns and fills['slippage_bps'].mean() > 5:\n        summary['recommendations'].append({\n            'type': 'execution',\n            'severity': 'medium',\n            'message': f'High average slippage ({fills[\"slippage_bps\"].mean():.1f} bps). Consider limit orders or better execution timing.'\n        })\n    \n    # Intraday pattern recommendations\n    if analyze_intraday_patterns and 'hourly_performance' in locals():\n        worst_hour = hourly_performance['pnl']['mean'].idxmin()\n        if hourly_performance.loc[worst_hour, ('pnl', 'mean')] < -50:\n            summary['recommendations'].append({\n                'type': 'timing',\n                'severity': 'low',\n                'message': f'Poor performance at {worst_hour}:00. Consider avoiding trades during this hour.'\n            })\n\n# Display recommendations\nif summary['recommendations']:\n    print(\"🎯 Recommendations:\")\n    for rec in sorted(summary['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']]):\n        severity_icon = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}[rec['severity']]\n        print(f\"\\n{severity_icon} [{rec['severity'].upper()}] {rec['type'].title()}\")\n        print(f\"   {rec['message']}\")\nelse:\n    print(\"✅ No critical issues identified\")\n\n# Save summary\nwith open(run_dir / 'analysis_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(f\"\\n📄 Analysis summary saved to: analysis_summary.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Export key dataframes for further analysis\nprint(\"\\n💾 EXPORTING RESULTS\")\nprint(\"=\" * 80)\n\nexports = {}\n\n# Export trades if available\nif len(trades_df) > 0:\n    trades_export_path = run_dir / 'analyzed_trades.csv'\n    trades_df.to_csv(trades_export_path, index=False)\n    exports['trades'] = 'analyzed_trades.csv'\n    print(f\"✅ Exported {len(trades_df)} trades to {trades_export_path}\")\n    \n    # Export performance summary\n    if 'pnl' in trades_df:\n        # Convert numpy types to Python native types for JSON serialization\n        performance_summary = {\n            'total_trades': int(len(trades_df)),\n            'total_pnl': float(trades_df['pnl'].sum()),\n            'win_rate': float((trades_df['pnl'] > 0).mean()),\n            'avg_pnl': float(trades_df['pnl'].mean()),\n            'best_trade': float(trades_df['pnl'].max()),\n            'worst_trade': float(trades_df['pnl'].min()),\n            'avg_duration_bars': float(trades_df['duration_bars'].mean()) if 'duration_bars' in trades_df else None\n        }\n        \n        perf_summary_path = run_dir / 'performance_summary.json'\n        with open(perf_summary_path, 'w') as f:\n            json.dump(performance_summary, f, indent=2)\n        exports['performance_summary'] = 'performance_summary.json'\n        print(f\"✅ Exported performance summary\")\n\n# Export slippage analysis if available\nif len(trades_df) > 0 and 'slippage_entry' in trades_df:\n    slippage_summary = trades_df[['trade_id', 'symbol', 'entry_fill_price', 'exit_fill_price', \n                                  'slippage_entry', 'slippage_exit']].copy()\n    slippage_path = run_dir / 'slippage_analysis.csv'\n    slippage_summary.to_csv(slippage_path, index=False)\n    exports['slippage'] = 'slippage_analysis.csv'\n    print(f\"✅ Exported slippage analysis\")\n\n# Export equity curve if calculated\nif 'equity_df' in locals() and len(equity_df) > 0:\n    equity_path = run_dir / 'equity_curve.csv'\n    equity_df.to_csv(equity_path, index=False)\n    exports['equity_curve'] = 'equity_curve.csv'\n    print(f\"✅ Exported equity curve\")\n\n# Create final report\nreport = {\n    'analysis_complete': True,\n    'timestamp': datetime.now().isoformat(),\n    'exports': exports,\n    'summary': summary,\n    'global_store_used': use_global_store,\n    'trades_file_location': str(store_path / 'T*.parquet') if use_global_store else None\n}\n\nfinal_report_path = run_dir / 'final_report.json'\nwith open(final_report_path, 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint(f\"\\n✅ Analysis complete! Results saved to {run_dir}\")\n\nif use_global_store:\n    print(f\"\\n📁 Note: Signal traces are in the global store at: {store_path}\")\n    print(f\"   Trade data may be in: {store_path}/T*.parquet\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}