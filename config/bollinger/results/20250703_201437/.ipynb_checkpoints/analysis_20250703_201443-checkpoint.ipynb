{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39832a10",
   "metadata": {
    "papermill": {
     "duration": 0.008193,
     "end_time": "2025-07-04T03:14:44.909060",
     "exception": false,
     "start_time": "2025-07-04T03:14:44.900867",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Trading System Analysis\n",
    "\n",
    "This notebook provides complete analysis of the full trading system including signals, portfolio, and execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Signal generation analysis\n",
    "- Trade execution analysis\n",
    "- Portfolio performance metrics\n",
    "- Risk analysis\n",
    "- Execution cost analysis\n",
    "- Position and fill analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb166bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:44.917969Z",
     "iopub.status.busy": "2025-07-04T03:14:44.917712Z",
     "iopub.status.idle": "2025-07-04T03:14:44.921799Z",
     "shell.execute_reply": "2025-07-04T03:14:44.921363Z"
    },
    "papermill": {
     "duration": 0.009264,
     "end_time": "2025-07-04T03:14:44.922966",
     "exception": false,
     "start_time": "2025-07-04T03:14:44.913702",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "\n",
    "# Analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_slippage = True\n",
    "analyze_intraday_patterns = True\n",
    "market_timezone = \"America/New_York\"\n",
    "\n",
    "# Performance thresholds\n",
    "min_sharpe_ratio = 1.0\n",
    "max_acceptable_drawdown = 0.20  # 20%\n",
    "min_win_rate = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb2cb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:44.929178Z",
     "iopub.status.busy": "2025-07-04T03:14:44.929029Z",
     "iopub.status.idle": "2025-07-04T03:14:44.931453Z",
     "shell.execute_reply": "2025-07-04T03:14:44.931161Z"
    },
    "papermill": {
     "duration": 0.006245,
     "end_time": "2025-07-04T03:14:44.932255",
     "exception": false,
     "start_time": "2025-07-04T03:14:44.926010",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_dir = \"/Users/daws/ADMF-PC/config/bollinger/results/20250703_201437\"\n",
    "config_name = \"bollinger\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "global_traces_dir = \"/Users/daws/ADMF-PC/traces\"\n",
    "min_strategies_to_analyze = 20\n",
    "sharpe_threshold = 1.0\n",
    "correlation_threshold = 0.7\n",
    "top_n_strategies = 10\n",
    "ensemble_size = 5\n",
    "calculate_all_performance = True\n",
    "performance_limit = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634c198",
   "metadata": {
    "papermill": {
     "duration": 0.002145,
     "end_time": "2025-07-04T03:14:44.936895",
     "exception": false,
     "start_time": "2025-07-04T03:14:44.934750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c8bd73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:44.942198Z",
     "iopub.status.busy": "2025-07-04T03:14:44.942070Z",
     "iopub.status.idle": "2025-07-04T03:14:45.369382Z",
     "shell.execute_reply": "2025-07-04T03:14:45.369115Z"
    },
    "papermill": {
     "duration": 0.430736,
     "end_time": "2025-07-04T03:14:45.370066",
     "exception": false,
     "start_time": "2025-07-04T03:14:44.939330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing run: 20250703_201437\n",
      "Full path: /Users/daws/ADMF-PC/config/bollinger/results/20250703_201437\n",
      "Config: bollinger\n",
      "Symbol(s): ['SPY']\n",
      "Timeframe: 5m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Convert run_dir to Path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03394d66",
   "metadata": {
    "papermill": {
     "duration": 0.00146,
     "end_time": "2025-07-04T03:14:45.373380",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.371920",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Metadata and Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074c2853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.376688Z",
     "iopub.status.busy": "2025-07-04T03:14:45.376538Z",
     "iopub.status.idle": "2025-07-04T03:14:45.380797Z",
     "shell.execute_reply": "2025-07-04T03:14:45.380603Z"
    },
    "papermill": {
     "duration": 0.006664,
     "end_time": "2025-07-04T03:14:45.381413",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.374749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run metadata loaded\n",
      "   Total bars: 16614\n",
      "   Total signals: 16601\n",
      "   Total orders: 2066\n",
      "   Total fills: 2066\n",
      "   Total positions: 2066\n",
      "\n",
      "📁 Global traces path: /Users/daws/ADMF-PC/traces\n",
      "\n",
      "📊 Available traces:\n",
      "   Global signals: ✅\n",
      "   Local signals: ❌\n",
      "   Portfolio: ❌\n",
      "   Execution: ❌\n"
     ]
    }
   ],
   "source": [
    "# Load run metadata\n",
    "metadata_path = run_dir / 'metadata.json'\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Run metadata loaded\")\n",
    "    print(f\"   Total bars: {metadata.get('total_bars', 'N/A')}\")\n",
    "    print(f\"   Total signals: {metadata.get('total_signals', 'N/A')}\")\n",
    "    print(f\"   Total orders: {metadata.get('total_orders', 'N/A')}\")\n",
    "    print(f\"   Total fills: {metadata.get('total_fills', 'N/A')}\")\n",
    "    print(f\"   Total positions: {metadata.get('total_positions', 'N/A')}\")\n",
    "    \n",
    "    # Get global traces path\n",
    "    global_traces_path = Path(metadata.get('global_traces_path', '/Users/daws/ADMF-PC/traces'))\n",
    "    print(f\"\\n📁 Global traces path: {global_traces_path}\")\n",
    "else:\n",
    "    print(\"❌ No metadata.json found\")\n",
    "    metadata = {}\n",
    "    global_traces_path = Path('/Users/daws/ADMF-PC/traces')\n",
    "\n",
    "# Check what traces are available in global store\n",
    "store_path = global_traces_path / 'store'\n",
    "has_global_signals = store_path.exists() and any(store_path.glob('*.parquet'))\n",
    "\n",
    "# For backward compatibility, also check run directory\n",
    "traces_dir = run_dir / 'traces'\n",
    "has_local_signals = (traces_dir / 'signals').exists() if traces_dir.exists() else False\n",
    "has_portfolio = (traces_dir / 'portfolio').exists() if traces_dir.exists() else False\n",
    "has_execution = (traces_dir / 'execution').exists() if traces_dir.exists() else False\n",
    "\n",
    "print(f\"\\n📊 Available traces:\")\n",
    "print(f\"   Global signals: {'✅' if has_global_signals else '❌'}\")\n",
    "print(f\"   Local signals: {'✅' if has_local_signals else '❌'}\")\n",
    "print(f\"   Portfolio: {'✅' if has_portfolio else '❌'}\")\n",
    "print(f\"   Execution: {'✅' if has_execution else '❌'}\")\n",
    "\n",
    "# Determine trace location\n",
    "use_global_store = has_global_signals and not has_local_signals\n",
    "is_full_system = metadata.get('total_orders', 0) > 0 or has_portfolio or has_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53579b59",
   "metadata": {
    "papermill": {
     "duration": 0.001412,
     "end_time": "2025-07-04T03:14:45.384284",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.382872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5b2913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.387666Z",
     "iopub.status.busy": "2025-07-04T03:14:45.387560Z",
     "iopub.status.idle": "2025-07-04T03:14:45.416725Z",
     "shell.execute_reply": "2025-07-04T03:14:45.416509Z"
    },
    "papermill": {
     "duration": 0.031532,
     "end_time": "2025-07-04T03:14:45.417341",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.385809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded market data from: /Users/daws/ADMF-PC/data/SPY_5m.csv\n",
      "   Date range: 2024-03-26 13:30:00+00:00 to 2025-04-02 19:20:00+00:00\n",
      "   Total bars: 20769\n"
     ]
    }
   ],
   "source": [
    "# Load market data\n",
    "market_data = None\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Try different possible locations\n",
    "        data_paths = [\n",
    "            run_dir / f'data/{symbol}_{timeframe}.csv',\n",
    "            run_dir / f'{symbol}_{timeframe}.csv',\n",
    "            run_dir.parent / f'data/{symbol}_{timeframe}.csv',\n",
    "            Path(f'/Users/daws/ADMF-PC/data/{symbol}_{timeframe}.csv')\n",
    "        ]\n",
    "        \n",
    "        for data_path in data_paths:\n",
    "            if data_path.exists():\n",
    "                market_data = pd.read_csv(data_path)\n",
    "                market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
    "                market_data = market_data.sort_values('timestamp')\n",
    "                \n",
    "                # Add derived fields\n",
    "                market_data['returns'] = market_data['close'].pct_change()\n",
    "                market_data['log_returns'] = np.log(market_data['close'] / market_data['close'].shift(1))\n",
    "                market_data['hour'] = market_data['timestamp'].dt.hour\n",
    "                market_data['minute'] = market_data['timestamp'].dt.minute\n",
    "                market_data['day_of_week'] = market_data['timestamp'].dt.dayofweek\n",
    "                \n",
    "                print(f\"✅ Loaded market data from: {data_path}\")\n",
    "                print(f\"   Date range: {market_data['timestamp'].min()} to {market_data['timestamp'].max()}\")\n",
    "                print(f\"   Total bars: {len(market_data)}\")\n",
    "                break\n",
    "        \n",
    "        if market_data is not None:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol}: {e}\")\n",
    "\n",
    "if market_data is None:\n",
    "    print(\"❌ Could not load market data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c462c95",
   "metadata": {
    "papermill": {
     "duration": 0.001439,
     "end_time": "2025-07-04T03:14:45.420463",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.419024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c210911b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.423788Z",
     "iopub.status.busy": "2025-07-04T03:14:45.423675Z",
     "iopub.status.idle": "2025-07-04T03:14:45.428278Z",
     "shell.execute_reply": "2025-07-04T03:14:45.428063Z"
    },
    "papermill": {
     "duration": 0.006997,
     "end_time": "2025-07-04T03:14:45.428890",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.421893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 SIGNAL ANALYSIS\n",
      "================================================================================\n",
      "No strategy index found in run directory\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze signals if available\n",
    "if has_global_signals or has_local_signals:\n",
    "    print(\"\\n📊 SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load strategy index\n",
    "    strategy_index_path = run_dir / 'strategy_index.parquet'\n",
    "    if strategy_index_path.exists():\n",
    "        strategy_index = pd.read_parquet(strategy_index_path)\n",
    "        print(f\"Loaded {len(strategy_index)} strategies from run index\")\n",
    "        \n",
    "        # Show strategy distribution\n",
    "        by_type = strategy_index['strategy_type'].value_counts()\n",
    "        print(\"\\nStrategies by type:\")\n",
    "        for stype, count in by_type.items():\n",
    "            print(f\"  {stype}: {count}\")\n",
    "    else:\n",
    "        print(\"No strategy index found in run directory\")\n",
    "        strategy_index = pd.DataFrame()\n",
    "    \n",
    "    # Analyze signal patterns from global store\n",
    "    if use_global_store and len(strategy_index) > 0:\n",
    "        print(\"\\n📊 Analyzing signals from global store...\")\n",
    "        \n",
    "        # Get trace paths from metadata components\n",
    "        signal_counts = []\n",
    "        components = metadata.get('components', {})\n",
    "        \n",
    "        for comp_name, comp_data in components.items():\n",
    "            if comp_data.get('type') == 'strategy' and 'trace_path' in comp_data:\n",
    "                trace_path = Path(comp_data['trace_path'])\n",
    "                if trace_path.exists():\n",
    "                    signals = pd.read_parquet(trace_path)\n",
    "                    \n",
    "                    # Count actual signal changes (non-zero values)\n",
    "                    signal_changes = signals[signals['val'] != 0]\n",
    "                    \n",
    "                    signal_counts.append({\n",
    "                        'strategy_type': comp_data.get('strategy_type'),\n",
    "                        'strategy_hash': comp_data.get('strategy_hash'),\n",
    "                        'total_signals': len(signals),\n",
    "                        'signal_changes': len(signal_changes),\n",
    "                        'long_signals': (signal_changes['val'] > 0).sum(),\n",
    "                        'short_signals': (signal_changes['val'] < 0).sum(),\n",
    "                        'signals_per_1000_bars': len(signal_changes) / (metadata.get('total_bars', 1000) / 1000)\n",
    "                    })\n",
    "                    \n",
    "                    # Show sample signals\n",
    "                    if len(signal_changes) > 0:\n",
    "                        print(f\"\\n  Strategy: {comp_data.get('strategy_type')} ({comp_data.get('strategy_hash', '')[:8]})\")\n",
    "                        print(f\"    Signal changes: {len(signal_changes)}\")\n",
    "                        print(f\"    First signal: {signal_changes.iloc[0]['ts']} -> {signal_changes.iloc[0]['val']}\")\n",
    "                        print(f\"    Last signal: {signal_changes.iloc[-1]['ts']} -> {signal_changes.iloc[-1]['val']}\")\n",
    "        \n",
    "        if signal_counts:\n",
    "            signal_df = pd.DataFrame(signal_counts)\n",
    "            print(\"\\n📊 Signal frequency analysis:\")\n",
    "            print(signal_df.to_string(index=False))\n",
    "            \n",
    "            # Check if signals were generated but no trades\n",
    "            if metadata.get('total_signals', 0) > 0 and metadata.get('total_orders', 0) == 0:\n",
    "                print(\"\\n⚠️ WARNING: Signals were generated but no orders were created!\")\n",
    "                print(\"Possible reasons:\")\n",
    "                print(\"  - Risk constraints (stop loss/take profit) may be too tight\")\n",
    "                print(\"  - Position sizing returned 0 shares\")\n",
    "                print(\"  - Intraday constraints prevented trades\")\n",
    "                print(\"  - Check the execution logs for more details\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No signal traces found in global store\")\n",
    "            \n",
    "    # Analyze from local traces (backward compatibility)\n",
    "    elif has_local_signals:\n",
    "        print(\"\\n📊 Analyzing signals from local traces...\")\n",
    "        # Original local trace analysis code here\n",
    "else:\n",
    "    print(\"\\n⚠️ No signal traces available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fc688",
   "metadata": {
    "papermill": {
     "duration": 0.001455,
     "end_time": "2025-07-04T03:14:45.431863",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.430408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Portfolio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc00d023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.435214Z",
     "iopub.status.busy": "2025-07-04T03:14:45.435107Z",
     "iopub.status.idle": "2025-07-04T03:14:45.447093Z",
     "shell.execute_reply": "2025-07-04T03:14:45.446882Z"
    },
    "papermill": {
     "duration": 0.014424,
     "end_time": "2025-07-04T03:14:45.447697",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.433273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💼 PORTFOLIO ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔍 Debug: Searching for portfolio traces in global store...\n",
      "Files in /Users/daws/ADMF-PC/traces/store:\n",
      "\n",
      "⚠️ No portfolio traces found in global store\n",
      "This suggests portfolio events may not be traced to the global store yet\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze portfolio data\n",
    "if is_full_system:\n",
    "    print(\"\\n💼 PORTFOLIO ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check both global and local traces\n",
    "    portfolio_traces_found = False\n",
    "    orders = pd.DataFrame()\n",
    "    positions_opened = pd.DataFrame()\n",
    "    positions_closed = pd.DataFrame()\n",
    "    \n",
    "    # First check global store for portfolio traces\n",
    "    if use_global_store:\n",
    "        # Look for portfolio traces in global store structure\n",
    "        # Check common patterns for portfolio data\n",
    "        global_portfolio_paths = [\n",
    "            global_traces_path / 'portfolio' / 'orders',\n",
    "            global_traces_path / 'orders',\n",
    "            store_path  # Flat structure in store\n",
    "        ]\n",
    "        \n",
    "        for path in global_portfolio_paths:\n",
    "            if path.exists():\n",
    "                # Look for order files\n",
    "                order_files = list(path.glob('*order*.parquet'))\n",
    "                if order_files:\n",
    "                    print(f\"Found {len(order_files)} order files in {path}\")\n",
    "                    # Load and concatenate all order files\n",
    "                    order_dfs = []\n",
    "                    for f in order_files:\n",
    "                        try:\n",
    "                            df = pd.read_parquet(f)\n",
    "                            order_dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error loading {f}: {e}\")\n",
    "                    if order_dfs:\n",
    "                        orders = pd.concat(order_dfs, ignore_index=True)\n",
    "                        orders['timestamp'] = pd.to_datetime(orders['timestamp'])\n",
    "                        portfolio_traces_found = True\n",
    "                        print(f\"✅ Loaded {len(orders)} orders from global store\")\n",
    "                        break\n",
    "        \n",
    "        # Look for position traces\n",
    "        position_open_files = []\n",
    "        position_close_files = []\n",
    "        \n",
    "        # Search for position files with various naming patterns\n",
    "        for path in global_portfolio_paths:\n",
    "            if path.exists():\n",
    "                position_open_files.extend(path.glob('*position*open*.parquet'))\n",
    "                position_open_files.extend(path.glob('*positions_open*.parquet'))\n",
    "                position_close_files.extend(path.glob('*position*close*.parquet'))\n",
    "                position_close_files.extend(path.glob('*positions_close*.parquet'))\n",
    "        \n",
    "        # Load position opens\n",
    "        if position_open_files:\n",
    "            pos_open_dfs = []\n",
    "            for f in position_open_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(f)\n",
    "                    pos_open_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {f}: {e}\")\n",
    "            if pos_open_dfs:\n",
    "                positions_opened = pd.concat(pos_open_dfs, ignore_index=True)\n",
    "                positions_opened['timestamp'] = pd.to_datetime(positions_opened['timestamp'])\n",
    "                portfolio_traces_found = True\n",
    "                print(f\"✅ Loaded {len(positions_opened)} position opens from global store\")\n",
    "        \n",
    "        # Load position closes\n",
    "        if position_close_files:\n",
    "            pos_close_dfs = []\n",
    "            for f in position_close_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(f)\n",
    "                    pos_close_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {f}: {e}\")\n",
    "            if pos_close_dfs:\n",
    "                positions_closed = pd.concat(pos_close_dfs, ignore_index=True)\n",
    "                positions_closed['timestamp'] = pd.to_datetime(positions_closed['timestamp'])\n",
    "                portfolio_traces_found = True\n",
    "                print(f\"✅ Loaded {len(positions_closed)} position closes from global store\")\n",
    "    \n",
    "    # Fall back to local traces if not found in global\n",
    "    if not portfolio_traces_found and has_portfolio:\n",
    "        # Original local portfolio loading code\n",
    "        orders_path = traces_dir / 'portfolio' / 'orders' / 'orders.parquet'\n",
    "        if orders_path.exists():\n",
    "            orders = pd.read_parquet(orders_path)\n",
    "            orders['timestamp'] = pd.to_datetime(orders['timestamp'])\n",
    "            print(f\"✅ Loaded {len(orders)} orders from local traces\")\n",
    "        \n",
    "        positions_open_path = traces_dir / 'portfolio' / 'positions_open' / 'positions_open.parquet'\n",
    "        positions_close_path = traces_dir / 'portfolio' / 'positions_close' / 'positions_close.parquet'\n",
    "        \n",
    "        if positions_open_path.exists():\n",
    "            positions_opened = pd.read_parquet(positions_open_path)\n",
    "            positions_opened['timestamp'] = pd.to_datetime(positions_opened['timestamp'])\n",
    "            print(f\"✅ Loaded {len(positions_opened)} position opens from local traces\")\n",
    "        \n",
    "        if positions_close_path.exists():\n",
    "            positions_closed = pd.read_parquet(positions_close_path)\n",
    "            positions_closed['timestamp'] = pd.to_datetime(positions_closed['timestamp'])\n",
    "            print(f\"✅ Loaded {len(positions_closed)} position closes from local traces\")\n",
    "    \n",
    "    # Analyze the data we found\n",
    "    if len(orders) > 0:\n",
    "        print(\"\\nOrder Statistics:\")\n",
    "        print(f\"  Total orders: {len(orders)}\")\n",
    "        if 'side' in orders.columns:\n",
    "            print(f\"  Buy orders: {(orders['side'] == 'buy').sum()}\")\n",
    "            print(f\"  Sell orders: {(orders['side'] == 'sell').sum()}\")\n",
    "        if 'order_type' in orders.columns:\n",
    "            print(\"\\nOrder types:\")\n",
    "            print(orders['order_type'].value_counts())\n",
    "    \n",
    "    # Analyze positions and create trades dataframe\n",
    "    trades_df = pd.DataFrame()\n",
    "    if len(positions_opened) > 0 and len(positions_closed) > 0:\n",
    "        # Match opens and closes\n",
    "        trades = []\n",
    "        for _, close in positions_closed.iterrows():\n",
    "            # Find matching open\n",
    "            position_id = close.get('position_id', close.get('metadata', {}).get('position_id'))\n",
    "            if position_id:\n",
    "                matching_opens = positions_opened[\n",
    "                    positions_opened.apply(lambda x: x.get('position_id', x.get('metadata', {}).get('position_id')) == position_id, axis=1)\n",
    "                ]\n",
    "                if len(matching_opens) > 0:\n",
    "                    open_pos = matching_opens.iloc[0]\n",
    "                    trades.append({\n",
    "                        'position_id': position_id,\n",
    "                        'symbol': close.get('symbol', close.get('metadata', {}).get('symbol')),\n",
    "                        'entry_time': open_pos['timestamp'],\n",
    "                        'exit_time': close['timestamp'],\n",
    "                        'entry_price': open_pos.get('price', open_pos.get('metadata', {}).get('entry_price', 0)),\n",
    "                        'exit_price': close.get('price', close.get('metadata', {}).get('exit_price', 0)),\n",
    "                        'quantity': close.get('metadata', {}).get('quantity', 0),\n",
    "                        'pnl': close.get('metadata', {}).get('realized_pnl', 0)\n",
    "                    })\n",
    "        \n",
    "        if trades:\n",
    "            trades_df = pd.DataFrame(trades)\n",
    "            trades_df['duration'] = (trades_df['exit_time'] - trades_df['entry_time']).dt.total_seconds() / 60  # minutes\n",
    "            trades_df['return'] = (trades_df['exit_price'] - trades_df['entry_price']) / trades_df['entry_price']\n",
    "            \n",
    "            print(\"\\n📊 Trade Statistics:\")\n",
    "            print(f\"  Total trades: {len(trades_df)}\")\n",
    "            print(f\"  Win rate: {(trades_df['pnl'] > 0).mean()*100:.1f}%\")\n",
    "            print(f\"  Average PnL: ${trades_df['pnl'].mean():.2f}\")\n",
    "            print(f\"  Total PnL: ${trades_df['pnl'].sum():.2f}\")\n",
    "            print(f\"  Average duration: {trades_df['duration'].mean():.1f} minutes\")\n",
    "            print(f\"  Average return: {trades_df['return'].mean()*100:.3f}%\")\n",
    "    \n",
    "    # Debug: show what files exist in global store\n",
    "    if not portfolio_traces_found and use_global_store:\n",
    "        print(\"\\n🔍 Debug: Searching for portfolio traces in global store...\")\n",
    "        print(f\"Files in {store_path}:\")\n",
    "        all_files = list(store_path.glob('*.parquet'))\n",
    "        # Group by pattern\n",
    "        order_files = [f for f in all_files if 'order' in f.name.lower()]\n",
    "        position_files = [f for f in all_files if 'position' in f.name.lower()]\n",
    "        \n",
    "        if order_files:\n",
    "            print(f\"\\nFound {len(order_files)} files with 'order' in name:\")\n",
    "            for f in order_files[:5]:\n",
    "                print(f\"  - {f.name}\")\n",
    "        \n",
    "        if position_files:\n",
    "            print(f\"\\nFound {len(position_files)} files with 'position' in name:\")\n",
    "            for f in position_files[:5]:\n",
    "                print(f\"  - {f.name}\")\n",
    "                \n",
    "        if not order_files and not position_files:\n",
    "            print(\"\\n⚠️ No portfolio traces found in global store\")\n",
    "            print(\"This suggests portfolio events may not be traced to the global store yet\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No portfolio data to analyze (no orders/fills/positions recorded)\")\n",
    "    trades_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3279c27",
   "metadata": {
    "papermill": {
     "duration": 0.001521,
     "end_time": "2025-07-04T03:14:45.450785",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.449264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebbe227",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.454609Z",
     "iopub.status.busy": "2025-07-04T03:14:45.454499Z",
     "iopub.status.idle": "2025-07-04T03:14:45.460579Z",
     "shell.execute_reply": "2025-07-04T03:14:45.460385Z"
    },
    "papermill": {
     "duration": 0.008579,
     "end_time": "2025-07-04T03:14:45.461141",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.452562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ EXECUTION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "🔍 Debug: Searching for execution traces in global store...\n",
      "\n",
      "⚠️ No fill traces found in global store\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze execution data\n",
    "if is_full_system:\n",
    "    print(\"\\n⚡ EXECUTION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    fills = pd.DataFrame()\n",
    "    execution_traces_found = False\n",
    "    \n",
    "    # First check global store for execution traces\n",
    "    if use_global_store:\n",
    "        # Look for fill traces in global store\n",
    "        global_execution_paths = [\n",
    "            global_traces_path / 'execution' / 'fills',\n",
    "            global_traces_path / 'fills',\n",
    "            store_path  # Flat structure\n",
    "        ]\n",
    "        \n",
    "        for path in global_execution_paths:\n",
    "            if path.exists():\n",
    "                fill_files = list(path.glob('*fill*.parquet'))\n",
    "                if fill_files:\n",
    "                    print(f\"Found {len(fill_files)} fill files in {path}\")\n",
    "                    fill_dfs = []\n",
    "                    for f in fill_files:\n",
    "                        try:\n",
    "                            df = pd.read_parquet(f)\n",
    "                            fill_dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error loading {f}: {e}\")\n",
    "                    if fill_dfs:\n",
    "                        fills = pd.concat(fill_dfs, ignore_index=True)\n",
    "                        fills['timestamp'] = pd.to_datetime(fills['timestamp'])\n",
    "                        execution_traces_found = True\n",
    "                        print(f\"✅ Loaded {len(fills)} fills from global store\")\n",
    "                        break\n",
    "    \n",
    "    # Fall back to local traces if not found\n",
    "    if not execution_traces_found and has_execution:\n",
    "        fills_path = traces_dir / 'execution' / 'fills' / 'fills.parquet'\n",
    "        if fills_path.exists():\n",
    "            fills = pd.read_parquet(fills_path)\n",
    "            fills['timestamp'] = pd.to_datetime(fills['timestamp'])\n",
    "            print(f\"✅ Loaded {len(fills)} fills from local traces\")\n",
    "    \n",
    "    # Analyze fills\n",
    "    if len(fills) > 0:\n",
    "        # Extract metadata\n",
    "        if 'metadata' in fills.columns:\n",
    "            # Expand metadata column\n",
    "            fill_details = pd.json_normalize(fills['metadata'])\n",
    "            fills = pd.concat([fills, fill_details], axis=1)\n",
    "        \n",
    "        # Fill analysis\n",
    "        print(\"\\nFill Statistics:\")\n",
    "        if 'quantity' in fills:\n",
    "            print(f\"  Total volume: {fills['quantity'].sum():,.0f} shares\")\n",
    "            print(f\"  Average fill size: {fills['quantity'].mean():.0f} shares\")\n",
    "        \n",
    "        if 'price' in fills:\n",
    "            print(f\"  Average fill price: ${fills['price'].mean():.2f}\")\n",
    "        \n",
    "        # Slippage analysis\n",
    "        if analyze_slippage and 'expected_price' in fills and 'price' in fills:\n",
    "            fills['slippage'] = (fills['price'] - fills['expected_price']) / fills['expected_price']\n",
    "            fills['slippage_bps'] = fills['slippage'] * 10000\n",
    "            \n",
    "            print(\"\\n💸 Slippage Analysis:\")\n",
    "            print(f\"  Average slippage: {fills['slippage_bps'].mean():.1f} bps\")\n",
    "            print(f\"  Slippage std dev: {fills['slippage_bps'].std():.1f} bps\")\n",
    "            print(f\"  Positive slippage: {(fills['slippage'] > 0).mean()*100:.1f}%\")\n",
    "            print(f\"  Total slippage cost: ${(fills['slippage'] * fills['quantity'] * fills['price']).sum():.2f}\")\n",
    "        \n",
    "        # Execution cost analysis\n",
    "        if 'commission' in fills:\n",
    "            print(\"\\n💰 Execution Costs:\")\n",
    "            print(f\"  Total commissions: ${fills['commission'].sum():.2f}\")\n",
    "            print(f\"  Average commission: ${fills['commission'].mean():.2f}\")\n",
    "            print(f\"  Commission as % of volume: {fills['commission'].sum() / (fills['quantity'] * fills['price']).sum() * 100:.3f}%\")\n",
    "    \n",
    "    # Debug if no execution traces found\n",
    "    if not execution_traces_found and use_global_store:\n",
    "        print(\"\\n🔍 Debug: Searching for execution traces in global store...\")\n",
    "        all_files = list(store_path.glob('*.parquet'))\n",
    "        fill_files = [f for f in all_files if 'fill' in f.name.lower()]\n",
    "        \n",
    "        if fill_files:\n",
    "            print(f\"\\nFound {len(fill_files)} files with 'fill' in name:\")\n",
    "            for f in fill_files[:5]:\n",
    "                print(f\"  - {f.name}\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No fill traces found in global store\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No execution data to analyze\")\n",
    "    fills = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea154b",
   "metadata": {
    "papermill": {
     "duration": 0.001511,
     "end_time": "2025-07-04T03:14:45.464199",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.462688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b4e32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.467996Z",
     "iopub.status.busy": "2025-07-04T03:14:45.467824Z",
     "iopub.status.idle": "2025-07-04T03:14:45.472428Z",
     "shell.execute_reply": "2025-07-04T03:14:45.472240Z"
    },
    "papermill": {
     "duration": 0.007097,
     "end_time": "2025-07-04T03:14:45.472986",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.465889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ No trades available for performance analysis\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall performance metrics if we have trades\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n📈 PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate equity curve from trades\n",
    "    initial_capital = 100000  # Assumed\n",
    "    equity = initial_capital\n",
    "    equity_curve = [{'timestamp': trades_df['entry_time'].min(), 'equity': equity}]\n",
    "    \n",
    "    for _, trade in trades_df.sort_values('exit_time').iterrows():\n",
    "        equity += trade['pnl']\n",
    "        equity_curve.append({'timestamp': trade['exit_time'], 'equity': equity})\n",
    "    \n",
    "    equity_df = pd.DataFrame(equity_curve)\n",
    "    equity_df['returns'] = equity_df['equity'].pct_change()\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_return = (equity_df['equity'].iloc[-1] / initial_capital - 1)\n",
    "    \n",
    "    # Sharpe ratio (assuming daily returns)\n",
    "    daily_returns = trades_df.groupby(trades_df['exit_time'].dt.date)['pnl'].sum() / equity\n",
    "    if len(daily_returns) > 1 and daily_returns.std() > 0:\n",
    "        sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    # Max drawdown\n",
    "    cummax = equity_df['equity'].expanding().max()\n",
    "    drawdown = (equity_df['equity'] / cummax - 1)\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Win/loss statistics\n",
    "    winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "    losing_trades = trades_df[trades_df['pnl'] <= 0]\n",
    "    \n",
    "    print(f\"Total Return: {total_return*100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown*100:.2f}%\")\n",
    "    print(f\"\\nTrade Statistics:\")\n",
    "    print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "    print(f\"  Average Win: ${winning_trades['pnl'].mean():.2f}\" if len(winning_trades) > 0 else \"  Average Win: N/A\")\n",
    "    print(f\"  Average Loss: ${losing_trades['pnl'].mean():.2f}\" if len(losing_trades) > 0 else \"  Average Loss: N/A\")\n",
    "    print(f\"  Profit Factor: {winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum()):.2f}\" if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else \"  Profit Factor: N/A\")\n",
    "    \n",
    "    # Performance vs thresholds\n",
    "    print(f\"\\n🎯 Performance vs Thresholds:\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_ratio:.2f} {'✅' if sharpe_ratio >= min_sharpe_ratio else '❌'} (min: {min_sharpe_ratio})\")\n",
    "    print(f\"  Max Drawdown: {abs(max_drawdown)*100:.1f}% {'✅' if abs(max_drawdown) <= max_acceptable_drawdown else '❌'} (max: {max_acceptable_drawdown*100:.0f}%)\")\n",
    "    print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}% {'✅' if len(winning_trades)/len(trades_df) >= min_win_rate else '❌'} (min: {min_win_rate*100:.0f}%)\")\n",
    "    \n",
    "    # Plot equity curve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(equity_df['timestamp'], equity_df['equity'])\n",
    "    plt.title('Portfolio Equity Curve')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Equity ($)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot drawdown\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.fill_between(equity_df['timestamp'], drawdown * 100, 0, alpha=0.3, color='red')\n",
    "    plt.title('Portfolio Drawdown')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Drawdown (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n⚠️ No trades available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702e49b",
   "metadata": {
    "papermill": {
     "duration": 0.001551,
     "end_time": "2025-07-04T03:14:45.476192",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.474641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Intraday Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "869d4d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.479642Z",
     "iopub.status.busy": "2025-07-04T03:14:45.479553Z",
     "iopub.status.idle": "2025-07-04T03:14:45.483717Z",
     "shell.execute_reply": "2025-07-04T03:14:45.483520Z"
    },
    "papermill": {
     "duration": 0.006605,
     "end_time": "2025-07-04T03:14:45.484306",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.477701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze intraday patterns if requested\n",
    "if analyze_intraday_patterns and len(trades_df) > 0:\n",
    "    print(\"\\n⏰ INTRADAY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract hour of entry and exit\n",
    "    trades_df['entry_hour'] = trades_df['entry_time'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time'].dt.hour\n",
    "    trades_df['entry_day'] = trades_df['entry_time'].dt.dayofweek\n",
    "    \n",
    "    # Performance by hour of day\n",
    "    hourly_performance = trades_df.groupby('entry_hour').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Win rate by hour\n",
    "    hourly_win_rate = trades_df.groupby('entry_hour').apply(\n",
    "        lambda x: (x['pnl'] > 0).mean() * 100\n",
    "    )\n",
    "    \n",
    "    # Performance by day of week\n",
    "    daily_performance = trades_df.groupby('entry_day').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Trades by hour\n",
    "    ax = axes[0, 0]\n",
    "    hourly_performance['pnl']['count'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Number of Trades by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Trade Count')\n",
    "    \n",
    "    # Win rate by hour\n",
    "    ax = axes[0, 1]\n",
    "    hourly_win_rate.plot(kind='bar', ax=ax, color='green')\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Win Rate by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Win Rate (%)')\n",
    "    \n",
    "    # Average PnL by hour\n",
    "    ax = axes[1, 0]\n",
    "    hourly_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='blue')\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    # Performance by day of week\n",
    "    ax = axes[1, 1]\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    daily_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='purple')\n",
    "    ax.set_xticklabels(days[:len(daily_performance)], rotation=0)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Day of Week')\n",
    "    ax.set_xlabel('Day of Week')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best and worst times\n",
    "    print(\"\\n🕐 Best Trading Hours:\")\n",
    "    best_hours = hourly_performance['pnl']['mean'].nlargest(3)\n",
    "    for hour, avg_pnl in best_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")\n",
    "    \n",
    "    print(\"\\n🕐 Worst Trading Hours:\")\n",
    "    worst_hours = hourly_performance['pnl']['mean'].nsmallest(3)\n",
    "    for hour, avg_pnl in worst_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0675d44",
   "metadata": {
    "papermill": {
     "duration": 0.001588,
     "end_time": "2025-07-04T03:14:45.487731",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.486143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2335aa7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.491297Z",
     "iopub.status.busy": "2025-07-04T03:14:45.491200Z",
     "iopub.status.idle": "2025-07-04T03:14:45.494879Z",
     "shell.execute_reply": "2025-07-04T03:14:45.494655Z"
    },
    "papermill": {
     "duration": 0.006196,
     "end_time": "2025-07-04T03:14:45.495475",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.489279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprehensive risk analysis\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n⚠️ RISK ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Trade duration analysis\n",
    "    print(\"Trade Duration Statistics:\")\n",
    "    print(f\"  Average: {trades_df['duration'].mean():.1f} minutes\")\n",
    "    print(f\"  Median: {trades_df['duration'].median():.1f} minutes\")\n",
    "    print(f\"  Shortest: {trades_df['duration'].min():.1f} minutes\")\n",
    "    print(f\"  Longest: {trades_df['duration'].max():.1f} minutes\")\n",
    "    \n",
    "    # Consecutive wins/losses\n",
    "    trades_df['is_win'] = trades_df['pnl'] > 0\n",
    "    trades_df['streak'] = (trades_df['is_win'] != trades_df['is_win'].shift()).cumsum()\n",
    "    \n",
    "    win_streaks = trades_df[trades_df['is_win']].groupby('streak').size()\n",
    "    loss_streaks = trades_df[~trades_df['is_win']].groupby('streak').size()\n",
    "    \n",
    "    print(f\"\\nStreak Analysis:\")\n",
    "    print(f\"  Max consecutive wins: {win_streaks.max() if len(win_streaks) > 0 else 0}\")\n",
    "    print(f\"  Max consecutive losses: {loss_streaks.max() if len(loss_streaks) > 0 else 0}\")\n",
    "    print(f\"  Average win streak: {win_streaks.mean():.1f}\" if len(win_streaks) > 0 else \"  Average win streak: N/A\")\n",
    "    print(f\"  Average loss streak: {loss_streaks.mean():.1f}\" if len(loss_streaks) > 0 else \"  Average loss streak: N/A\")\n",
    "    \n",
    "    # Risk-adjusted returns\n",
    "    if trades_df['return'].std() > 0:\n",
    "        information_ratio = trades_df['return'].mean() / trades_df['return'].std()\n",
    "        print(f\"\\nRisk-Adjusted Metrics:\")\n",
    "        print(f\"  Information Ratio: {information_ratio:.3f}\")\n",
    "        print(f\"  Return/Risk: {trades_df['return'].mean() / trades_df['return'].std():.3f}\")\n",
    "    \n",
    "    # Value at Risk (VaR)\n",
    "    var_95 = np.percentile(trades_df['pnl'], 5)\n",
    "    var_99 = np.percentile(trades_df['pnl'], 1)\n",
    "    \n",
    "    print(f\"\\nValue at Risk (VaR):\")\n",
    "    print(f\"  95% VaR: ${var_95:.2f}\")\n",
    "    print(f\"  99% VaR: ${var_99:.2f}\")\n",
    "    \n",
    "    # Plot PnL distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(trades_df['pnl'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "    plt.axvline(trades_df['pnl'].mean(), color='green', linestyle='--', alpha=0.5, label='Mean PnL')\n",
    "    plt.axvline(var_95, color='orange', linestyle='--', alpha=0.5, label='95% VaR')\n",
    "    plt.xlabel('PnL ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('PnL Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935ec95",
   "metadata": {
    "papermill": {
     "duration": 0.001549,
     "end_time": "2025-07-04T03:14:45.498586",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.497037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94766bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.502307Z",
     "iopub.status.busy": "2025-07-04T03:14:45.502178Z",
     "iopub.status.idle": "2025-07-04T03:14:45.507243Z",
     "shell.execute_reply": "2025-07-04T03:14:45.507043Z"
    },
    "papermill": {
     "duration": 0.007679,
     "end_time": "2025-07-04T03:14:45.507815",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.500136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📋 SUMMARY AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "✅ No critical issues identified\n",
      "\n",
      "📄 Analysis summary saved to: analysis_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Generate summary and recommendations\n",
    "print(\"\\n📋 SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'run_info': {\n",
    "        'run_id': run_dir.name,\n",
    "        'config_name': config_name,\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'is_full_system': is_full_system\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'total_bars': metadata.get('total_bars', 0),\n",
    "        'total_signals': metadata.get('total_signals', 0),\n",
    "        'total_orders': metadata.get('total_orders', 0),\n",
    "        'total_fills': metadata.get('total_fills', 0),\n",
    "        'total_positions': metadata.get('total_positions', 0)\n",
    "    },\n",
    "    'performance_summary': {},\n",
    "    'risk_summary': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "if len(trades_df) > 0:\n",
    "    # Performance summary\n",
    "    summary['performance_summary'] = {\n",
    "        'total_trades': len(trades_df),\n",
    "        'total_return': float(total_return),\n",
    "        'sharpe_ratio': float(sharpe_ratio),\n",
    "        'max_drawdown': float(max_drawdown),\n",
    "        'win_rate': float(len(winning_trades)/len(trades_df)),\n",
    "        'profit_factor': float(winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())) if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Risk summary\n",
    "    summary['risk_summary'] = {\n",
    "        'var_95': float(var_95),\n",
    "        'var_99': float(var_99),\n",
    "        'max_consecutive_losses': int(loss_streaks.max()) if len(loss_streaks) > 0 else 0,\n",
    "        'avg_trade_duration_minutes': float(trades_df['duration'].mean())\n",
    "    }\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if sharpe_ratio < min_sharpe_ratio:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'high',\n",
    "            'message': f'Sharpe ratio ({sharpe_ratio:.2f}) below minimum threshold ({min_sharpe_ratio}). Consider parameter optimization.'\n",
    "        })\n",
    "    \n",
    "    if abs(max_drawdown) > max_acceptable_drawdown:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'risk',\n",
    "            'severity': 'high',\n",
    "            'message': f'Maximum drawdown ({abs(max_drawdown)*100:.1f}%) exceeds acceptable limit ({max_acceptable_drawdown*100:.0f}%). Implement stricter risk controls.'\n",
    "        })\n",
    "    \n",
    "    if len(winning_trades)/len(trades_df) < min_win_rate:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'medium',\n",
    "            'message': f'Win rate ({len(winning_trades)/len(trades_df)*100:.1f}%) below minimum ({min_win_rate*100:.0f}%). Review entry criteria.'\n",
    "        })\n",
    "    \n",
    "    # Execution-specific recommendations\n",
    "    if 'slippage_bps' in fills.columns and fills['slippage_bps'].mean() > 5:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'execution',\n",
    "            'severity': 'medium',\n",
    "            'message': f'High average slippage ({fills[\"slippage_bps\"].mean():.1f} bps). Consider limit orders or better execution timing.'\n",
    "        })\n",
    "    \n",
    "    # Intraday pattern recommendations\n",
    "    if analyze_intraday_patterns and 'hourly_performance' in locals():\n",
    "        worst_hour = hourly_performance['pnl']['mean'].idxmin()\n",
    "        if hourly_performance.loc[worst_hour, ('pnl', 'mean')] < -50:\n",
    "            summary['recommendations'].append({\n",
    "                'type': 'timing',\n",
    "                'severity': 'low',\n",
    "                'message': f'Poor performance at {worst_hour}:00. Consider avoiding trades during this hour.'\n",
    "            })\n",
    "\n",
    "# Display recommendations\n",
    "if summary['recommendations']:\n",
    "    print(\"🎯 Recommendations:\")\n",
    "    for rec in sorted(summary['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']]):\n",
    "        severity_icon = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}[rec['severity']]\n",
    "        print(f\"\\n{severity_icon} [{rec['severity'].upper()}] {rec['type'].title()}\")\n",
    "        print(f\"   {rec['message']}\")\n",
    "else:\n",
    "    print(\"✅ No critical issues identified\")\n",
    "\n",
    "# Save summary\n",
    "with open(run_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n📄 Analysis summary saved to: analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833265d",
   "metadata": {
    "papermill": {
     "duration": 0.001671,
     "end_time": "2025-07-04T03:14:45.511136",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.509465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5af9ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:14:45.514852Z",
     "iopub.status.busy": "2025-07-04T03:14:45.514679Z",
     "iopub.status.idle": "2025-07-04T03:14:45.517675Z",
     "shell.execute_reply": "2025-07-04T03:14:45.517461Z"
    },
    "papermill": {
     "duration": 0.005512,
     "end_time": "2025-07-04T03:14:45.518255",
     "exception": false,
     "start_time": "2025-07-04T03:14:45.512743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 EXPORTING RESULTS\n",
      "================================================================================\n",
      "\n",
      "✅ Analysis complete! Results saved to /Users/daws/ADMF-PC/config/bollinger/results/20250703_201437\n"
     ]
    }
   ],
   "source": [
    "# Export key dataframes for further analysis\n",
    "print(\"\\n💾 EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exports = {}\n",
    "\n",
    "# Export trades if available\n",
    "if len(trades_df) > 0:\n",
    "    trades_df.to_csv(run_dir / 'analyzed_trades.csv', index=False)\n",
    "    exports['trades'] = 'analyzed_trades.csv'\n",
    "    print(f\"✅ Exported {len(trades_df)} trades\")\n",
    "\n",
    "# Export fills analysis if available\n",
    "if len(fills) > 0 and 'slippage_bps' in fills.columns:\n",
    "    slippage_summary = fills[['timestamp', 'symbol', 'quantity', 'price', 'expected_price', 'slippage_bps']]\n",
    "    slippage_summary.to_csv(run_dir / 'slippage_analysis.csv', index=False)\n",
    "    exports['slippage'] = 'slippage_analysis.csv'\n",
    "    print(f\"✅ Exported slippage analysis for {len(fills)} fills\")\n",
    "\n",
    "# Export performance metrics\n",
    "if 'equity_df' in locals():\n",
    "    equity_df.to_csv(run_dir / 'equity_curve.csv', index=False)\n",
    "    exports['equity_curve'] = 'equity_curve.csv'\n",
    "    print(f\"✅ Exported equity curve\")\n",
    "\n",
    "# Create final report\n",
    "report = {\n",
    "    'analysis_complete': True,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'exports': exports,\n",
    "    'summary': summary\n",
    "}\n",
    "\n",
    "with open(run_dir / 'final_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Results saved to {run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.736288,
   "end_time": "2025-07-04T03:14:45.735829",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/daws/ADMF-PC/src/analytics/templates/analysis.ipynb",
   "output_path": "config/bollinger/results/20250703_201437/analysis_20250703_201443.ipynb",
   "parameters": {
    "calculate_all_performance": true,
    "config_name": "bollinger",
    "correlation_threshold": 0.7,
    "ensemble_size": 5,
    "global_traces_dir": "/Users/daws/ADMF-PC/traces",
    "min_strategies_to_analyze": 20,
    "performance_limit": 100,
    "run_dir": "/Users/daws/ADMF-PC/config/bollinger/results/20250703_201437",
    "sharpe_threshold": 1.0,
    "symbols": [
     "SPY"
    ],
    "timeframe": "5m",
    "top_n_strategies": 10
   },
   "start_time": "2025-07-04T03:14:43.999541",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}