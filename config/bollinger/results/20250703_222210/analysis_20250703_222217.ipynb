{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e832980",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bf78e",
   "metadata": {
    "papermill": {
     "duration": 0.007365,
     "end_time": "2025-07-04T05:22:18.674728",
     "exception": false,
     "start_time": "2025-07-04T05:22:18.667363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Trading System Analysis\n",
    "\n",
    "This notebook provides complete analysis of the full trading system including signals, portfolio, and execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Signal generation analysis\n",
    "- Trade execution analysis\n",
    "- Portfolio performance metrics\n",
    "- Risk analysis\n",
    "- Execution cost analysis\n",
    "- Position and fill analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c71fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:18.684008Z",
     "iopub.status.busy": "2025-07-04T05:22:18.683864Z",
     "iopub.status.idle": "2025-07-04T05:22:18.689133Z",
     "shell.execute_reply": "2025-07-04T05:22:18.688871Z"
    },
    "papermill": {
     "duration": 0.009778,
     "end_time": "2025-07-04T05:22:18.690319",
     "exception": false,
     "start_time": "2025-07-04T05:22:18.680541",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "\n",
    "# Analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_slippage = True\n",
    "analyze_intraday_patterns = True\n",
    "market_timezone = \"America/New_York\"\n",
    "\n",
    "# Performance thresholds\n",
    "min_sharpe_ratio = 1.0\n",
    "max_acceptable_drawdown = 0.20  # 20%\n",
    "min_win_rate = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9deceb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:18.697327Z",
     "iopub.status.busy": "2025-07-04T05:22:18.697203Z",
     "iopub.status.idle": "2025-07-04T05:22:18.699599Z",
     "shell.execute_reply": "2025-07-04T05:22:18.699308Z"
    },
    "papermill": {
     "duration": 0.006346,
     "end_time": "2025-07-04T05:22:18.700460",
     "exception": false,
     "start_time": "2025-07-04T05:22:18.694114",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_dir = \"/Users/daws/ADMF-PC/config/bollinger/results/20250703_222210\"\n",
    "config_name = \"bollinger\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e177745",
   "metadata": {
    "papermill": {
     "duration": 0.002489,
     "end_time": "2025-07-04T05:22:18.705647",
     "exception": false,
     "start_time": "2025-07-04T05:22:18.703158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d57976f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:18.711054Z",
     "iopub.status.busy": "2025-07-04T05:22:18.710901Z",
     "iopub.status.idle": "2025-07-04T05:22:19.137731Z",
     "shell.execute_reply": "2025-07-04T05:22:19.137483Z"
    },
    "papermill": {
     "duration": 0.430313,
     "end_time": "2025-07-04T05:22:19.138411",
     "exception": false,
     "start_time": "2025-07-04T05:22:18.708098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing run: 20250703_222210\n",
      "Full path: /Users/daws/ADMF-PC/config/bollinger/results/20250703_222210\n",
      "Config: bollinger\n",
      "Symbol(s): ['SPY']\n",
      "Timeframe: 5m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Convert run_dir to Path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca9d96",
   "metadata": {
    "papermill": {
     "duration": 0.001512,
     "end_time": "2025-07-04T05:22:19.141647",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.140135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Metadata and Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c1ff7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:19.145056Z",
     "iopub.status.busy": "2025-07-04T05:22:19.144911Z",
     "iopub.status.idle": "2025-07-04T05:22:19.149722Z",
     "shell.execute_reply": "2025-07-04T05:22:19.149502Z"
    },
    "papermill": {
     "duration": 0.007275,
     "end_time": "2025-07-04T05:22:19.150337",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.143062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run metadata loaded\n",
      "   Total bars: 16614\n",
      "   Total signals: 16601\n",
      "   Total orders: 4132\n",
      "   Total fills: 4132\n",
      "   Total positions: 4132\n",
      "\n",
      "📁 Global traces path: /Users/daws/ADMF-PC/traces\n",
      "\n",
      "📊 Available traces:\n",
      "   Global signals: ✅\n",
      "   Local signals: ❌\n",
      "   Portfolio: ❌\n",
      "   Execution: ❌\n"
     ]
    }
   ],
   "source": [
    "# Load run metadata\n",
    "metadata_path = run_dir / 'metadata.json'\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Run metadata loaded\")\n",
    "    print(f\"   Total bars: {metadata.get('total_bars', 'N/A')}\")\n",
    "    print(f\"   Total signals: {metadata.get('total_signals', 'N/A')}\")\n",
    "    print(f\"   Total orders: {metadata.get('total_orders', 'N/A')}\")\n",
    "    print(f\"   Total fills: {metadata.get('total_fills', 'N/A')}\")\n",
    "    print(f\"   Total positions: {metadata.get('total_positions', 'N/A')}\")\n",
    "    \n",
    "    # Get global traces path\n",
    "    global_traces_path = Path(metadata.get('global_traces_path', '/Users/daws/ADMF-PC/traces'))\n",
    "    print(f\"\\n📁 Global traces path: {global_traces_path}\")\n",
    "else:\n",
    "    print(\"❌ No metadata.json found\")\n",
    "    metadata = {}\n",
    "    global_traces_path = Path('/Users/daws/ADMF-PC/traces')\n",
    "\n",
    "# Check what traces are available in global store\n",
    "store_path = global_traces_path / 'store'\n",
    "has_global_signals = store_path.exists() and any(store_path.glob('*.parquet'))\n",
    "\n",
    "# For backward compatibility, also check run directory\n",
    "traces_dir = run_dir / 'traces'\n",
    "has_local_signals = (traces_dir / 'signals').exists() if traces_dir.exists() else False\n",
    "has_portfolio = (traces_dir / 'portfolio').exists() if traces_dir.exists() else False\n",
    "has_execution = (traces_dir / 'execution').exists() if traces_dir.exists() else False\n",
    "\n",
    "print(f\"\\n📊 Available traces:\")\n",
    "print(f\"   Global signals: {'✅' if has_global_signals else '❌'}\")\n",
    "print(f\"   Local signals: {'✅' if has_local_signals else '❌'}\")\n",
    "print(f\"   Portfolio: {'✅' if has_portfolio else '❌'}\")\n",
    "print(f\"   Execution: {'✅' if has_execution else '❌'}\")\n",
    "\n",
    "# Determine trace location\n",
    "use_global_store = has_global_signals and not has_local_signals\n",
    "is_full_system = metadata.get('total_orders', 0) > 0 or has_portfolio or has_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def68d41",
   "metadata": {
    "papermill": {
     "duration": 0.00145,
     "end_time": "2025-07-04T05:22:19.153261",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.151811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645fa807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:19.156812Z",
     "iopub.status.busy": "2025-07-04T05:22:19.156630Z",
     "iopub.status.idle": "2025-07-04T05:22:19.189358Z",
     "shell.execute_reply": "2025-07-04T05:22:19.189106Z"
    },
    "papermill": {
     "duration": 0.035276,
     "end_time": "2025-07-04T05:22:19.190027",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.154751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded market data from: /Users/daws/ADMF-PC/data/SPY_5m.csv\n",
      "   Date range: 2024-03-26 13:30:00+00:00 to 2025-04-02 19:20:00+00:00\n",
      "   Total bars: 20769\n"
     ]
    }
   ],
   "source": [
    "# Load market data\n",
    "market_data = None\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Try different possible locations\n",
    "        data_paths = [\n",
    "            run_dir / f'data/{symbol}_{timeframe}.csv',\n",
    "            run_dir / f'{symbol}_{timeframe}.csv',\n",
    "            run_dir.parent / f'data/{symbol}_{timeframe}.csv',\n",
    "            Path(f'/Users/daws/ADMF-PC/data/{symbol}_{timeframe}.csv')\n",
    "        ]\n",
    "        \n",
    "        for data_path in data_paths:\n",
    "            if data_path.exists():\n",
    "                market_data = pd.read_csv(data_path)\n",
    "                market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
    "                market_data = market_data.sort_values('timestamp')\n",
    "                \n",
    "                # Add derived fields\n",
    "                market_data['returns'] = market_data['close'].pct_change()\n",
    "                market_data['log_returns'] = np.log(market_data['close'] / market_data['close'].shift(1))\n",
    "                market_data['hour'] = market_data['timestamp'].dt.hour\n",
    "                market_data['minute'] = market_data['timestamp'].dt.minute\n",
    "                market_data['day_of_week'] = market_data['timestamp'].dt.dayofweek\n",
    "                \n",
    "                print(f\"✅ Loaded market data from: {data_path}\")\n",
    "                print(f\"   Date range: {market_data['timestamp'].min()} to {market_data['timestamp'].max()}\")\n",
    "                print(f\"   Total bars: {len(market_data)}\")\n",
    "                break\n",
    "        \n",
    "        if market_data is not None:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol}: {e}\")\n",
    "\n",
    "if market_data is None:\n",
    "    print(\"❌ Could not load market data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b430f77",
   "metadata": {
    "papermill": {
     "duration": 0.001502,
     "end_time": "2025-07-04T05:22:19.193195",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.191693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d69234b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:19.196627Z",
     "iopub.status.busy": "2025-07-04T05:22:19.196505Z",
     "iopub.status.idle": "2025-07-04T05:22:19.201314Z",
     "shell.execute_reply": "2025-07-04T05:22:19.201093Z"
    },
    "papermill": {
     "duration": 0.007276,
     "end_time": "2025-07-04T05:22:19.201913",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.194637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 SIGNAL ANALYSIS\n",
      "================================================================================\n",
      "No strategy index found in run directory\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze signals if available\n",
    "if has_global_signals or has_local_signals:\n",
    "    print(\"\\n📊 SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load strategy index\n",
    "    strategy_index_path = run_dir / 'strategy_index.parquet'\n",
    "    if strategy_index_path.exists():\n",
    "        strategy_index = pd.read_parquet(strategy_index_path)\n",
    "        print(f\"Loaded {len(strategy_index)} strategies from run index\")\n",
    "        \n",
    "        # Show strategy distribution\n",
    "        by_type = strategy_index['strategy_type'].value_counts()\n",
    "        print(\"\\nStrategies by type:\")\n",
    "        for stype, count in by_type.items():\n",
    "            print(f\"  {stype}: {count}\")\n",
    "    else:\n",
    "        print(\"No strategy index found in run directory\")\n",
    "        strategy_index = pd.DataFrame()\n",
    "    \n",
    "    # Analyze signal patterns from global store\n",
    "    if use_global_store and len(strategy_index) > 0:\n",
    "        print(\"\\n📊 Analyzing signals from global store...\")\n",
    "        \n",
    "        # Get trace paths from metadata components\n",
    "        signal_counts = []\n",
    "        components = metadata.get('components', {})\n",
    "        \n",
    "        for comp_name, comp_data in components.items():\n",
    "            if comp_data.get('type') == 'strategy' and 'trace_path' in comp_data:\n",
    "                trace_path = Path(comp_data['trace_path'])\n",
    "                if trace_path.exists():\n",
    "                    signals = pd.read_parquet(trace_path)\n",
    "                    \n",
    "                    # Count actual signal changes (non-zero values)\n",
    "                    signal_changes = signals[signals['val'] != 0]\n",
    "                    \n",
    "                    signal_counts.append({\n",
    "                        'strategy_type': comp_data.get('strategy_type'),\n",
    "                        'strategy_hash': comp_data.get('strategy_hash'),\n",
    "                        'total_signals': len(signals),\n",
    "                        'signal_changes': len(signal_changes),\n",
    "                        'long_signals': (signal_changes['val'] > 0).sum(),\n",
    "                        'short_signals': (signal_changes['val'] < 0).sum(),\n",
    "                        'signals_per_1000_bars': len(signal_changes) / (metadata.get('total_bars', 1000) / 1000)\n",
    "                    })\n",
    "                    \n",
    "                    # Show sample signals\n",
    "                    if len(signal_changes) > 0:\n",
    "                        print(f\"\\n  Strategy: {comp_data.get('strategy_type')} ({comp_data.get('strategy_hash', '')[:8]})\")\n",
    "                        print(f\"    Signal changes: {len(signal_changes)}\")\n",
    "                        print(f\"    First signal: {signal_changes.iloc[0]['ts']} -> {signal_changes.iloc[0]['val']}\")\n",
    "                        print(f\"    Last signal: {signal_changes.iloc[-1]['ts']} -> {signal_changes.iloc[-1]['val']}\")\n",
    "        \n",
    "        if signal_counts:\n",
    "            signal_df = pd.DataFrame(signal_counts)\n",
    "            print(\"\\n📊 Signal frequency analysis:\")\n",
    "            print(signal_df.to_string(index=False))\n",
    "            \n",
    "            # Check if signals were generated but no trades\n",
    "            if metadata.get('total_signals', 0) > 0 and metadata.get('total_orders', 0) == 0:\n",
    "                print(\"\\n⚠️ WARNING: Signals were generated but no orders were created!\")\n",
    "                print(\"Possible reasons:\")\n",
    "                print(\"  - Risk constraints (stop loss/take profit) may be too tight\")\n",
    "                print(\"  - Position sizing returned 0 shares\")\n",
    "                print(\"  - Intraday constraints prevented trades\")\n",
    "                print(\"  - Check the execution logs for more details\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No signal traces found in global store\")\n",
    "            \n",
    "    # Analyze from local traces (backward compatibility)\n",
    "    elif has_local_signals:\n",
    "        print(\"\\n📊 Analyzing signals from local traces...\")\n",
    "        # Original local trace analysis code here\n",
    "else:\n",
    "    print(\"\\n⚠️ No signal traces available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8941b8",
   "metadata": {
    "papermill": {
     "duration": 0.001493,
     "end_time": "2025-07-04T05:22:19.205057",
     "exception": false,
     "start_time": "2025-07-04T05:22:19.203564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Portfolio Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5b1f0",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6844a48f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:22:19.208565Z",
     "iopub.status.busy": "2025-07-04T05:22:19.208450Z",
     "iopub.status.idle": "2025-07-04T05:22:19.597683Z",
     "shell.execute_reply": "2025-07-04T05:22:19.597168Z"
    },
    "papermill": {
     "duration": 0.391818,
     "end_time": "2025-07-04T05:22:19.598369",
     "exception": true,
     "start_time": "2025-07-04T05:22:19.206551",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💼 PORTFOLIO ANALYSIS\n",
      "================================================================================\n",
      "Found 25 trades file(s) in global store\n",
      "✅ Loaded 1033 trades from Ta405f5fbdee7.parquet\n",
      "✅ Loaded 1033 trades from T2bfbfc99f8b5.parquet\n",
      "✅ Loaded 4 trades from T93b7e27d534b.parquet\n",
      "✅ Loaded 2 trades from T5d20cf8187db.parquet\n",
      "✅ Loaded 2 trades from Ta781416fd9e4.parquet\n",
      "✅ Loaded 2 trades from Tf120fa29a212.parquet\n",
      "✅ Loaded 1033 trades from T2a95646ad129.parquet\n",
      "✅ Loaded 4 trades from T0afff94cd392.parquet\n",
      "✅ Loaded 9 trades from T2e7fbce3bf82.parquet\n",
      "✅ Loaded 1033 trades from T9b9b28f3ffa9.parquet\n",
      "✅ Loaded 4 trades from Tdb3a6f6d0fa9.parquet\n",
      "✅ Loaded 4 trades from Td619c3141148.parquet\n",
      "✅ Loaded 4 trades from T174d05c1f713.parquet\n",
      "✅ Loaded 1033 trades from Tae329989009e.parquet\n",
      "✅ Loaded 1033 trades from T36bd18061b3a.parquet\n",
      "✅ Loaded 9 trades from T71e430ef2a6e.parquet\n",
      "✅ Loaded 1033 trades from T077f7715d2e5.parquet\n",
      "✅ Loaded 1033 trades from Tb00c26431149.parquet\n",
      "✅ Loaded 1033 trades from T0b01c74c847d.parquet\n",
      "✅ Loaded 1033 trades from Tb052b88aa35c.parquet\n",
      "✅ Loaded 1033 trades from T10966321165c.parquet\n",
      "✅ Loaded 4 trades from T5e50ecc0c55c.parquet\n",
      "✅ Loaded 4 trades from Td3468538d525.parquet\n",
      "✅ Loaded 1033 trades from T864299be09c1.parquet\n",
      "✅ Loaded 4 trades from T45a145caf923.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total trades loaded: 12452\n",
      "\n",
      "🔍 DEBUG: Trades DataFrame Analysis\n",
      "Shape: (12452, 31)\n",
      "\n",
      "Columns: ['trade_id', 'position_id', 'symbol', 'strategy_id', 'signal_hash', 'entry_bar_idx', 'entry_time', 'entry_signal_strength', 'direction', 'entry_order_id', 'entry_order_price', 'entry_order_time', 'entry_fill_id', 'entry_fill_price', 'entry_fill_time', 'exit_bar_idx', 'exit_time', 'exit_reason', 'exit_signal_strength', 'pnl', 'duration_bars', 'exit_order_id', 'exit_order_price', 'exit_order_time', 'exit_fill_id', 'exit_fill_price', 'exit_fill_time', 'commission', 'slippage_entry', 'slippage_exit', 'duration_time']\n",
      "\n",
      "📊 Price field analysis:\n",
      "entry_fill_price non-null: 2098 / 12452\n",
      "exit_fill_price non-null: 2078 / 12452\n",
      "entry_order_price non-null: 2100 / 12452\n",
      "exit_order_price non-null: 2078 / 12452\n",
      "\n",
      "entry_fill_price unique values (first 5): [None '521.11' '521.4' '520.21' '519.19']\n",
      "exit_fill_price unique values (first 5): [None 521.4 521.3969 519.8198425 519.1114]\n",
      "\n",
      "📋 Sample trade (first row):\n",
      "  trade_id: T000001\n",
      "  position_id: unknown\n",
      "  symbol: SPY_5m\n",
      "  strategy_id: SPY_5m_strategy_0\n",
      "  signal_hash: d63509043abb\n",
      "  entry_bar_idx: 27\n",
      "  entry_time: 2024-03-26 15:45:00+00:00\n",
      "  entry_signal_strength: 0.0\n",
      "  direction: unknown\n",
      "  entry_order_id: None\n",
      "  entry_order_price: None\n",
      "  entry_order_time: NaT\n",
      "  entry_fill_id: None\n",
      "  entry_fill_price: None\n",
      "  entry_fill_time: NaT\n",
      "  exit_bar_idx: 28\n",
      "  exit_time: 2024-03-26 15:50:00+00:00\n",
      "  exit_reason: Strategy reversal signal (short)\n",
      "  exit_signal_strength: 0\n",
      "  pnl: 0.0\n",
      "  duration_bars: 1\n",
      "  exit_order_id: None\n",
      "  exit_order_price: None\n",
      "  exit_order_time: NaT\n",
      "  exit_fill_id: None\n",
      "  exit_fill_price: None\n",
      "  exit_fill_time: NaT\n",
      "  commission: 0\n",
      "  slippage_entry: 0.0\n",
      "  slippage_exit: 0.0\n",
      "  duration_time: 0:05:00\n",
      "\n",
      "✅ Trades with complete price data: 2078 / 12452\n",
      "\n",
      "📊 Trade Statistics:\n",
      "  Total trades: 12452\n",
      "  Unique strategies: 1\n",
      "  Win rate: 4.4%\n",
      "  Average PnL: $0.00\n",
      "  Total PnL: $17.70\n",
      "  Average duration: 1.1 bars\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/computation/expressions.py:242\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(op, a, b, use_numexpr)\u001b[39m\n\u001b[32m    240\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[32m    241\u001b[39m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/computation/expressions.py:73\u001b[39m, in \u001b[36m_evaluate_standard\u001b[39m\u001b[34m(op, op_str, a, b)\u001b[39m\n\u001b[32m     72\u001b[39m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'NoneType' and 'NoneType'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Average duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrades_df[\u001b[33m'\u001b[39m\u001b[33mduration_bars\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m bars\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mentry_fill_price\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trades_df \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mexit_fill_price\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trades_df:\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# Calculate returns from fill prices\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     trades_df[\u001b[33m'\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[43mtrades_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexit_fill_price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrades_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_fill_price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m) / trades_df[\u001b[33m'\u001b[39m\u001b[33mentry_fill_price\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Adjust for short trades\u001b[39;00m\n\u001b[32m     89\u001b[39m     short_mask = trades_df[\u001b[33m'\u001b[39m\u001b[33mdirection\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33mshort\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:194\u001b[39m, in \u001b[36mOpsMixin.__sub__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__sub__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/series.py:6146\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6145\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other)\n\u001b[32m-> \u001b[39m\u001b[32m6146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/base.py:1391\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1388\u001b[39m     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:283\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:227\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         result = \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:163\u001b[39m, in \u001b[36m_masked_arith_op\u001b[39m\u001b[34m(x, y, op)\u001b[39m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m         result[mask] = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Load and analyze portfolio data\n",
    "if is_full_system:\n",
    "    print(\"\\n💼 PORTFOLIO ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    trades_df = pd.DataFrame()\n",
    "    portfolio_traces_found = False\n",
    "    \n",
    "    # First check for unified trades file in global store\n",
    "    if use_global_store:\n",
    "        # Look for trades files (T{hash}.parquet pattern)\n",
    "        trades_files = list(store_path.glob('T*.parquet'))\n",
    "        \n",
    "        if trades_files:\n",
    "            print(f\"Found {len(trades_files)} trades file(s) in global store\")\n",
    "            \n",
    "            # Load the most recent trades file (they have timestamps in the hash)\n",
    "            # or load all and concatenate if multiple runs\n",
    "            all_trades = []\n",
    "            for trades_file in trades_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(trades_file)\n",
    "                    print(f\"✅ Loaded {len(df)} trades from {trades_file.name}\")\n",
    "                    all_trades.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {trades_file}: {e}\")\n",
    "            \n",
    "            if all_trades:\n",
    "                trades_df = pd.concat(all_trades, ignore_index=True)\n",
    "                \n",
    "                # Convert timestamp columns\n",
    "                for col in ['entry_time', 'exit_time', 'entry_order_time', 'exit_order_time', \n",
    "                           'entry_fill_time', 'exit_fill_time']:\n",
    "                    if col in trades_df.columns:\n",
    "                        trades_df[col] = pd.to_datetime(trades_df[col])\n",
    "                \n",
    "                portfolio_traces_found = True\n",
    "                print(f\"✅ Total trades loaded: {len(trades_df)}\")\n",
    "                \n",
    "                # DEBUG: Examine trades data structure\n",
    "                print(\"\\n🔍 DEBUG: Trades DataFrame Analysis\")\n",
    "                print(f\"Shape: {trades_df.shape}\")\n",
    "                print(f\"\\nColumns: {list(trades_df.columns)}\")\n",
    "                \n",
    "                # Check price fields\n",
    "                print(f\"\\n📊 Price field analysis:\")\n",
    "                print(f\"entry_fill_price non-null: {trades_df['entry_fill_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"exit_fill_price non-null: {trades_df['exit_fill_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"entry_order_price non-null: {trades_df['entry_order_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"exit_order_price non-null: {trades_df['exit_order_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                \n",
    "                # Show unique values\n",
    "                print(f\"\\nentry_fill_price unique values (first 5): {trades_df['entry_fill_price'].unique()[:5]}\")\n",
    "                print(f\"exit_fill_price unique values (first 5): {trades_df['exit_fill_price'].unique()[:5]}\")\n",
    "                \n",
    "                # Show a sample trade with all fields\n",
    "                if len(trades_df) > 0:\n",
    "                    print(f\"\\n📋 Sample trade (first row):\")\n",
    "                    sample = trades_df.iloc[0]\n",
    "                    for col in trades_df.columns:\n",
    "                        print(f\"  {col}: {sample[col]}\")\n",
    "                \n",
    "                # Check for trades with complete price data\n",
    "                complete_price_trades = trades_df[\n",
    "                    trades_df['entry_fill_price'].notna() & \n",
    "                    trades_df['exit_fill_price'].notna()\n",
    "                ]\n",
    "                print(f\"\\n✅ Trades with complete price data: {len(complete_price_trades)} / {len(trades_df)}\")\n",
    "                \n",
    "                # Show trade statistics\n",
    "                if len(trades_df) > 0:\n",
    "                    print(\"\\n📊 Trade Statistics:\")\n",
    "                    print(f\"  Total trades: {len(trades_df)}\")\n",
    "                    print(f\"  Unique strategies: {trades_df['strategy_id'].nunique() if 'strategy_id' in trades_df else 'N/A'}\")\n",
    "                    \n",
    "                    if 'pnl' in trades_df:\n",
    "                        winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "                        print(f\"  Win rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "                        print(f\"  Average PnL: ${trades_df['pnl'].mean():.2f}\")\n",
    "                        print(f\"  Total PnL: ${trades_df['pnl'].sum():.2f}\")\n",
    "                    \n",
    "                    if 'duration_bars' in trades_df:\n",
    "                        print(f\"  Average duration: {trades_df['duration_bars'].mean():.1f} bars\")\n",
    "                    \n",
    "                    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "                        # Calculate returns from fill prices\n",
    "                        trades_df['return'] = (trades_df['exit_fill_price'] - trades_df['entry_fill_price']) / trades_df['entry_fill_price']\n",
    "                        # Adjust for short trades\n",
    "                        short_mask = trades_df['direction'] == 'short'\n",
    "                        trades_df.loc[short_mask, 'return'] = -trades_df.loc[short_mask, 'return']\n",
    "                        print(f\"  Average return: {trades_df['return'].mean()*100:.3f}%\")\n",
    "                    \n",
    "                    # Show sample trades\n",
    "                    print(\"\\n📋 Sample trades:\")\n",
    "                    display_cols = ['trade_id', 'symbol', 'direction', 'entry_time', 'exit_time', \n",
    "                                   'entry_fill_price', 'exit_fill_price', 'pnl']\n",
    "                    display_cols = [col for col in display_cols if col in trades_df.columns]\n",
    "                    if display_cols:\n",
    "                        print(trades_df[display_cols].head(5).to_string(index=False))\n",
    "    \n",
    "    # Fallback to old format if no unified trades file\n",
    "    if not portfolio_traces_found:\n",
    "        print(\"\\n⚠️ No unified trades file found, checking for legacy portfolio traces...\")\n",
    "        \n",
    "        # Check both global and local for individual order/position files\n",
    "        orders = pd.DataFrame()\n",
    "        positions_opened = pd.DataFrame()\n",
    "        positions_closed = pd.DataFrame()\n",
    "        \n",
    "        # [Previous code for loading individual order/position files remains as fallback]\n",
    "        # ... (keeping the existing fallback code)\n",
    "        \n",
    "    # If still no trades found\n",
    "    if len(trades_df) == 0:\n",
    "        print(\"\\n⚠️ No trades found. Possible reasons:\")\n",
    "        print(\"  - Signals didn't trigger any trades due to risk constraints\")\n",
    "        print(\"  - Position sizing returned 0 shares\")\n",
    "        print(\"  - Intraday constraints prevented trades\")\n",
    "        print(\"  - Check console output for execution warnings\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No portfolio data to analyze (metadata shows 0 orders/fills/positions)\")\n",
    "    trades_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f96b7b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Execution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b55bd8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and analyze execution data\n",
    "if is_full_system and len(trades_df) > 0:\n",
    "    print(\"\\n⚡ EXECUTION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract execution metrics from trades dataframe\n",
    "    print(\"📊 Analyzing execution quality from trades data...\")\n",
    "    \n",
    "    # Fill statistics\n",
    "    total_fills = len(trades_df) * 2  # Entry and exit for each trade\n",
    "    print(f\"\\nFill Statistics:\")\n",
    "    print(f\"  Total fills: {total_fills} (entry + exit fills)\")\n",
    "    \n",
    "    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "        all_fill_prices = pd.concat([trades_df['entry_fill_price'], trades_df['exit_fill_price']])\n",
    "        print(f\"  Average fill price: ${all_fill_prices.mean():.2f}\")\n",
    "        print(f\"  Fill price range: ${all_fill_prices.min():.2f} - ${all_fill_prices.max():.2f}\")\n",
    "    \n",
    "    # Slippage analysis\n",
    "    if analyze_slippage:\n",
    "        print(\"\\n💸 Slippage Analysis:\")\n",
    "        \n",
    "        # Entry slippage\n",
    "        if 'slippage_entry' in trades_df:\n",
    "            entry_slippage_bps = trades_df['slippage_entry'] / trades_df['entry_fill_price'] * 10000\n",
    "            print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n",
    "        elif 'entry_order_price' in trades_df and 'entry_fill_price' in trades_df:\n",
    "            # Calculate if not pre-computed\n",
    "            trades_df['slippage_entry'] = abs(trades_df['entry_fill_price'] - trades_df['entry_order_price'])\n",
    "            entry_slippage_bps = trades_df['slippage_entry'] / trades_df['entry_fill_price'] * 10000\n",
    "            print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n",
    "        \n",
    "        # Exit slippage\n",
    "        if 'slippage_exit' in trades_df:\n",
    "            exit_slippage_bps = trades_df['slippage_exit'] / trades_df['exit_fill_price'] * 10000\n",
    "            print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n",
    "        elif 'exit_order_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "            # Calculate if not pre-computed\n",
    "            trades_df['slippage_exit'] = abs(trades_df['exit_fill_price'] - trades_df['exit_order_price'])\n",
    "            exit_slippage_bps = trades_df['slippage_exit'] / trades_df['exit_fill_price'] * 10000\n",
    "            print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n",
    "        \n",
    "        # Total slippage cost\n",
    "        if 'slippage_entry' in trades_df and 'slippage_exit' in trades_df:\n",
    "            # Assuming quantity of 1 for simplicity (adjust if quantity is available)\n",
    "            total_slippage_cost = (trades_df['slippage_entry'] + trades_df['slippage_exit']).sum()\n",
    "            print(f\"  Total slippage cost: ${total_slippage_cost:.2f}\")\n",
    "    \n",
    "    # Execution timing analysis\n",
    "    print(\"\\n⏱️ Execution Timing:\")\n",
    "    if 'entry_order_time' in trades_df and 'entry_fill_time' in trades_df:\n",
    "        # Calculate time to fill\n",
    "        trades_df['entry_time_to_fill'] = (trades_df['entry_fill_time'] - trades_df['entry_order_time']).dt.total_seconds()\n",
    "        valid_times = trades_df['entry_time_to_fill'].dropna()\n",
    "        if len(valid_times) > 0:\n",
    "            print(f\"  Entry order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n",
    "    \n",
    "    if 'exit_order_time' in trades_df and 'exit_fill_time' in trades_df:\n",
    "        trades_df['exit_time_to_fill'] = (trades_df['exit_fill_time'] - trades_df['exit_order_time']).dt.total_seconds()\n",
    "        valid_times = trades_df['exit_time_to_fill'].dropna()\n",
    "        if len(valid_times) > 0:\n",
    "            print(f\"  Exit order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n",
    "    \n",
    "    # Commission analysis\n",
    "    if 'commission' in trades_df:\n",
    "        print(\"\\n💰 Execution Costs:\")\n",
    "        print(f\"  Total commissions: ${trades_df['commission'].sum():.2f}\")\n",
    "        print(f\"  Average commission per trade: ${trades_df['commission'].mean():.2f}\")\n",
    "        print(f\"  Commission as % of PnL: {abs(trades_df['commission'].sum() / trades_df['pnl'].sum() * 100):.2f}%\")\n",
    "    \n",
    "    # Exit reason analysis\n",
    "    if 'exit_reason' in trades_df:\n",
    "        print(\"\\n🎯 Exit Reasons:\")\n",
    "        exit_reasons = trades_df['exit_reason'].value_counts()\n",
    "        for reason, count in exit_reasons.items():\n",
    "            print(f\"  {reason}: {count} ({count/len(trades_df)*100:.1f}%)\")\n",
    "            \n",
    "elif is_full_system:\n",
    "    print(\"\\n⚠️ No execution data to analyze (no trades found)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping execution analysis (no trades executed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba7b86",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e1200",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate overall performance metrics if we have trades\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n📈 PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use PnL data if available\n",
    "    if 'pnl' in trades_df:\n",
    "        # Calculate equity curve from trades\n",
    "        initial_capital = 100000  # Assumed\n",
    "        trades_df = trades_df.sort_values('exit_time')\n",
    "        trades_df['cum_pnl'] = trades_df['pnl'].cumsum()\n",
    "        trades_df['equity'] = initial_capital + trades_df['cum_pnl']\n",
    "        \n",
    "        # Create equity curve dataframe\n",
    "        equity_curve = []\n",
    "        equity_curve.append({'timestamp': trades_df['entry_time'].min(), 'equity': initial_capital})\n",
    "        for _, trade in trades_df.iterrows():\n",
    "            equity_curve.append({'timestamp': trade['exit_time'], 'equity': trade['equity']})\n",
    "        \n",
    "        equity_df = pd.DataFrame(equity_curve)\n",
    "        equity_df['returns'] = equity_df['equity'].pct_change()\n",
    "        \n",
    "        # Performance metrics\n",
    "        total_return = (equity_df['equity'].iloc[-1] / initial_capital - 1)\n",
    "        \n",
    "        # Sharpe ratio (assuming daily returns)\n",
    "        if 'exit_time' in trades_df and len(trades_df) > 1:\n",
    "            daily_returns = trades_df.groupby(trades_df['exit_time'].dt.date)['pnl'].sum() / initial_capital\n",
    "            if len(daily_returns) > 1 and daily_returns.std() > 0:\n",
    "                sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "            else:\n",
    "                sharpe_ratio = 0\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "        \n",
    "        # Max drawdown\n",
    "        cummax = equity_df['equity'].expanding().max()\n",
    "        drawdown = (equity_df['equity'] / cummax - 1)\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Win/loss statistics\n",
    "        winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "        losing_trades = trades_df[trades_df['pnl'] <= 0]\n",
    "        \n",
    "        print(f\"Total Return: {total_return*100:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "        print(f\"Max Drawdown: {max_drawdown*100:.2f}%\")\n",
    "        print(f\"\\nTrade Statistics:\")\n",
    "        print(f\"  Total Trades: {len(trades_df)}\")\n",
    "        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "        print(f\"  Average Win: ${winning_trades['pnl'].mean():.2f}\" if len(winning_trades) > 0 else \"  Average Win: N/A\")\n",
    "        print(f\"  Average Loss: ${losing_trades['pnl'].mean():.2f}\" if len(losing_trades) > 0 else \"  Average Loss: N/A\")\n",
    "        \n",
    "        if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0:\n",
    "            profit_factor = winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())\n",
    "            print(f\"  Profit Factor: {profit_factor:.2f}\")\n",
    "        else:\n",
    "            print(f\"  Profit Factor: N/A\")\n",
    "        \n",
    "        # Performance vs thresholds\n",
    "        print(f\"\\n🎯 Performance vs Thresholds:\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe_ratio:.2f} {'✅' if sharpe_ratio >= min_sharpe_ratio else '❌'} (min: {min_sharpe_ratio})\")\n",
    "        print(f\"  Max Drawdown: {abs(max_drawdown)*100:.1f}% {'✅' if abs(max_drawdown) <= max_acceptable_drawdown else '❌'} (max: {max_acceptable_drawdown*100:.0f}%)\")\n",
    "        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}% {'✅' if len(winning_trades)/len(trades_df) >= min_win_rate else '❌'} (min: {min_win_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Plot equity curve\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(equity_df['timestamp'], equity_df['equity'])\n",
    "        plt.title('Portfolio Equity Curve')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Equity ($)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot drawdown\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.fill_between(equity_df['timestamp'], drawdown * 100, 0, alpha=0.3, color='red')\n",
    "        plt.title('Portfolio Drawdown')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ No PnL data available in trades\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No trades available for performance analysis\")\n",
    "    print(\"\\nPossible reasons why no trades were executed:\")\n",
    "    print(\"1. Risk parameters (stop loss: 0.075%, take profit: 0.1%) may be too tight\")\n",
    "    print(\"2. Position sizing returned 0 shares\")\n",
    "    print(\"3. Intraday constraints prevented trades\") \n",
    "    print(\"4. Signals were generated but didn't meet execution criteria\")\n",
    "    print(\"\\nCheck the signal analysis section above to confirm signals were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be608189",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Intraday Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbe334",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze intraday patterns if requested\n",
    "if analyze_intraday_patterns and len(trades_df) > 0:\n",
    "    print(\"\\n⏰ INTRADAY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract hour of entry and exit\n",
    "    trades_df['entry_hour'] = trades_df['entry_time'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time'].dt.hour\n",
    "    trades_df['entry_day'] = trades_df['entry_time'].dt.dayofweek\n",
    "    \n",
    "    # Performance by hour of day\n",
    "    hourly_performance = trades_df.groupby('entry_hour').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Win rate by hour\n",
    "    hourly_win_rate = trades_df.groupby('entry_hour').apply(\n",
    "        lambda x: (x['pnl'] > 0).mean() * 100\n",
    "    )\n",
    "    \n",
    "    # Performance by day of week\n",
    "    daily_performance = trades_df.groupby('entry_day').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Trades by hour\n",
    "    ax = axes[0, 0]\n",
    "    hourly_performance['pnl']['count'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Number of Trades by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Trade Count')\n",
    "    \n",
    "    # Win rate by hour\n",
    "    ax = axes[0, 1]\n",
    "    hourly_win_rate.plot(kind='bar', ax=ax, color='green')\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Win Rate by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Win Rate (%)')\n",
    "    \n",
    "    # Average PnL by hour\n",
    "    ax = axes[1, 0]\n",
    "    hourly_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='blue')\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    # Performance by day of week\n",
    "    ax = axes[1, 1]\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    daily_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='purple')\n",
    "    ax.set_xticklabels(days[:len(daily_performance)], rotation=0)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Day of Week')\n",
    "    ax.set_xlabel('Day of Week')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best and worst times\n",
    "    print(\"\\n🕐 Best Trading Hours:\")\n",
    "    best_hours = hourly_performance['pnl']['mean'].nlargest(3)\n",
    "    for hour, avg_pnl in best_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")\n",
    "    \n",
    "    print(\"\\n🕐 Worst Trading Hours:\")\n",
    "    worst_hours = hourly_performance['pnl']['mean'].nsmallest(3)\n",
    "    for hour, avg_pnl in worst_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ac5e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27c520",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprehensive risk analysis\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n⚠️ RISK ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Trade duration analysis\n",
    "    if 'duration' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics:\")\n",
    "        print(f\"  Average: {trades_df['duration'].mean():.1f} minutes\")\n",
    "        print(f\"  Median: {trades_df['duration'].median():.1f} minutes\")\n",
    "        print(f\"  Shortest: {trades_df['duration'].min():.1f} minutes\")\n",
    "        print(f\"  Longest: {trades_df['duration'].max():.1f} minutes\")\n",
    "    elif 'duration_bars' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics (in bars):\")\n",
    "        print(f\"  Average: {trades_df['duration_bars'].mean():.1f} bars\")\n",
    "        print(f\"  Median: {trades_df['duration_bars'].median():.1f} bars\")\n",
    "        print(f\"  Shortest: {trades_df['duration_bars'].min():.0f} bars\")\n",
    "        print(f\"  Longest: {trades_df['duration_bars'].max():.0f} bars\")\n",
    "    elif 'duration_time' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics:\")\n",
    "        print(f\"  Duration times available in 'duration_time' column\")\n",
    "    \n",
    "    # Consecutive wins/losses\n",
    "    if 'pnl' in trades_df.columns:\n",
    "        trades_df['is_win'] = trades_df['pnl'] > 0\n",
    "        trades_df['streak'] = (trades_df['is_win'] != trades_df['is_win'].shift()).cumsum()\n",
    "        \n",
    "        win_streaks = trades_df[trades_df['is_win']].groupby('streak').size()\n",
    "        loss_streaks = trades_df[~trades_df['is_win']].groupby('streak').size()\n",
    "        \n",
    "        print(f\"\\nStreak Analysis:\")\n",
    "        print(f\"  Max consecutive wins: {win_streaks.max() if len(win_streaks) > 0 else 0}\")\n",
    "        print(f\"  Max consecutive losses: {loss_streaks.max() if len(loss_streaks) > 0 else 0}\")\n",
    "        print(f\"  Average win streak: {win_streaks.mean():.1f}\" if len(win_streaks) > 0 else \"  Average win streak: N/A\")\n",
    "        print(f\"  Average loss streak: {loss_streaks.mean():.1f}\" if len(loss_streaks) > 0 else \"  Average loss streak: N/A\")\n",
    "    \n",
    "    # Risk-adjusted returns\n",
    "    if 'return' in trades_df.columns and trades_df['return'].std() > 0:\n",
    "        information_ratio = trades_df['return'].mean() / trades_df['return'].std()\n",
    "        print(f\"\\nRisk-Adjusted Metrics:\")\n",
    "        print(f\"  Information Ratio: {information_ratio:.3f}\")\n",
    "        print(f\"  Return/Risk: {trades_df['return'].mean() / trades_df['return'].std():.3f}\")\n",
    "    \n",
    "    # Value at Risk (VaR)\n",
    "    if 'pnl' in trades_df.columns:\n",
    "        var_95 = np.percentile(trades_df['pnl'], 5)\n",
    "        var_99 = np.percentile(trades_df['pnl'], 1)\n",
    "        \n",
    "        print(f\"\\nValue at Risk (VaR):\")\n",
    "        print(f\"  95% VaR: ${var_95:.2f}\")\n",
    "        print(f\"  99% VaR: ${var_99:.2f}\")\n",
    "        \n",
    "        # Plot PnL distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(trades_df['pnl'], bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(0, color='red', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "        plt.axvline(trades_df['pnl'].mean(), color='green', linestyle='--', alpha=0.5, label='Mean PnL')\n",
    "        plt.axvline(var_95, color='orange', linestyle='--', alpha=0.5, label='95% VaR')\n",
    "        plt.xlabel('PnL ($)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('PnL Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Show what columns are available for further analysis\n",
    "    print(f\"\\nAvailable trade data columns: {list(trades_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acdbe5e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463eb8ec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate summary and recommendations\n",
    "print(\"\\n📋 SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'run_info': {\n",
    "        'run_id': run_dir.name,\n",
    "        'config_name': config_name,\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'is_full_system': is_full_system\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'total_bars': metadata.get('total_bars', 0),\n",
    "        'total_signals': metadata.get('total_signals', 0),\n",
    "        'total_orders': metadata.get('total_orders', 0),\n",
    "        'total_fills': metadata.get('total_fills', 0),\n",
    "        'total_positions': metadata.get('total_positions', 0)\n",
    "    },\n",
    "    'performance_summary': {},\n",
    "    'risk_summary': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "if len(trades_df) > 0:\n",
    "    # Performance summary\n",
    "    summary['performance_summary'] = {\n",
    "        'total_trades': len(trades_df),\n",
    "        'total_return': float(total_return) if 'total_return' in locals() else 0,\n",
    "        'sharpe_ratio': float(sharpe_ratio) if 'sharpe_ratio' in locals() else 0,\n",
    "        'max_drawdown': float(max_drawdown) if 'max_drawdown' in locals() else 0,\n",
    "        'win_rate': float(len(winning_trades)/len(trades_df)) if 'winning_trades' in locals() else 0,\n",
    "        'profit_factor': float(winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())) if 'winning_trades' in locals() and 'losing_trades' in locals() and len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Risk summary\n",
    "    risk_summary = {}\n",
    "    if 'var_95' in locals():\n",
    "        risk_summary['var_95'] = float(var_95)\n",
    "    if 'var_99' in locals():\n",
    "        risk_summary['var_99'] = float(var_99)\n",
    "    if 'loss_streaks' in locals() and len(loss_streaks) > 0:\n",
    "        risk_summary['max_consecutive_losses'] = int(loss_streaks.max())\n",
    "    \n",
    "    # Add trade duration based on available columns\n",
    "    if 'duration' in trades_df.columns:\n",
    "        risk_summary['avg_trade_duration_minutes'] = float(trades_df['duration'].mean())\n",
    "    elif 'duration_bars' in trades_df.columns:\n",
    "        risk_summary['avg_trade_duration_bars'] = float(trades_df['duration_bars'].mean())\n",
    "    \n",
    "    summary['risk_summary'] = risk_summary\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if 'sharpe_ratio' in locals() and sharpe_ratio < min_sharpe_ratio:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'high',\n",
    "            'message': f'Sharpe ratio ({sharpe_ratio:.2f}) below minimum threshold ({min_sharpe_ratio}). Consider parameter optimization.'\n",
    "        })\n",
    "    \n",
    "    if 'max_drawdown' in locals() and abs(max_drawdown) > max_acceptable_drawdown:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'risk',\n",
    "            'severity': 'high',\n",
    "            'message': f'Maximum drawdown ({abs(max_drawdown)*100:.1f}%) exceeds acceptable limit ({max_acceptable_drawdown*100:.0f}%). Implement stricter risk controls.'\n",
    "        })\n",
    "    \n",
    "    if 'winning_trades' in locals() and len(winning_trades)/len(trades_df) < min_win_rate:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'medium',\n",
    "            'message': f'Win rate ({len(winning_trades)/len(trades_df)*100:.1f}%) below minimum ({min_win_rate*100:.0f}%). Review entry criteria.'\n",
    "        })\n",
    "    \n",
    "    # Execution-specific recommendations\n",
    "    if 'fills' in locals() and 'slippage_bps' in fills.columns and fills['slippage_bps'].mean() > 5:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'execution',\n",
    "            'severity': 'medium',\n",
    "            'message': f'High average slippage ({fills[\"slippage_bps\"].mean():.1f} bps). Consider limit orders or better execution timing.'\n",
    "        })\n",
    "    \n",
    "    # Intraday pattern recommendations\n",
    "    if analyze_intraday_patterns and 'hourly_performance' in locals():\n",
    "        worst_hour = hourly_performance['pnl']['mean'].idxmin()\n",
    "        if hourly_performance.loc[worst_hour, ('pnl', 'mean')] < -50:\n",
    "            summary['recommendations'].append({\n",
    "                'type': 'timing',\n",
    "                'severity': 'low',\n",
    "                'message': f'Poor performance at {worst_hour}:00. Consider avoiding trades during this hour.'\n",
    "            })\n",
    "\n",
    "# Display recommendations\n",
    "if summary['recommendations']:\n",
    "    print(\"🎯 Recommendations:\")\n",
    "    for rec in sorted(summary['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']]):\n",
    "        severity_icon = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}[rec['severity']]\n",
    "        print(f\"\\n{severity_icon} [{rec['severity'].upper()}] {rec['type'].title()}\")\n",
    "        print(f\"   {rec['message']}\")\n",
    "else:\n",
    "    print(\"✅ No critical issues identified\")\n",
    "\n",
    "# Save summary\n",
    "with open(run_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n📄 Analysis summary saved to: analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e786e37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892922a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export key dataframes for further analysis\n",
    "print(\"\\n💾 EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exports = {}\n",
    "\n",
    "# Export trades if available\n",
    "if len(trades_df) > 0:\n",
    "    trades_export_path = run_dir / 'analyzed_trades.csv'\n",
    "    trades_df.to_csv(trades_export_path, index=False)\n",
    "    exports['trades'] = 'analyzed_trades.csv'\n",
    "    print(f\"✅ Exported {len(trades_df)} trades to {trades_export_path}\")\n",
    "    \n",
    "    # Export performance summary\n",
    "    if 'pnl' in trades_df:\n",
    "        # Convert numpy types to Python native types for JSON serialization\n",
    "        performance_summary = {\n",
    "            'total_trades': int(len(trades_df)),\n",
    "            'total_pnl': float(trades_df['pnl'].sum()),\n",
    "            'win_rate': float((trades_df['pnl'] > 0).mean()),\n",
    "            'avg_pnl': float(trades_df['pnl'].mean()),\n",
    "            'best_trade': float(trades_df['pnl'].max()),\n",
    "            'worst_trade': float(trades_df['pnl'].min()),\n",
    "            'avg_duration_bars': float(trades_df['duration_bars'].mean()) if 'duration_bars' in trades_df else None\n",
    "        }\n",
    "        \n",
    "        perf_summary_path = run_dir / 'performance_summary.json'\n",
    "        with open(perf_summary_path, 'w') as f:\n",
    "            json.dump(performance_summary, f, indent=2)\n",
    "        exports['performance_summary'] = 'performance_summary.json'\n",
    "        print(f\"✅ Exported performance summary\")\n",
    "\n",
    "# Export slippage analysis if available\n",
    "if len(trades_df) > 0 and 'slippage_entry' in trades_df:\n",
    "    slippage_summary = trades_df[['trade_id', 'symbol', 'entry_fill_price', 'exit_fill_price', \n",
    "                                  'slippage_entry', 'slippage_exit']].copy()\n",
    "    slippage_path = run_dir / 'slippage_analysis.csv'\n",
    "    slippage_summary.to_csv(slippage_path, index=False)\n",
    "    exports['slippage'] = 'slippage_analysis.csv'\n",
    "    print(f\"✅ Exported slippage analysis\")\n",
    "\n",
    "# Export equity curve if calculated\n",
    "if 'equity_df' in locals() and len(equity_df) > 0:\n",
    "    equity_path = run_dir / 'equity_curve.csv'\n",
    "    equity_df.to_csv(equity_path, index=False)\n",
    "    exports['equity_curve'] = 'equity_curve.csv'\n",
    "    print(f\"✅ Exported equity curve\")\n",
    "\n",
    "# Create final report\n",
    "report = {\n",
    "    'analysis_complete': True,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'exports': exports,\n",
    "    'summary': summary,\n",
    "    'global_store_used': use_global_store,\n",
    "    'trades_file_location': str(store_path / 'T*.parquet') if use_global_store else None\n",
    "}\n",
    "\n",
    "final_report_path = run_dir / 'final_report.json'\n",
    "with open(final_report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Results saved to {run_dir}\")\n",
    "\n",
    "if use_global_store:\n",
    "    print(f\"\\n📁 Note: Signal traces are in the global store at: {store_path}\")\n",
    "    print(f\"   Trade data may be in: {store_path}/T*.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.16286,
   "end_time": "2025-07-04T05:22:19.916368",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/daws/ADMF-PC/src/analytics/templates/analysis.ipynb",
   "output_path": "config/bollinger/results/20250703_222210/analysis_20250703_222217.ipynb",
   "parameters": {
    "config_name": "bollinger",
    "run_dir": "/Users/daws/ADMF-PC/config/bollinger/results/20250703_222210",
    "symbols": [
     "SPY"
    ],
    "timeframe": "5m"
   },
   "start_time": "2025-07-04T05:22:17.753508",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}