{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1ed23d",
   "metadata": {
    "papermill": {
     "duration": 0.00285,
     "end_time": "2025-07-04T03:12:09.644765",
     "exception": false,
     "start_time": "2025-07-04T03:12:09.641915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Trading System Analysis\n",
    "\n",
    "This notebook provides complete analysis of the full trading system including signals, portfolio, and execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Signal generation analysis\n",
    "- Trade execution analysis\n",
    "- Portfolio performance metrics\n",
    "- Risk analysis\n",
    "- Execution cost analysis\n",
    "- Position and fill analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c8fd0d",
   "metadata": {
    "papermill": {
     "duration": 0.005916,
     "end_time": "2025-07-04T03:12:09.652689",
     "exception": false,
     "start_time": "2025-07-04T03:12:09.646773",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "\n",
    "# Analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_slippage = True\n",
    "analyze_intraday_patterns = True\n",
    "market_timezone = \"America/New_York\"\n",
    "\n",
    "# Performance thresholds\n",
    "min_sharpe_ratio = 1.0\n",
    "max_acceptable_drawdown = 0.20  # 20%\n",
    "min_win_rate = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fead4062",
   "metadata": {
    "papermill": {
     "duration": 0.004606,
     "end_time": "2025-07-04T03:12:09.659086",
     "exception": false,
     "start_time": "2025-07-04T03:12:09.654480",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_dir = \"/Users/daws/ADMF-PC/config/bollinger/results/20250703_201201\"\n",
    "config_name = \"bollinger\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "global_traces_dir = \"/Users/daws/ADMF-PC/traces\"\n",
    "min_strategies_to_analyze = 20\n",
    "sharpe_threshold = 1.0\n",
    "correlation_threshold = 0.7\n",
    "top_n_strategies = 10\n",
    "ensemble_size = 5\n",
    "calculate_all_performance = True\n",
    "performance_limit = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00336fe3",
   "metadata": {
    "papermill": {
     "duration": 0.001554,
     "end_time": "2025-07-04T03:12:09.662308",
     "exception": false,
     "start_time": "2025-07-04T03:12:09.660754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c859bb7",
   "metadata": {
    "papermill": {
     "duration": 0.450243,
     "end_time": "2025-07-04T03:12:10.114083",
     "exception": false,
     "start_time": "2025-07-04T03:12:09.663840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing run: 20250703_201201\n",
      "Full path: /Users/daws/ADMF-PC/config/bollinger/results/20250703_201201\n",
      "Config: bollinger\n",
      "Symbol(s): ['SPY']\n",
      "Timeframe: 5m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Convert run_dir to Path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32230042",
   "metadata": {
    "papermill": {
     "duration": 0.001687,
     "end_time": "2025-07-04T03:12:10.117457",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.115770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Metadata and Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8a4578",
   "metadata": {
    "papermill": {
     "duration": 0.007024,
     "end_time": "2025-07-04T03:12:10.125926",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.118902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Run metadata loaded\n",
      "   Total bars: 16614\n",
      "   Total signals: 16601\n",
      "   Total orders: 2066\n",
      "   Total fills: 2066\n",
      "   Total positions: 2066\n",
      "\n",
      "üìÅ Global traces path: /Users/daws/ADMF-PC/traces\n",
      "\n",
      "üìä Available traces:\n",
      "   Global signals: ‚úÖ\n",
      "   Local signals: ‚ùå\n",
      "   Portfolio: ‚ùå\n",
      "   Execution: ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# Load run metadata\n",
    "metadata_path = run_dir / 'metadata.json'\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Run metadata loaded\")\n",
    "    print(f\"   Total bars: {metadata.get('total_bars', 'N/A')}\")\n",
    "    print(f\"   Total signals: {metadata.get('total_signals', 'N/A')}\")\n",
    "    print(f\"   Total orders: {metadata.get('total_orders', 'N/A')}\")\n",
    "    print(f\"   Total fills: {metadata.get('total_fills', 'N/A')}\")\n",
    "    print(f\"   Total positions: {metadata.get('total_positions', 'N/A')}\")\n",
    "    \n",
    "    # Get global traces path\n",
    "    global_traces_path = Path(metadata.get('global_traces_path', '/Users/daws/ADMF-PC/traces'))\n",
    "    print(f\"\\nüìÅ Global traces path: {global_traces_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No metadata.json found\")\n",
    "    metadata = {}\n",
    "    global_traces_path = Path('/Users/daws/ADMF-PC/traces')\n",
    "\n",
    "# Check what traces are available in global store\n",
    "store_path = global_traces_path / 'store'\n",
    "has_global_signals = store_path.exists() and any(store_path.glob('*.parquet'))\n",
    "\n",
    "# For backward compatibility, also check run directory\n",
    "traces_dir = run_dir / 'traces'\n",
    "has_local_signals = (traces_dir / 'signals').exists() if traces_dir.exists() else False\n",
    "has_portfolio = (traces_dir / 'portfolio').exists() if traces_dir.exists() else False\n",
    "has_execution = (traces_dir / 'execution').exists() if traces_dir.exists() else False\n",
    "\n",
    "print(f\"\\nüìä Available traces:\")\n",
    "print(f\"   Global signals: {'‚úÖ' if has_global_signals else '‚ùå'}\")\n",
    "print(f\"   Local signals: {'‚úÖ' if has_local_signals else '‚ùå'}\")\n",
    "print(f\"   Portfolio: {'‚úÖ' if has_portfolio else '‚ùå'}\")\n",
    "print(f\"   Execution: {'‚úÖ' if has_execution else '‚ùå'}\")\n",
    "\n",
    "# Determine trace location\n",
    "use_global_store = has_global_signals and not has_local_signals\n",
    "is_full_system = metadata.get('total_orders', 0) > 0 or has_portfolio or has_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184337b2",
   "metadata": {
    "papermill": {
     "duration": 0.001519,
     "end_time": "2025-07-04T03:12:10.129170",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.127651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35c0d2ec",
   "metadata": {
    "papermill": {
     "duration": 0.034181,
     "end_time": "2025-07-04T03:12:10.164978",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.130797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded market data from: /Users/daws/ADMF-PC/data/SPY_5m.csv\n",
      "   Date range: 2024-03-26 13:30:00+00:00 to 2025-04-02 19:20:00+00:00\n",
      "   Total bars: 20769\n"
     ]
    }
   ],
   "source": [
    "# Load market data\n",
    "market_data = None\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Try different possible locations\n",
    "        data_paths = [\n",
    "            run_dir / f'data/{symbol}_{timeframe}.csv',\n",
    "            run_dir / f'{symbol}_{timeframe}.csv',\n",
    "            run_dir.parent / f'data/{symbol}_{timeframe}.csv',\n",
    "            Path(f'/Users/daws/ADMF-PC/data/{symbol}_{timeframe}.csv')\n",
    "        ]\n",
    "        \n",
    "        for data_path in data_paths:\n",
    "            if data_path.exists():\n",
    "                market_data = pd.read_csv(data_path)\n",
    "                market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
    "                market_data = market_data.sort_values('timestamp')\n",
    "                \n",
    "                # Add derived fields\n",
    "                market_data['returns'] = market_data['close'].pct_change()\n",
    "                market_data['log_returns'] = np.log(market_data['close'] / market_data['close'].shift(1))\n",
    "                market_data['hour'] = market_data['timestamp'].dt.hour\n",
    "                market_data['minute'] = market_data['timestamp'].dt.minute\n",
    "                market_data['day_of_week'] = market_data['timestamp'].dt.dayofweek\n",
    "                \n",
    "                print(f\"‚úÖ Loaded market data from: {data_path}\")\n",
    "                print(f\"   Date range: {market_data['timestamp'].min()} to {market_data['timestamp'].max()}\")\n",
    "                print(f\"   Total bars: {len(market_data)}\")\n",
    "                break\n",
    "        \n",
    "        if market_data is not None:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol}: {e}\")\n",
    "\n",
    "if market_data is None:\n",
    "    print(\"‚ùå Could not load market data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0870d3",
   "metadata": {
    "papermill": {
     "duration": 0.001411,
     "end_time": "2025-07-04T03:12:10.167954",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.166543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3e921d",
   "metadata": {
    "papermill": {
     "duration": 0.007104,
     "end_time": "2025-07-04T03:12:10.176559",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.169455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä SIGNAL ANALYSIS\n",
      "================================================================================\n",
      "No strategy index found in run directory\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze signals if available\n",
    "if has_global_signals or has_local_signals:\n",
    "    print(\"\\nüìä SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load strategy index\n",
    "    strategy_index_path = run_dir / 'strategy_index.parquet'\n",
    "    if strategy_index_path.exists():\n",
    "        strategy_index = pd.read_parquet(strategy_index_path)\n",
    "        print(f\"Loaded {len(strategy_index)} strategies from run index\")\n",
    "        \n",
    "        # Show strategy distribution\n",
    "        by_type = strategy_index['strategy_type'].value_counts()\n",
    "        print(\"\\nStrategies by type:\")\n",
    "        for stype, count in by_type.items():\n",
    "            print(f\"  {stype}: {count}\")\n",
    "    else:\n",
    "        print(\"No strategy index found in run directory\")\n",
    "        strategy_index = pd.DataFrame()\n",
    "    \n",
    "    # Analyze signal patterns from global store\n",
    "    if use_global_store and len(strategy_index) > 0:\n",
    "        print(\"\\nüìä Analyzing signals from global store...\")\n",
    "        \n",
    "        # Get trace paths from metadata components\n",
    "        signal_counts = []\n",
    "        components = metadata.get('components', {})\n",
    "        \n",
    "        for comp_name, comp_data in components.items():\n",
    "            if comp_data.get('type') == 'strategy' and 'trace_path' in comp_data:\n",
    "                trace_path = Path(comp_data['trace_path'])\n",
    "                if trace_path.exists():\n",
    "                    signals = pd.read_parquet(trace_path)\n",
    "                    \n",
    "                    # Count actual signal changes (non-zero values)\n",
    "                    signal_changes = signals[signals['val'] != 0]\n",
    "                    \n",
    "                    signal_counts.append({\n",
    "                        'strategy_type': comp_data.get('strategy_type'),\n",
    "                        'strategy_hash': comp_data.get('strategy_hash'),\n",
    "                        'total_signals': len(signals),\n",
    "                        'signal_changes': len(signal_changes),\n",
    "                        'long_signals': (signal_changes['val'] > 0).sum(),\n",
    "                        'short_signals': (signal_changes['val'] < 0).sum(),\n",
    "                        'signals_per_1000_bars': len(signal_changes) / (metadata.get('total_bars', 1000) / 1000)\n",
    "                    })\n",
    "                    \n",
    "                    # Show sample signals\n",
    "                    if len(signal_changes) > 0:\n",
    "                        print(f\"\\n  Strategy: {comp_data.get('strategy_type')} ({comp_data.get('strategy_hash', '')[:8]})\")\n",
    "                        print(f\"    Signal changes: {len(signal_changes)}\")\n",
    "                        print(f\"    First signal: {signal_changes.iloc[0]['ts']} -> {signal_changes.iloc[0]['val']}\")\n",
    "                        print(f\"    Last signal: {signal_changes.iloc[-1]['ts']} -> {signal_changes.iloc[-1]['val']}\")\n",
    "        \n",
    "        if signal_counts:\n",
    "            signal_df = pd.DataFrame(signal_counts)\n",
    "            print(\"\\nüìä Signal frequency analysis:\")\n",
    "            print(signal_df.to_string(index=False))\n",
    "            \n",
    "            # Check if signals were generated but no trades\n",
    "            if metadata.get('total_signals', 0) > 0 and metadata.get('total_orders', 0) == 0:\n",
    "                print(\"\\n‚ö†Ô∏è WARNING: Signals were generated but no orders were created!\")\n",
    "                print(\"Possible reasons:\")\n",
    "                print(\"  - Risk constraints (stop loss/take profit) may be too tight\")\n",
    "                print(\"  - Position sizing returned 0 shares\")\n",
    "                print(\"  - Intraday constraints prevented trades\")\n",
    "                print(\"  - Check the execution logs for more details\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è No signal traces found in global store\")\n",
    "            \n",
    "    # Analyze from local traces (backward compatibility)\n",
    "    elif has_local_signals:\n",
    "        print(\"\\nüìä Analyzing signals from local traces...\")\n",
    "        # Original local trace analysis code here\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No signal traces available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e9363",
   "metadata": {
    "papermill": {
     "duration": 0.001471,
     "end_time": "2025-07-04T03:12:10.179624",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.178153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Portfolio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e3809e8",
   "metadata": {
    "papermill": {
     "duration": 0.007763,
     "end_time": "2025-07-04T03:12:10.188886",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.181123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è No portfolio traces available\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze portfolio data if available\n",
    "if has_portfolio:\n",
    "    print(\"\\nüíº PORTFOLIO ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load orders\n",
    "    orders_path = traces_dir / 'portfolio' / 'orders' / 'orders.parquet'\n",
    "    if orders_path.exists():\n",
    "        orders = pd.read_parquet(orders_path)\n",
    "        orders['timestamp'] = pd.to_datetime(orders['timestamp'])\n",
    "        print(f\"Loaded {len(orders)} orders\")\n",
    "        \n",
    "        # Order analysis\n",
    "        print(\"\\nOrder Statistics:\")\n",
    "        print(f\"  Buy orders: {(orders['side'] == 'buy').sum() if 'side' in orders else 'N/A'}\")\n",
    "        print(f\"  Sell orders: {(orders['side'] == 'sell').sum() if 'side' in orders else 'N/A'}\")\n",
    "        if 'order_type' in orders:\n",
    "            print(\"\\nOrder types:\")\n",
    "            print(orders['order_type'].value_counts())\n",
    "    else:\n",
    "        orders = pd.DataFrame()\n",
    "        print(\"No orders found\")\n",
    "    \n",
    "    # Load positions\n",
    "    positions_open_path = traces_dir / 'portfolio' / 'positions_open' / 'positions_open.parquet'\n",
    "    positions_close_path = traces_dir / 'portfolio' / 'positions_close' / 'positions_close.parquet'\n",
    "    \n",
    "    positions_opened = pd.DataFrame()\n",
    "    positions_closed = pd.DataFrame()\n",
    "    \n",
    "    if positions_open_path.exists():\n",
    "        positions_opened = pd.read_parquet(positions_open_path)\n",
    "        positions_opened['timestamp'] = pd.to_datetime(positions_opened['timestamp'])\n",
    "        print(f\"\\nLoaded {len(positions_opened)} position opens\")\n",
    "    \n",
    "    if positions_close_path.exists():\n",
    "        positions_closed = pd.read_parquet(positions_close_path)\n",
    "        positions_closed['timestamp'] = pd.to_datetime(positions_closed['timestamp'])\n",
    "        print(f\"Loaded {len(positions_closed)} position closes\")\n",
    "    \n",
    "    # Analyze positions\n",
    "    if len(positions_opened) > 0 and len(positions_closed) > 0:\n",
    "        # Match opens and closes\n",
    "        trades = []\n",
    "        for _, close in positions_closed.iterrows():\n",
    "            # Find matching open\n",
    "            position_id = close.get('position_id', close.get('metadata', {}).get('position_id'))\n",
    "            if position_id:\n",
    "                matching_opens = positions_opened[\n",
    "                    positions_opened.apply(lambda x: x.get('position_id', x.get('metadata', {}).get('position_id')) == position_id, axis=1)\n",
    "                ]\n",
    "                if len(matching_opens) > 0:\n",
    "                    open_pos = matching_opens.iloc[0]\n",
    "                    trades.append({\n",
    "                        'position_id': position_id,\n",
    "                        'symbol': close.get('symbol', close.get('metadata', {}).get('symbol')),\n",
    "                        'entry_time': open_pos['timestamp'],\n",
    "                        'exit_time': close['timestamp'],\n",
    "                        'entry_price': open_pos.get('price', open_pos.get('metadata', {}).get('entry_price', 0)),\n",
    "                        'exit_price': close.get('price', close.get('metadata', {}).get('exit_price', 0)),\n",
    "                        'quantity': close.get('metadata', {}).get('quantity', 0),\n",
    "                        'pnl': close.get('metadata', {}).get('realized_pnl', 0)\n",
    "                    })\n",
    "        \n",
    "        if trades:\n",
    "            trades_df = pd.DataFrame(trades)\n",
    "            trades_df['duration'] = (trades_df['exit_time'] - trades_df['entry_time']).dt.total_seconds() / 60  # minutes\n",
    "            trades_df['return'] = (trades_df['exit_price'] - trades_df['entry_price']) / trades_df['entry_price']\n",
    "            \n",
    "            print(\"\\nüìä Trade Statistics:\")\n",
    "            print(f\"  Total trades: {len(trades_df)}\")\n",
    "            print(f\"  Win rate: {(trades_df['pnl'] > 0).mean()*100:.1f}%\")\n",
    "            print(f\"  Average PnL: ${trades_df['pnl'].mean():.2f}\")\n",
    "            print(f\"  Total PnL: ${trades_df['pnl'].sum():.2f}\")\n",
    "            print(f\"  Average duration: {trades_df['duration'].mean():.1f} minutes\")\n",
    "            print(f\"  Average return: {trades_df['return'].mean()*100:.3f}%\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No portfolio traces available\")\n",
    "    trades_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67ff85",
   "metadata": {
    "papermill": {
     "duration": 0.001496,
     "end_time": "2025-07-04T03:12:10.191920",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.190424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98875a5c",
   "metadata": {
    "papermill": {
     "duration": 0.006239,
     "end_time": "2025-07-04T03:12:10.199647",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.193408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è No execution traces available\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze execution data if available\n",
    "if has_execution:\n",
    "    print(\"\\n‚ö° EXECUTION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load fills\n",
    "    fills_path = traces_dir / 'execution' / 'fills' / 'fills.parquet'\n",
    "    if fills_path.exists():\n",
    "        fills = pd.read_parquet(fills_path)\n",
    "        fills['timestamp'] = pd.to_datetime(fills['timestamp'])\n",
    "        print(f\"Loaded {len(fills)} fills\")\n",
    "        \n",
    "        # Extract metadata\n",
    "        if 'metadata' in fills.columns:\n",
    "            # Expand metadata column\n",
    "            fill_details = pd.json_normalize(fills['metadata'])\n",
    "            fills = pd.concat([fills, fill_details], axis=1)\n",
    "        \n",
    "        # Fill analysis\n",
    "        print(\"\\nFill Statistics:\")\n",
    "        if 'quantity' in fills:\n",
    "            print(f\"  Total volume: {fills['quantity'].sum():,.0f} shares\")\n",
    "            print(f\"  Average fill size: {fills['quantity'].mean():.0f} shares\")\n",
    "        \n",
    "        if 'price' in fills:\n",
    "            print(f\"  Average fill price: ${fills['price'].mean():.2f}\")\n",
    "        \n",
    "        # Slippage analysis\n",
    "        if analyze_slippage and 'expected_price' in fills and 'price' in fills:\n",
    "            fills['slippage'] = (fills['price'] - fills['expected_price']) / fills['expected_price']\n",
    "            fills['slippage_bps'] = fills['slippage'] * 10000\n",
    "            \n",
    "            print(\"\\nüí∏ Slippage Analysis:\")\n",
    "            print(f\"  Average slippage: {fills['slippage_bps'].mean():.1f} bps\")\n",
    "            print(f\"  Slippage std dev: {fills['slippage_bps'].std():.1f} bps\")\n",
    "            print(f\"  Positive slippage: {(fills['slippage'] > 0).mean()*100:.1f}%\")\n",
    "            print(f\"  Total slippage cost: ${(fills['slippage'] * fills['quantity'] * fills['price']).sum():.2f}\")\n",
    "        \n",
    "        # Execution cost analysis\n",
    "        if 'commission' in fills:\n",
    "            print(\"\\nüí∞ Execution Costs:\")\n",
    "            print(f\"  Total commissions: ${fills['commission'].sum():.2f}\")\n",
    "            print(f\"  Average commission: ${fills['commission'].mean():.2f}\")\n",
    "            print(f\"  Commission as % of volume: {fills['commission'].sum() / (fills['quantity'] * fills['price']).sum() * 100:.3f}%\")\n",
    "    else:\n",
    "        fills = pd.DataFrame()\n",
    "        print(\"No fills found\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No execution traces available\")\n",
    "    fills = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e521a",
   "metadata": {
    "papermill": {
     "duration": 0.001571,
     "end_time": "2025-07-04T03:12:10.202686",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.201115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d856eb25",
   "metadata": {
    "papermill": {
     "duration": 0.007206,
     "end_time": "2025-07-04T03:12:10.211384",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.204178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è No trades available for performance analysis\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall performance metrics if we have trades\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\nüìà PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate equity curve from trades\n",
    "    initial_capital = 100000  # Assumed\n",
    "    equity = initial_capital\n",
    "    equity_curve = [{'timestamp': trades_df['entry_time'].min(), 'equity': equity}]\n",
    "    \n",
    "    for _, trade in trades_df.sort_values('exit_time').iterrows():\n",
    "        equity += trade['pnl']\n",
    "        equity_curve.append({'timestamp': trade['exit_time'], 'equity': equity})\n",
    "    \n",
    "    equity_df = pd.DataFrame(equity_curve)\n",
    "    equity_df['returns'] = equity_df['equity'].pct_change()\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_return = (equity_df['equity'].iloc[-1] / initial_capital - 1)\n",
    "    \n",
    "    # Sharpe ratio (assuming daily returns)\n",
    "    daily_returns = trades_df.groupby(trades_df['exit_time'].dt.date)['pnl'].sum() / equity\n",
    "    if len(daily_returns) > 1 and daily_returns.std() > 0:\n",
    "        sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "    \n",
    "    # Max drawdown\n",
    "    cummax = equity_df['equity'].expanding().max()\n",
    "    drawdown = (equity_df['equity'] / cummax - 1)\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Win/loss statistics\n",
    "    winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "    losing_trades = trades_df[trades_df['pnl'] <= 0]\n",
    "    \n",
    "    print(f\"Total Return: {total_return*100:.2f}%\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown*100:.2f}%\")\n",
    "    print(f\"\\nTrade Statistics:\")\n",
    "    print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "    print(f\"  Average Win: ${winning_trades['pnl'].mean():.2f}\" if len(winning_trades) > 0 else \"  Average Win: N/A\")\n",
    "    print(f\"  Average Loss: ${losing_trades['pnl'].mean():.2f}\" if len(losing_trades) > 0 else \"  Average Loss: N/A\")\n",
    "    print(f\"  Profit Factor: {winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum()):.2f}\" if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else \"  Profit Factor: N/A\")\n",
    "    \n",
    "    # Performance vs thresholds\n",
    "    print(f\"\\nüéØ Performance vs Thresholds:\")\n",
    "    print(f\"  Sharpe Ratio: {sharpe_ratio:.2f} {'‚úÖ' if sharpe_ratio >= min_sharpe_ratio else '‚ùå'} (min: {min_sharpe_ratio})\")\n",
    "    print(f\"  Max Drawdown: {abs(max_drawdown)*100:.1f}% {'‚úÖ' if abs(max_drawdown) <= max_acceptable_drawdown else '‚ùå'} (max: {max_acceptable_drawdown*100:.0f}%)\")\n",
    "    print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}% {'‚úÖ' if len(winning_trades)/len(trades_df) >= min_win_rate else '‚ùå'} (min: {min_win_rate*100:.0f}%)\")\n",
    "    \n",
    "    # Plot equity curve\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(equity_df['timestamp'], equity_df['equity'])\n",
    "    plt.title('Portfolio Equity Curve')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Equity ($)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot drawdown\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.fill_between(equity_df['timestamp'], drawdown * 100, 0, alpha=0.3, color='red')\n",
    "    plt.title('Portfolio Drawdown')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Drawdown (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No trades available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af4185",
   "metadata": {
    "papermill": {
     "duration": 0.001561,
     "end_time": "2025-07-04T03:12:10.214484",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.212923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Intraday Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d23439",
   "metadata": {
    "papermill": {
     "duration": 0.006787,
     "end_time": "2025-07-04T03:12:10.222849",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.216062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze intraday patterns if requested\n",
    "if analyze_intraday_patterns and len(trades_df) > 0:\n",
    "    print(\"\\n‚è∞ INTRADAY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract hour of entry and exit\n",
    "    trades_df['entry_hour'] = trades_df['entry_time'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time'].dt.hour\n",
    "    trades_df['entry_day'] = trades_df['entry_time'].dt.dayofweek\n",
    "    \n",
    "    # Performance by hour of day\n",
    "    hourly_performance = trades_df.groupby('entry_hour').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Win rate by hour\n",
    "    hourly_win_rate = trades_df.groupby('entry_hour').apply(\n",
    "        lambda x: (x['pnl'] > 0).mean() * 100\n",
    "    )\n",
    "    \n",
    "    # Performance by day of week\n",
    "    daily_performance = trades_df.groupby('entry_day').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Trades by hour\n",
    "    ax = axes[0, 0]\n",
    "    hourly_performance['pnl']['count'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Number of Trades by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Trade Count')\n",
    "    \n",
    "    # Win rate by hour\n",
    "    ax = axes[0, 1]\n",
    "    hourly_win_rate.plot(kind='bar', ax=ax, color='green')\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Win Rate by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Win Rate (%)')\n",
    "    \n",
    "    # Average PnL by hour\n",
    "    ax = axes[1, 0]\n",
    "    hourly_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='blue')\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    # Performance by day of week\n",
    "    ax = axes[1, 1]\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    daily_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='purple')\n",
    "    ax.set_xticklabels(days[:len(daily_performance)], rotation=0)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Day of Week')\n",
    "    ax.set_xlabel('Day of Week')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best and worst times\n",
    "    print(\"\\nüïê Best Trading Hours:\")\n",
    "    best_hours = hourly_performance['pnl']['mean'].nlargest(3)\n",
    "    for hour, avg_pnl in best_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")\n",
    "    \n",
    "    print(\"\\nüïê Worst Trading Hours:\")\n",
    "    worst_hours = hourly_performance['pnl']['mean'].nsmallest(3)\n",
    "    for hour, avg_pnl in worst_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d5ecc9",
   "metadata": {
    "papermill": {
     "duration": 0.001902,
     "end_time": "2025-07-04T03:12:10.226429",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.224527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51aae68a",
   "metadata": {
    "papermill": {
     "duration": 0.00653,
     "end_time": "2025-07-04T03:12:10.234489",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.227959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprehensive risk analysis\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n‚ö†Ô∏è RISK ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Trade duration analysis\n",
    "    print(\"Trade Duration Statistics:\")\n",
    "    print(f\"  Average: {trades_df['duration'].mean():.1f} minutes\")\n",
    "    print(f\"  Median: {trades_df['duration'].median():.1f} minutes\")\n",
    "    print(f\"  Shortest: {trades_df['duration'].min():.1f} minutes\")\n",
    "    print(f\"  Longest: {trades_df['duration'].max():.1f} minutes\")\n",
    "    \n",
    "    # Consecutive wins/losses\n",
    "    trades_df['is_win'] = trades_df['pnl'] > 0\n",
    "    trades_df['streak'] = (trades_df['is_win'] != trades_df['is_win'].shift()).cumsum()\n",
    "    \n",
    "    win_streaks = trades_df[trades_df['is_win']].groupby('streak').size()\n",
    "    loss_streaks = trades_df[~trades_df['is_win']].groupby('streak').size()\n",
    "    \n",
    "    print(f\"\\nStreak Analysis:\")\n",
    "    print(f\"  Max consecutive wins: {win_streaks.max() if len(win_streaks) > 0 else 0}\")\n",
    "    print(f\"  Max consecutive losses: {loss_streaks.max() if len(loss_streaks) > 0 else 0}\")\n",
    "    print(f\"  Average win streak: {win_streaks.mean():.1f}\" if len(win_streaks) > 0 else \"  Average win streak: N/A\")\n",
    "    print(f\"  Average loss streak: {loss_streaks.mean():.1f}\" if len(loss_streaks) > 0 else \"  Average loss streak: N/A\")\n",
    "    \n",
    "    # Risk-adjusted returns\n",
    "    if trades_df['return'].std() > 0:\n",
    "        information_ratio = trades_df['return'].mean() / trades_df['return'].std()\n",
    "        print(f\"\\nRisk-Adjusted Metrics:\")\n",
    "        print(f\"  Information Ratio: {information_ratio:.3f}\")\n",
    "        print(f\"  Return/Risk: {trades_df['return'].mean() / trades_df['return'].std():.3f}\")\n",
    "    \n",
    "    # Value at Risk (VaR)\n",
    "    var_95 = np.percentile(trades_df['pnl'], 5)\n",
    "    var_99 = np.percentile(trades_df['pnl'], 1)\n",
    "    \n",
    "    print(f\"\\nValue at Risk (VaR):\")\n",
    "    print(f\"  95% VaR: ${var_95:.2f}\")\n",
    "    print(f\"  99% VaR: ${var_99:.2f}\")\n",
    "    \n",
    "    # Plot PnL distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(trades_df['pnl'], bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "    plt.axvline(trades_df['pnl'].mean(), color='green', linestyle='--', alpha=0.5, label='Mean PnL')\n",
    "    plt.axvline(var_95, color='orange', linestyle='--', alpha=0.5, label='95% VaR')\n",
    "    plt.xlabel('PnL ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('PnL Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71beaff",
   "metadata": {
    "papermill": {
     "duration": 0.001562,
     "end_time": "2025-07-04T03:12:10.237651",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.236089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84f1f48b",
   "metadata": {
    "papermill": {
     "duration": 0.007742,
     "end_time": "2025-07-04T03:12:10.246899",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.239157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SUMMARY AND RECOMMENDATIONS\n",
      "================================================================================\n",
      "‚úÖ No critical issues identified\n",
      "\n",
      "üìÑ Analysis summary saved to: analysis_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Generate summary and recommendations\n",
    "print(\"\\nüìã SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'run_info': {\n",
    "        'run_id': run_dir.name,\n",
    "        'config_name': config_name,\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'is_full_system': is_full_system\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'total_bars': metadata.get('total_bars', 0),\n",
    "        'total_signals': metadata.get('total_signals', 0),\n",
    "        'total_orders': metadata.get('total_orders', 0),\n",
    "        'total_fills': metadata.get('total_fills', 0),\n",
    "        'total_positions': metadata.get('total_positions', 0)\n",
    "    },\n",
    "    'performance_summary': {},\n",
    "    'risk_summary': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "if len(trades_df) > 0:\n",
    "    # Performance summary\n",
    "    summary['performance_summary'] = {\n",
    "        'total_trades': len(trades_df),\n",
    "        'total_return': float(total_return),\n",
    "        'sharpe_ratio': float(sharpe_ratio),\n",
    "        'max_drawdown': float(max_drawdown),\n",
    "        'win_rate': float(len(winning_trades)/len(trades_df)),\n",
    "        'profit_factor': float(winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())) if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Risk summary\n",
    "    summary['risk_summary'] = {\n",
    "        'var_95': float(var_95),\n",
    "        'var_99': float(var_99),\n",
    "        'max_consecutive_losses': int(loss_streaks.max()) if len(loss_streaks) > 0 else 0,\n",
    "        'avg_trade_duration_minutes': float(trades_df['duration'].mean())\n",
    "    }\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if sharpe_ratio < min_sharpe_ratio:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'high',\n",
    "            'message': f'Sharpe ratio ({sharpe_ratio:.2f}) below minimum threshold ({min_sharpe_ratio}). Consider parameter optimization.'\n",
    "        })\n",
    "    \n",
    "    if abs(max_drawdown) > max_acceptable_drawdown:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'risk',\n",
    "            'severity': 'high',\n",
    "            'message': f'Maximum drawdown ({abs(max_drawdown)*100:.1f}%) exceeds acceptable limit ({max_acceptable_drawdown*100:.0f}%). Implement stricter risk controls.'\n",
    "        })\n",
    "    \n",
    "    if len(winning_trades)/len(trades_df) < min_win_rate:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'medium',\n",
    "            'message': f'Win rate ({len(winning_trades)/len(trades_df)*100:.1f}%) below minimum ({min_win_rate*100:.0f}%). Review entry criteria.'\n",
    "        })\n",
    "    \n",
    "    # Execution-specific recommendations\n",
    "    if 'slippage_bps' in fills.columns and fills['slippage_bps'].mean() > 5:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'execution',\n",
    "            'severity': 'medium',\n",
    "            'message': f'High average slippage ({fills[\"slippage_bps\"].mean():.1f} bps). Consider limit orders or better execution timing.'\n",
    "        })\n",
    "    \n",
    "    # Intraday pattern recommendations\n",
    "    if analyze_intraday_patterns and 'hourly_performance' in locals():\n",
    "        worst_hour = hourly_performance['pnl']['mean'].idxmin()\n",
    "        if hourly_performance.loc[worst_hour, ('pnl', 'mean')] < -50:\n",
    "            summary['recommendations'].append({\n",
    "                'type': 'timing',\n",
    "                'severity': 'low',\n",
    "                'message': f'Poor performance at {worst_hour}:00. Consider avoiding trades during this hour.'\n",
    "            })\n",
    "\n",
    "# Display recommendations\n",
    "if summary['recommendations']:\n",
    "    print(\"üéØ Recommendations:\")\n",
    "    for rec in sorted(summary['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']]):\n",
    "        severity_icon = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}[rec['severity']]\n",
    "        print(f\"\\n{severity_icon} [{rec['severity'].upper()}] {rec['type'].title()}\")\n",
    "        print(f\"   {rec['message']}\")\n",
    "else:\n",
    "    print(\"‚úÖ No critical issues identified\")\n",
    "\n",
    "# Save summary\n",
    "with open(run_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìÑ Analysis summary saved to: analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f740e97",
   "metadata": {
    "papermill": {
     "duration": 0.001555,
     "end_time": "2025-07-04T03:12:10.250050",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.248495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "557202ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T03:12:10.253899Z",
     "iopub.status.busy": "2025-07-04T03:12:10.253798Z",
     "iopub.status.idle": "2025-07-04T03:12:10.256803Z",
     "shell.execute_reply": "2025-07-04T03:12:10.256596Z"
    },
    "papermill": {
     "duration": 0.005611,
     "end_time": "2025-07-04T03:12:10.257396",
     "exception": false,
     "start_time": "2025-07-04T03:12:10.251785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ EXPORTING RESULTS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Analysis complete! Results saved to /Users/daws/ADMF-PC/config/bollinger/results/20250703_201201\n"
     ]
    }
   ],
   "source": [
    "# Export key dataframes for further analysis\n",
    "print(\"\\nüíæ EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exports = {}\n",
    "\n",
    "# Export trades if available\n",
    "if len(trades_df) > 0:\n",
    "    trades_df.to_csv(run_dir / 'analyzed_trades.csv', index=False)\n",
    "    exports['trades'] = 'analyzed_trades.csv'\n",
    "    print(f\"‚úÖ Exported {len(trades_df)} trades\")\n",
    "\n",
    "# Export fills analysis if available\n",
    "if len(fills) > 0 and 'slippage_bps' in fills.columns:\n",
    "    slippage_summary = fills[['timestamp', 'symbol', 'quantity', 'price', 'expected_price', 'slippage_bps']]\n",
    "    slippage_summary.to_csv(run_dir / 'slippage_analysis.csv', index=False)\n",
    "    exports['slippage'] = 'slippage_analysis.csv'\n",
    "    print(f\"‚úÖ Exported slippage analysis for {len(fills)} fills\")\n",
    "\n",
    "# Export performance metrics\n",
    "if 'equity_df' in locals():\n",
    "    equity_df.to_csv(run_dir / 'equity_curve.csv', index=False)\n",
    "    exports['equity_curve'] = 'equity_curve.csv'\n",
    "    print(f\"‚úÖ Exported equity curve\")\n",
    "\n",
    "# Create final report\n",
    "report = {\n",
    "    'analysis_complete': True,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'exports': exports,\n",
    "    'summary': summary\n",
    "}\n",
    "\n",
    "with open(run_dir / 'final_report.json', 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete! Results saved to {run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.767573,
   "end_time": "2025-07-04T03:12:10.475010",
   "environment_variables": {},
   "exception": null,
   "input_path": "/Users/daws/ADMF-PC/src/analytics/templates/analysis.ipynb",
   "output_path": "config/bollinger/results/20250703_201201/analysis_20250703_201208.ipynb",
   "parameters": {
    "calculate_all_performance": true,
    "config_name": "bollinger",
    "correlation_threshold": 0.7,
    "ensemble_size": 5,
    "global_traces_dir": "/Users/daws/ADMF-PC/traces",
    "min_strategies_to_analyze": 20,
    "performance_limit": 100,
    "run_dir": "/Users/daws/ADMF-PC/config/bollinger/results/20250703_201201",
    "sharpe_threshold": 1,
    "symbols": [
     "SPY"
    ],
    "timeframe": "5m",
    "top_n_strategies": 10
   },
   "start_time": "2025-07-04T03:12:08.707437",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
