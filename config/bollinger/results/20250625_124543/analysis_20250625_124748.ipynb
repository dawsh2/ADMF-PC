{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ef0271d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52126fa",
   "metadata": {
    "papermill": {
     "duration": 0.018303,
     "end_time": "2025-06-25T19:47:49.752901",
     "exception": false,
     "start_time": "2025-06-25T19:47:49.734598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Universal Strategy Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis across all strategies tested in a parameter sweep.\n",
    "\n",
    "**Key Features:**\n",
    "- Cross-strategy performance comparison\n",
    "- Parameter sensitivity analysis\n",
    "- Correlation analysis for ensemble building\n",
    "- Regime-specific performance breakdown\n",
    "- Automatic identification of optimal strategies and ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339ed736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:49.773076Z",
     "iopub.status.busy": "2025-06-25T19:47:49.772813Z",
     "iopub.status.idle": "2025-06-25T19:47:49.777615Z",
     "shell.execute_reply": "2025-06-25T19:47:49.777316Z"
    },
    "papermill": {
     "duration": 0.014377,
     "end_time": "2025-06-25T19:47:49.778473",
     "exception": false,
     "start_time": "2025-06-25T19:47:49.764096",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "min_strategies_to_analyze = 20\n",
    "sharpe_threshold = 1.0\n",
    "correlation_threshold = 0.7\n",
    "top_n_strategies = 10\n",
    "ensemble_size = 5\n",
    "calculate_all_performance = True  # Set to False to limit analysis for large sweeps\n",
    "performance_limit = 100  # If calculate_all_performance is False, limit to this many\n",
    "\n",
    "# Enhanced analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_stop_losses = True  # Whether to analyze stop loss impact\n",
    "stop_loss_levels = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.75, 1.0]  # Stop loss percentages\n",
    "verify_intraday = True  # Whether to verify intraday constraints\n",
    "market_timezone = \"America/New_York\"  # Market timezone for constraint verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0efb0fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:49.791381Z",
     "iopub.status.busy": "2025-06-25T19:47:49.791251Z",
     "iopub.status.idle": "2025-06-25T19:47:49.793380Z",
     "shell.execute_reply": "2025-06-25T19:47:49.793109Z"
    },
    "papermill": {
     "duration": 0.009146,
     "end_time": "2025-06-25T19:47:49.794125",
     "exception": false,
     "start_time": "2025-06-25T19:47:49.784979",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_dir = \"/Users/daws/ADMF-PC/config/bollinger/results/20250625_124543\"\n",
    "config_name = \"bollinger\"\n",
    "symbols = [\"SPY_5m\"]\n",
    "timeframe = \"5m\"\n",
    "min_strategies_to_analyze = 20\n",
    "sharpe_threshold = 1.0\n",
    "correlation_threshold = 0.7\n",
    "top_n_strategies = 10\n",
    "ensemble_size = 5\n",
    "calculate_all_performance = True\n",
    "performance_limit = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc445b22",
   "metadata": {
    "papermill": {
     "duration": 0.005628,
     "end_time": "2025-06-25T19:47:49.805357",
     "exception": false,
     "start_time": "2025-06-25T19:47:49.799729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d4f7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:49.815965Z",
     "iopub.status.busy": "2025-06-25T19:47:49.815757Z",
     "iopub.status.idle": "2025-06-25T19:47:50.583449Z",
     "shell.execute_reply": "2025-06-25T19:47:50.583183Z"
    },
    "papermill": {
     "duration": 0.773447,
     "end_time": "2025-06-25T19:47:50.584126",
     "exception": false,
     "start_time": "2025-06-25T19:47:49.810679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing run: 20250625_124543\n",
      "Full path: /Users/daws/ADMF-PC/config/bollinger/results/20250625_124543\n",
      "Config: bollinger\n",
      "Symbol(s): ['SPY_5m']\n",
      "Timeframe: 5m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import duckdb\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Initialize DuckDB\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Convert run_dir to Path and resolve to absolute path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62aae28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:50.594130Z",
     "iopub.status.busy": "2025-06-25T19:47:50.593965Z",
     "iopub.status.idle": "2025-06-25T19:47:50.602998Z",
     "shell.execute_reply": "2025-06-25T19:47:50.602784Z"
    },
    "papermill": {
     "duration": 0.014534,
     "end_time": "2025-06-25T19:47:50.603619",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.589085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enhanced analysis helper functions\n",
    "import pytz\n",
    "from datetime import time\n",
    "\n",
    "def extract_trades(strategy_hash, trace_path, market_data, execution_cost_bps=1.0):\n",
    "    \"\"\"\n",
    "    Extract trades from signal trace with execution costs.\n",
    "    \n",
    "    Args:\n",
    "        strategy_hash: Strategy identifier\n",
    "        trace_path: Path to trace file\n",
    "        market_data: Market price data\n",
    "        execution_cost_bps: Round-trip execution cost in basis points (default 1bp)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with trade details including costs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        signals_path = run_dir / trace_path\n",
    "        signals = pd.read_parquet(signals_path)\n",
    "        signals['ts'] = pd.to_datetime(signals['ts'])\n",
    "        \n",
    "        # Merge with market data\n",
    "        df = market_data.merge(\n",
    "            signals[['ts', 'val', 'px']], \n",
    "            left_on='timestamp', \n",
    "            right_on='ts', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Forward fill signals\n",
    "        df['signal'] = df['val'].ffill().fillna(0)\n",
    "        df['position'] = df['signal'].replace({0: 0, 1: 1, -1: -1})\n",
    "        df['position_change'] = df['position'].diff().fillna(0)\n",
    "        \n",
    "        trades = []\n",
    "        current_trade = None\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if row['position_change'] != 0 and row['position'] != 0:\n",
    "                # New position opened\n",
    "                if current_trade is None:\n",
    "                    current_trade = {\n",
    "                        'entry_time': row['timestamp'],\n",
    "                        'entry_price': row['px'] if pd.notna(row['px']) else row['close'],\n",
    "                        'direction': row['position'],\n",
    "                        'entry_idx': idx\n",
    "                    }\n",
    "            elif current_trade is not None and (row['position'] == 0 or row['position_change'] != 0):\n",
    "                # Position closed\n",
    "                exit_price = row['px'] if pd.notna(row['px']) else row['close']\n",
    "                \n",
    "                # Calculate raw return\n",
    "                if current_trade['direction'] == 1:  # Long\n",
    "                    raw_return = (exit_price - current_trade['entry_price']) / current_trade['entry_price']\n",
    "                else:  # Short\n",
    "                    raw_return = (current_trade['entry_price'] - exit_price) / current_trade['entry_price']\n",
    "                \n",
    "                # Apply execution costs\n",
    "                cost_adjustment = execution_cost_bps / 10000  # Convert bps to decimal\n",
    "                net_return = raw_return - cost_adjustment\n",
    "                \n",
    "                trade = {\n",
    "                    'strategy_hash': strategy_hash,\n",
    "                    'entry_time': current_trade['entry_time'],\n",
    "                    'exit_time': row['timestamp'],\n",
    "                    'entry_price': current_trade['entry_price'],\n",
    "                    'exit_price': exit_price,\n",
    "                    'direction': current_trade['direction'],\n",
    "                    'raw_return': raw_return,\n",
    "                    'execution_cost': cost_adjustment,\n",
    "                    'net_return': net_return,\n",
    "                    'duration_minutes': (row['timestamp'] - current_trade['entry_time']).total_seconds() / 60\n",
    "                }\n",
    "                trades.append(trade)\n",
    "                \n",
    "                # Reset for next trade\n",
    "                current_trade = None\n",
    "                if row['position'] != 0 and row['position_change'] != 0:\n",
    "                    # Immediately open new position (reversal)\n",
    "                    current_trade = {\n",
    "                        'entry_time': row['timestamp'],\n",
    "                        'entry_price': row['px'] if pd.notna(row['px']) else row['close'],\n",
    "                        'direction': row['position'],\n",
    "                        'entry_idx': idx\n",
    "                    }\n",
    "        \n",
    "        return pd.DataFrame(trades)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting trades for {strategy_hash[:8]}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_stop_loss_impact(trades_df, stop_loss_levels=None):\n",
    "    \"\"\"\n",
    "    Calculate returns with various stop loss levels.\n",
    "    \n",
    "    Args:\n",
    "        trades_df: DataFrame of trades\n",
    "        stop_loss_levels: List of stop loss percentages (default 0.05% to 1%)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with returns for each stop loss level\n",
    "    \"\"\"\n",
    "    if stop_loss_levels is None:\n",
    "        stop_loss_levels = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.75, 1.0]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for sl_pct in stop_loss_levels:\n",
    "        sl_decimal = sl_pct / 100\n",
    "        \n",
    "        trades_with_sl = trades_df.copy()\n",
    "        stopped_out_count = 0\n",
    "        \n",
    "        # Apply stop loss to each trade\n",
    "        for idx, trade in trades_with_sl.iterrows():\n",
    "            if trade['raw_return'] < -sl_decimal:\n",
    "                trades_with_sl.loc[idx, 'raw_return'] = -sl_decimal\n",
    "                trades_with_sl.loc[idx, 'net_return'] = -sl_decimal - trade['execution_cost']\n",
    "                stopped_out_count += 1\n",
    "        \n",
    "        # Calculate metrics with stop loss\n",
    "        total_return = trades_with_sl['net_return'].sum()\n",
    "        avg_return = trades_with_sl['net_return'].mean()\n",
    "        win_rate = (trades_with_sl['net_return'] > 0).mean()\n",
    "        \n",
    "        results.append({\n",
    "            'stop_loss_pct': sl_pct,\n",
    "            'total_return': total_return,\n",
    "            'avg_return_per_trade': avg_return,\n",
    "            'win_rate': win_rate,\n",
    "            'stopped_out_count': stopped_out_count,\n",
    "            'stopped_out_rate': stopped_out_count / len(trades_with_sl) if len(trades_with_sl) > 0 else 0,\n",
    "            'num_trades': len(trades_with_sl)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def verify_intraday_constraint(trades_df, market_tz='America/New_York'):\n",
    "    \"\"\"\n",
    "    Verify that trades respect intraday constraints.\n",
    "    \n",
    "    Args:\n",
    "        trades_df: DataFrame of trades\n",
    "        market_tz: Market timezone (default NYSE)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with constraint verification results\n",
    "    \"\"\"\n",
    "    if len(trades_df) == 0:\n",
    "        return {\n",
    "            'total_trades': 0,\n",
    "            'overnight_positions': 0,\n",
    "            'overnight_position_pct': 0,\n",
    "            'after_hours_entries': 0,\n",
    "            'after_hours_exits': 0,\n",
    "            'fully_intraday': 0,\n",
    "            'avg_trade_duration_minutes': 0,\n",
    "            'max_trade_duration_minutes': 0,\n",
    "            'trades_over_390_minutes': 0\n",
    "        }\n",
    "    \n",
    "    # Convert to market timezone\n",
    "    market_tz_obj = pytz.timezone(market_tz)\n",
    "    \n",
    "    trades_df = trades_df.copy()\n",
    "    trades_df['entry_time_mkt'] = pd.to_datetime(trades_df['entry_time']).dt.tz_localize('UTC').dt.tz_convert(market_tz_obj)\n",
    "    trades_df['exit_time_mkt'] = pd.to_datetime(trades_df['exit_time']).dt.tz_localize('UTC').dt.tz_convert(market_tz_obj)\n",
    "    \n",
    "    # Market hours (9:30 AM - 4:00 PM ET)\n",
    "    market_open = time(9, 30)\n",
    "    market_close = time(16, 0)\n",
    "    \n",
    "    # Check for overnight positions\n",
    "    trades_df['entry_date'] = trades_df['entry_time_mkt'].dt.date\n",
    "    trades_df['exit_date'] = trades_df['exit_time_mkt'].dt.date\n",
    "    trades_df['overnight'] = trades_df['entry_date'] != trades_df['exit_date']\n",
    "    \n",
    "    # Check for after-hours trades\n",
    "    trades_df['entry_time_only'] = trades_df['entry_time_mkt'].dt.time\n",
    "    trades_df['exit_time_only'] = trades_df['exit_time_mkt'].dt.time\n",
    "    \n",
    "    trades_df['after_hours_entry'] = (\n",
    "        (trades_df['entry_time_only'] < market_open) | \n",
    "        (trades_df['entry_time_only'] >= market_close)\n",
    "    )\n",
    "    trades_df['after_hours_exit'] = (\n",
    "        (trades_df['exit_time_only'] < market_open) | \n",
    "        (trades_df['exit_time_only'] >= market_close)\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'total_trades': len(trades_df),\n",
    "        'overnight_positions': trades_df['overnight'].sum(),\n",
    "        'overnight_position_pct': trades_df['overnight'].mean() * 100,\n",
    "        'after_hours_entries': trades_df['after_hours_entry'].sum(),\n",
    "        'after_hours_exits': trades_df['after_hours_exit'].sum(),\n",
    "        'fully_intraday': (~trades_df['overnight']).sum(),\n",
    "        'avg_trade_duration_minutes': trades_df['duration_minutes'].mean(),\n",
    "        'max_trade_duration_minutes': trades_df['duration_minutes'].max(),\n",
    "        'trades_over_390_minutes': (trades_df['duration_minutes'] > 390).sum()  # Full trading day\n",
    "    }\n",
    "    \n",
    "    # Add hourly breakdown\n",
    "    trades_df['entry_hour'] = trades_df['entry_time_mkt'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time_mkt'].dt.hour\n",
    "    \n",
    "    results['entries_by_hour'] = trades_df['entry_hour'].value_counts().to_dict()\n",
    "    results['exits_by_hour'] = trades_df['exit_hour'].value_counts().to_dict()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb74db5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:50.612992Z",
     "iopub.status.busy": "2025-06-25T19:47:50.612894Z",
     "iopub.status.idle": "2025-06-25T19:47:50.616604Z",
     "shell.execute_reply": "2025-06-25T19:47:50.616402Z"
    },
    "papermill": {
     "duration": 0.009055,
     "end_time": "2025-06-25T19:47:50.617176",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.608121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found project root: /Users/daws/ADMF-PC\n",
      "âœ… Analysis snippets available at: /Users/daws/ADMF-PC/src/analytics/snippets\n",
      "âœ… SQL queries available at: /Users/daws/ADMF-PC/src/analytics/queries\n",
      "\n",
      "Use %load to load any snippet, e.g.:\n",
      "  %load /Users/daws/ADMF-PC/src/analytics/snippets/exploratory/signal_frequency.py\n",
      "  %load /Users/daws/ADMF-PC/src/analytics/snippets/ensembles/find_uncorrelated.py\n"
     ]
    }
   ],
   "source": [
    "# Setup path for loading analysis snippets\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the project root (where src/ directory is)\n",
    "current_path = Path(run_dir).resolve()\n",
    "project_root = None\n",
    "\n",
    "# Search up the directory tree for src/analytics/snippets\n",
    "for parent in current_path.parents:\n",
    "    if (parent / 'src' / 'analytics' / 'snippets').exists():\n",
    "        project_root = parent\n",
    "        break\n",
    "\n",
    "# If not found from run_dir, try from current working directory\n",
    "if not project_root:\n",
    "    cwd = Path.cwd()\n",
    "    for parent in [cwd] + list(cwd.parents):\n",
    "        if (parent / 'src' / 'analytics' / 'snippets').exists():\n",
    "            project_root = parent\n",
    "            break\n",
    "\n",
    "# Last resort: check common project locations\n",
    "if not project_root:\n",
    "    common_roots = [\n",
    "        Path('/Users/daws/ADMF-PC'),\n",
    "        Path.home() / 'ADMF-PC',\n",
    "        Path.cwd().parent.parent.parent.parent  # 4 levels up from typical results dir\n",
    "    ]\n",
    "    for root in common_roots:\n",
    "        if root.exists() and (root / 'src' / 'analytics' / 'snippets').exists():\n",
    "            project_root = root\n",
    "            break\n",
    "\n",
    "if project_root:\n",
    "    # Add to Python path if not already there\n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    snippets_path = project_root / 'src' / 'analytics' / 'snippets'\n",
    "    queries_path = project_root / 'src' / 'analytics' / 'queries'\n",
    "    print(f\"âœ… Found project root: {project_root}\")\n",
    "    print(f\"âœ… Analysis snippets available at: {snippets_path}\")\n",
    "    print(f\"âœ… SQL queries available at: {queries_path}\")\n",
    "    print(\"\\nUse %load to load any snippet, e.g.:\")\n",
    "    print(\"  %load {}/src/analytics/snippets/exploratory/signal_frequency.py\".format(project_root))\n",
    "    print(\"  %load {}/src/analytics/snippets/ensembles/find_uncorrelated.py\".format(project_root))\n",
    "else:\n",
    "    print(\"âš ï¸ Could not find project root with src/analytics/snippets\")\n",
    "    print(f\"  Searched from: {current_path}\")\n",
    "    print(f\"  Current working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e575b",
   "metadata": {
    "papermill": {
     "duration": 0.004642,
     "end_time": "2025-06-25T19:47:50.626403",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.621761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Strategy Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52117a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:50.636386Z",
     "iopub.status.busy": "2025-06-25T19:47:50.636276Z",
     "iopub.status.idle": "2025-06-25T19:47:50.700165Z",
     "shell.execute_reply": "2025-06-25T19:47:50.699933Z"
    },
    "papermill": {
     "duration": 0.069714,
     "end_time": "2025-06-25T19:47:50.700826",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.631112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 205 strategies from /Users/daws/ADMF-PC/config/bollinger/results/20250625_124543/strategy_index.parquet\n",
      "\n",
      "Strategies by type:\n",
      "  bollinger_bands: 205\n",
      "\n",
      "Columns: ['strategy_id', 'strategy_hash', 'strategy_type', 'symbol', 'timeframe', 'constraints', 'period', 'std_dev', 'trace_path']...\n"
     ]
    }
   ],
   "source": [
    "# Load strategy index - the catalog of all strategies tested\n",
    "strategy_index_path = run_dir / 'strategy_index.parquet'\n",
    "\n",
    "if strategy_index_path.exists():\n",
    "    strategy_index = pd.read_parquet(strategy_index_path)\n",
    "    print(f\"âœ… Loaded {len(strategy_index)} strategies from {strategy_index_path}\")\n",
    "    \n",
    "    # Show strategy type distribution\n",
    "    by_type = strategy_index['strategy_type'].value_counts()\n",
    "    print(\"\\nStrategies by type:\")\n",
    "    for stype, count in by_type.items():\n",
    "        print(f\"  {stype}: {count}\")\n",
    "        \n",
    "    # Show sample of columns\n",
    "    print(f\"\\nColumns: {list(strategy_index.columns)[:10]}...\")\n",
    "else:\n",
    "    print(f\"âŒ No strategy_index.parquet found at {strategy_index_path}\")\n",
    "    strategy_index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4477f",
   "metadata": {
    "papermill": {
     "duration": 0.005348,
     "end_time": "2025-06-25T19:47:50.711394",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.706046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_performance(strategy_hash, trace_path, market_data, execution_cost_bps=1.0):\n",
    "    \"\"\"Calculate performance metrics for a strategy with execution costs\"\"\"\n",
    "    try:\n",
    "        # Always use the global run_dir which is already resolved to absolute path\n",
    "        signals_path = run_dir / trace_path\n",
    "            \n",
    "        # Load sparse signals\n",
    "        signals = pd.read_parquet(signals_path)\n",
    "        signals['ts'] = pd.to_datetime(signals['ts'])\n",
    "        \n",
    "        # Merge with market data\n",
    "        df = market_data.merge(\n",
    "            signals[['ts', 'val']], \n",
    "            left_on='timestamp', \n",
    "            right_on='ts', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Forward fill signals (sparse to dense)\n",
    "        df['signal'] = df['val'].ffill().fillna(0)\n",
    "        \n",
    "        # Calculate returns\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['strategy_returns'] = df['returns'] * df['signal'].shift(1)\n",
    "        \n",
    "        # Count trades for execution cost calculation\n",
    "        df['position'] = df['signal']\n",
    "        df['position_change'] = df['position'].diff().fillna(0)\n",
    "        trades = (df['position_change'] != 0).sum()\n",
    "        \n",
    "        # Apply execution costs\n",
    "        # Each trade incurs cost (half on entry, half on exit)\n",
    "        cost_per_return = (execution_cost_bps / 10000) * trades / len(df[df['strategy_returns'] != 0])\n",
    "        df['strategy_returns_net'] = df['strategy_returns'] - cost_per_return\n",
    "        df['cum_returns'] = (1 + df['strategy_returns_net']).cumprod()\n",
    "        \n",
    "        # Metrics (using net returns)\n",
    "        total_return = df['cum_returns'].iloc[-1] - 1\n",
    "        \n",
    "        if df['strategy_returns_net'].std() > 0:\n",
    "            sharpe = df['strategy_returns_net'].mean() / df['strategy_returns_net'].std() * np.sqrt(252 * 78)\n",
    "        else:\n",
    "            sharpe = 0\n",
    "            \n",
    "        cummax = df['cum_returns'].expanding().max()\n",
    "        drawdown = (df['cum_returns'] / cummax - 1)\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        # Calculate win rate\n",
    "        positive_returns = df[df['strategy_returns_net'] > 0]['strategy_returns_net']\n",
    "        negative_returns = df[df['strategy_returns_net'] < 0]['strategy_returns_net']\n",
    "        win_rate = len(positive_returns) / (len(positive_returns) + len(negative_returns)) if (len(positive_returns) + len(negative_returns)) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'num_trades': trades,\n",
    "            'win_rate': win_rate,\n",
    "            'avg_return_per_trade': total_return / trades if trades > 0 else 0,\n",
    "            'total_execution_cost': (execution_cost_bps / 10000) * trades,\n",
    "            'df': df  # For later analysis\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating performance for {strategy_hash}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecdbd11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:50.722855Z",
     "iopub.status.busy": "2025-06-25T19:47:50.722742Z",
     "iopub.status.idle": "2025-06-25T19:47:50.725791Z",
     "shell.execute_reply": "2025-06-25T19:47:50.725582Z"
    },
    "papermill": {
     "duration": 0.008703,
     "end_time": "2025-06-25T19:47:50.726395",
     "exception": false,
     "start_time": "2025-06-25T19:47:50.717692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_performance(strategy_hash, trace_path, market_data):\n",
    "    \"\"\"Calculate performance metrics for a strategy\"\"\"\n",
    "    try:\n",
    "        # Always use the global run_dir which is already resolved to absolute path\n",
    "        signals_path = run_dir / trace_path\n",
    "            \n",
    "        # Load sparse signals\n",
    "        signals = pd.read_parquet(signals_path)\n",
    "        signals['ts'] = pd.to_datetime(signals['ts'])\n",
    "        \n",
    "        # Merge with market data\n",
    "        df = market_data.merge(\n",
    "            signals[['ts', 'val']], \n",
    "            left_on='timestamp', \n",
    "            right_on='ts', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Forward fill signals (sparse to dense)\n",
    "        df['signal'] = df['val'].ffill().fillna(0)\n",
    "        \n",
    "        # Calculate returns\n",
    "        df['returns'] = df['close'].pct_change()\n",
    "        df['strategy_returns'] = df['returns'] * df['signal'].shift(1)\n",
    "        df['cum_returns'] = (1 + df['strategy_returns']).cumprod()\n",
    "        \n",
    "        # Metrics\n",
    "        total_return = df['cum_returns'].iloc[-1] - 1\n",
    "        \n",
    "        if df['strategy_returns'].std() > 0:\n",
    "            sharpe = df['strategy_returns'].mean() / df['strategy_returns'].std() * np.sqrt(252 * 78)\n",
    "        else:\n",
    "            sharpe = 0\n",
    "            \n",
    "        cummax = df['cum_returns'].expanding().max()\n",
    "        drawdown = (df['cum_returns'] / cummax - 1)\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        # Count trades\n",
    "        trades = (df['signal'] != df['signal'].shift()).sum()\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe,\n",
    "            'max_drawdown': max_dd,\n",
    "            'num_trades': trades,\n",
    "            'df': df  # For later analysis\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating performance for {strategy_hash}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8d37",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90183cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T19:47:50.736235Z",
     "iopub.status.busy": "2025-06-25T19:47:50.736120Z",
     "iopub.status.idle": "2025-06-25T19:47:50.837244Z",
     "shell.execute_reply": "2025-06-25T19:47:50.836742Z"
    },
    "papermill": {
     "duration": 0.106931,
     "end_time": "2025-06-25T19:47:50.838020",
     "exception": true,
     "start_time": "2025-06-25T19:47:50.731089",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'market_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate performance for all strategies with execution costs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strategy_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mmarket_data\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      3\u001b[39m     performance_results = []\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Determine strategies to analyze based on parameters\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'market_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate performance for all strategies with execution costs\n",
    "if strategy_index is not None and market_data is not None:\n",
    "    performance_results = []\n",
    "    \n",
    "    # Determine strategies to analyze based on parameters\n",
    "    strategies_to_analyze = strategy_index\n",
    "    \n",
    "    if not calculate_all_performance and len(strategy_index) > performance_limit:\n",
    "        print(f\"Note: Large parameter sweep detected ({len(strategy_index)} strategies)\")\n",
    "        print(f\"Limiting analysis to {performance_limit} strategies (set calculate_all_performance=True to analyze all)\")\n",
    "        \n",
    "        # Sample diverse strategies across all types\n",
    "        strategies_to_analyze = strategy_index.groupby('strategy_type').apply(\n",
    "            lambda x: x.sample(n=min(len(x), performance_limit // strategy_index['strategy_type'].nunique()), \n",
    "                             random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nCalculating performance for {len(strategies_to_analyze)} strategies...\")\n",
    "    print(f\"Using run directory: {run_dir}\")\n",
    "    print(f\"Execution cost: {execution_cost_bps} basis points round-trip\")\n",
    "    \n",
    "    # Check if we already have cached performance metrics\n",
    "    cached_performance_path = run_dir / 'performance_metrics.parquet'\n",
    "    if cached_performance_path.exists() and calculate_all_performance:\n",
    "        print(f\"ðŸ“‚ Found cached performance metrics, loading...\")\n",
    "        performance_df = pd.read_parquet(cached_performance_path)\n",
    "        print(f\"âœ… Loaded performance for {len(performance_df)} strategies from cache\")\n",
    "    else:\n",
    "        # Calculate performance\n",
    "        for idx, row in strategies_to_analyze.iterrows():\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"  Progress: {idx}/{len(strategies_to_analyze)} ({idx/len(strategies_to_analyze)*100:.1f}%)\")\n",
    "                \n",
    "            perf = calculate_performance(row['strategy_hash'], row['trace_path'], market_data, execution_cost_bps)\n",
    "            \n",
    "            if perf:\n",
    "                # Combine strategy info with performance\n",
    "                result = {**row.to_dict(), **perf}\n",
    "                # Remove the full dataframe from results\n",
    "                result.pop('df', None)\n",
    "                performance_results.append(result)\n",
    "        \n",
    "        print(f\"  Progress: {len(strategies_to_analyze)}/{len(strategies_to_analyze)} (100.0%)\")\n",
    "        \n",
    "        performance_df = pd.DataFrame(performance_results)\n",
    "        print(f\"\\nâœ… Calculated performance for {len(performance_df)} strategies\")\n",
    "        \n",
    "        # Save performance results for future use (only if we calculated all)\n",
    "        if calculate_all_performance and len(performance_df) == len(strategy_index):\n",
    "            performance_df.to_parquet(cached_performance_path)\n",
    "            print(f\"ðŸ’¾ Saved performance metrics to: {cached_performance_path}\")\n",
    "else:\n",
    "    performance_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ Skipping performance calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c61876",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate performance for all strategies\n",
    "if strategy_index is not None and market_data is not None:\n",
    "    performance_results = []\n",
    "    \n",
    "    # Determine strategies to analyze based on parameters\n",
    "    strategies_to_analyze = strategy_index\n",
    "    \n",
    "    if not calculate_all_performance and len(strategy_index) > performance_limit:\n",
    "        print(f\"Note: Large parameter sweep detected ({len(strategy_index)} strategies)\")\n",
    "        print(f\"Limiting analysis to {performance_limit} strategies (set calculate_all_performance=True to analyze all)\")\n",
    "        \n",
    "        # Sample diverse strategies across all types\n",
    "        strategies_to_analyze = strategy_index.groupby('strategy_type').apply(\n",
    "            lambda x: x.sample(n=min(len(x), performance_limit // strategy_index['strategy_type'].nunique()), \n",
    "                             random_state=42)\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nCalculating performance for {len(strategies_to_analyze)} strategies...\")\n",
    "    print(f\"Using run directory: {run_dir}\")\n",
    "    \n",
    "    # Check if we already have cached performance metrics\n",
    "    cached_performance_path = run_dir / 'performance_metrics.parquet'\n",
    "    if cached_performance_path.exists() and calculate_all_performance:\n",
    "        print(f\"ðŸ“‚ Found cached performance metrics, loading...\")\n",
    "        performance_df = pd.read_parquet(cached_performance_path)\n",
    "        print(f\"âœ… Loaded performance for {len(performance_df)} strategies from cache\")\n",
    "    else:\n",
    "        # Calculate performance\n",
    "        for idx, row in strategies_to_analyze.iterrows():\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"  Progress: {idx}/{len(strategies_to_analyze)} ({idx/len(strategies_to_analyze)*100:.1f}%)\")\n",
    "                \n",
    "            perf = calculate_performance(row['strategy_hash'], row['trace_path'], market_data)\n",
    "            \n",
    "            if perf:\n",
    "                # Combine strategy info with performance\n",
    "                result = {**row.to_dict(), **perf}\n",
    "                # Remove the full dataframe from results\n",
    "                result.pop('df', None)\n",
    "                performance_results.append(result)\n",
    "        \n",
    "        print(f\"  Progress: {len(strategies_to_analyze)}/{len(strategies_to_analyze)} (100.0%)\")\n",
    "        \n",
    "        performance_df = pd.DataFrame(performance_results)\n",
    "        print(f\"\\nâœ… Calculated performance for {len(performance_df)} strategies\")\n",
    "        \n",
    "        # Save performance results for future use (only if we calculated all)\n",
    "        if calculate_all_performance and len(performance_df) == len(strategy_index):\n",
    "            performance_df.to_parquet(cached_performance_path)\n",
    "            print(f\"ðŸ’¾ Saved performance metrics to: {cached_performance_path}\")\n",
    "else:\n",
    "    performance_df = pd.DataFrame()\n",
    "    print(\"âš ï¸ Skipping performance calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2cb03e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(performance_df) > 0:\n",
    "    # Top performers across ALL strategy types\n",
    "    top_overall = performance_df.nlargest(top_n_strategies, 'sharpe_ratio')\n",
    "    \n",
    "    print(f\"\\nðŸ† Top {top_n_strategies} Strategies (All Types) - After {execution_cost_bps}bps Execution Costs:\")\n",
    "    print(\"=\" * 90)\n",
    "    \n",
    "    # Look for parameter columns (both with and without param_ prefix for compatibility)\n",
    "    all_param_cols = []\n",
    "    # Check for param_ prefixed columns\n",
    "    param_prefixed_cols = [col for col in top_overall.columns if col.startswith('param_')]\n",
    "    # Check for direct parameter columns (per trace-updates.md)\n",
    "    direct_param_cols = ['period', 'std_dev', 'fast_period', 'slow_period', 'multiplier', 'exit_threshold']\n",
    "    available_param_cols = [col for col in direct_param_cols if col in top_overall.columns]\n",
    "    \n",
    "    # Use whichever we find\n",
    "    if available_param_cols:\n",
    "        all_param_cols = available_param_cols\n",
    "    elif param_prefixed_cols:\n",
    "        all_param_cols = param_prefixed_cols\n",
    "    \n",
    "    for idx, row in top_overall.iterrows():\n",
    "        # Determine identifier to show\n",
    "        strategy_identifier = row.get('strategy_id', 'unknown')\n",
    "        if 'strategy_hash' in row and pd.notna(row['strategy_hash']):\n",
    "            # Check if all strategies have the same hash\n",
    "            if performance_df['strategy_hash'].nunique() > 1:\n",
    "                # Use hash if they're unique\n",
    "                strategy_identifier = row['strategy_hash'][:8]\n",
    "        \n",
    "        print(f\"\\n{row['strategy_type']} - {strategy_identifier}\")\n",
    "        print(f\"  Sharpe: {row['sharpe_ratio']:.2f} | Return: {row['total_return']:.1%} | Drawdown: {row['max_drawdown']:.1%}\")\n",
    "        print(f\"  Win Rate: {row['win_rate']:.1%} | Avg Return/Trade: {row['avg_return_per_trade']*100:.3f}% | Trades: {row['num_trades']}\")\n",
    "        print(f\"  Total Execution Cost: {row['total_execution_cost']*100:.2f}%\")\n",
    "        \n",
    "        # Show parameters\n",
    "        if all_param_cols:\n",
    "            # Filter out null parameters\n",
    "            valid_params = []\n",
    "            for col in all_param_cols[:5]:  # Show up to 5 parameters\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    param_name = col.replace('param_', '') if col.startswith('param_') else col\n",
    "                    valid_params.append(f\"{param_name}: {row[col]}\")\n",
    "            \n",
    "            if valid_params:\n",
    "                print(f\"  Params: {' | '.join(valid_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d5129d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if len(performance_df) > 0:\n",
    "    # Top performers across ALL strategy types\n",
    "    top_overall = performance_df.nlargest(top_n_strategies, 'sharpe_ratio')\n",
    "    \n",
    "    print(f\"\\nðŸ† Top {top_n_strategies} Strategies (All Types):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Look for parameter columns (both with and without param_ prefix for compatibility)\n",
    "    all_param_cols = []\n",
    "    # Check for param_ prefixed columns\n",
    "    param_prefixed_cols = [col for col in top_overall.columns if col.startswith('param_')]\n",
    "    # Check for direct parameter columns (per trace-updates.md)\n",
    "    direct_param_cols = ['period', 'std_dev', 'fast_period', 'slow_period', 'multiplier', 'exit_threshold']\n",
    "    available_param_cols = [col for col in direct_param_cols if col in top_overall.columns]\n",
    "    \n",
    "    # Use whichever we find\n",
    "    if available_param_cols:\n",
    "        all_param_cols = available_param_cols\n",
    "    elif param_prefixed_cols:\n",
    "        all_param_cols = param_prefixed_cols\n",
    "    \n",
    "    for idx, row in top_overall.iterrows():\n",
    "        # Determine identifier to show\n",
    "        strategy_identifier = row.get('strategy_id', 'unknown')\n",
    "        if 'strategy_hash' in row and pd.notna(row['strategy_hash']):\n",
    "            # Check if all strategies have the same hash\n",
    "            if performance_df['strategy_hash'].nunique() > 1:\n",
    "                # Use hash if they're unique\n",
    "                strategy_identifier = row['strategy_hash'][:8]\n",
    "        \n",
    "        print(f\"\\n{row['strategy_type']} - {strategy_identifier}\")\n",
    "        print(f\"  Sharpe: {row['sharpe_ratio']:.2f} | Return: {row['total_return']:.1%} | Drawdown: {row['max_drawdown']:.1%}\")\n",
    "        \n",
    "        # Show parameters\n",
    "        if all_param_cols:\n",
    "            # Filter out null parameters\n",
    "            valid_params = []\n",
    "            for col in all_param_cols[:5]:  # Show up to 5 parameters\n",
    "                if col in row and pd.notna(row[col]):\n",
    "                    param_name = col.replace('param_', '') if col.startswith('param_') else col\n",
    "                    valid_params.append(f\"{param_name}: {row[col]}\")\n",
    "            \n",
    "            if valid_params:\n",
    "                print(f\"  Params: {' | '.join(valid_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2183d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance by strategy type\n",
    "if len(performance_df) > 0:\n",
    "    type_summary = performance_df.groupby('strategy_type').agg({\n",
    "        'sharpe_ratio': ['mean', 'std', 'max'],\n",
    "        'total_return': ['mean', 'std', 'max'],\n",
    "        'strategy_hash': 'count'\n",
    "    }).round(3)\n",
    "    \n",
    "    type_summary.columns = ['_'.join(col).strip() for col in type_summary.columns]\n",
    "    type_summary = type_summary.rename(columns={'strategy_hash_count': 'count'})\n",
    "    type_summary = type_summary.sort_values('sharpe_ratio_mean', ascending=False)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Performance by Strategy Type:\")\n",
    "    print(type_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eee265",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26702ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizations for single or multiple strategy types\n",
    "if len(performance_df) > 0:\n",
    "    if performance_df['strategy_type'].nunique() > 1:\n",
    "        # Multiple strategy types - original visualization\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        # Box plot of Sharpe by type\n",
    "        plt.subplot(1, 2, 1)\n",
    "        performance_df.boxplot(column='sharpe_ratio', by='strategy_type', ax=plt.gca())\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.title('Sharpe Ratio Distribution by Strategy Type')\n",
    "        plt.suptitle('')  # Remove default title\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        \n",
    "        # Scatter: Return vs Sharpe\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for stype in performance_df['strategy_type'].unique():\n",
    "            mask = performance_df['strategy_type'] == stype\n",
    "            plt.scatter(performance_df.loc[mask, 'total_return'], \n",
    "                       performance_df.loc[mask, 'sharpe_ratio'],\n",
    "                       label=stype, alpha=0.6)\n",
    "        plt.xlabel('Total Return')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        plt.title('Return vs Risk-Adjusted Return')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Single strategy type - parameter analysis visualization\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 1. Sharpe ratio distribution\n",
    "        plt.subplot(2, 2, 1)\n",
    "        performance_df['sharpe_ratio'].hist(bins=20, alpha=0.7, color='blue')\n",
    "        plt.axvline(performance_df['sharpe_ratio'].mean(), color='red', linestyle='--', label=f'Mean: {performance_df[\"sharpe_ratio\"].mean():.2f}')\n",
    "        plt.axvline(performance_df['sharpe_ratio'].median(), color='green', linestyle='--', label=f'Median: {performance_df[\"sharpe_ratio\"].median():.2f}')\n",
    "        plt.xlabel('Sharpe Ratio')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Sharpe Ratio Distribution')\n",
    "        plt.legend()\n",
    "        \n",
    "        # 2. Return vs Sharpe scatter\n",
    "        plt.subplot(2, 2, 2)\n",
    "        # Determine which parameters exist (check both naming conventions)\n",
    "        param_cols = [col for col in performance_df.columns if col.startswith('param_')]\n",
    "        direct_param_cols = ['period', 'std_dev', 'fast_period', 'slow_period', 'multiplier', 'exit_threshold']\n",
    "        available_param_cols = [col for col in direct_param_cols if col in performance_df.columns]\n",
    "        \n",
    "        # Use direct parameter names if available, otherwise fall back to param_ prefix\n",
    "        if available_param_cols:\n",
    "            param_cols = available_param_cols\n",
    "        \n",
    "        if len(param_cols) >= 2:\n",
    "            # Use first two parameters for visualization\n",
    "            scatter = plt.scatter(performance_df['total_return'], \n",
    "                                 performance_df['sharpe_ratio'],\n",
    "                                 c=performance_df[param_cols[0]], \n",
    "                                 cmap='viridis',\n",
    "                                 s=performance_df[param_cols[1]]*50 if performance_df[param_cols[1]].max() < 10 else 50,\n",
    "                                 alpha=0.6)\n",
    "            plt.colorbar(scatter, label=param_cols[0].replace('param_', ''))\n",
    "            plt.title(f'Return vs Risk-Adjusted Return\\n(Color={param_cols[0].replace(\"param_\", \"\")}, Size={param_cols[1].replace(\"param_\", \"\")})')\n",
    "        else:\n",
    "            plt.scatter(performance_df['total_return'], \n",
    "                       performance_df['sharpe_ratio'],\n",
    "                       alpha=0.6)\n",
    "            plt.title('Return vs Risk-Adjusted Return')\n",
    "        plt.xlabel('Total Return')\n",
    "        plt.ylabel('Sharpe Ratio')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Parameter heatmap (if enough data and two numeric parameters)\n",
    "        if len(performance_df) > 10 and len(param_cols) >= 2:\n",
    "            plt.subplot(2, 2, 3)\n",
    "            try:\n",
    "                # Create pivot table for heatmap\n",
    "                pivot_sharpe = performance_df.pivot_table(\n",
    "                    values='sharpe_ratio', \n",
    "                    index=param_cols[0], \n",
    "                    columns=param_cols[1],\n",
    "                    aggfunc='mean'\n",
    "                )\n",
    "                if not pivot_sharpe.empty and pivot_sharpe.shape[0] > 1 and pivot_sharpe.shape[1] > 1:\n",
    "                    sns.heatmap(pivot_sharpe, cmap='RdYlGn', center=0, \n",
    "                               cbar_kws={'label': 'Sharpe Ratio'})\n",
    "                    plt.title(f'Sharpe Ratio by {param_cols[0].replace(\"param_\", \"\")} and {param_cols[1].replace(\"param_\", \"\")}')\n",
    "            except:\n",
    "                plt.text(0.5, 0.5, 'Not enough data for heatmap', \n",
    "                        ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        \n",
    "        # 4. Box plot of returns\n",
    "        plt.subplot(2, 2, 4)\n",
    "        performance_df.boxplot(column=['total_return', 'sharpe_ratio'])\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title('Performance Metrics Distribution')\n",
    "        plt.ylabel('Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional parameter analysis\n",
    "        if param_cols:\n",
    "            print(\"\\nðŸ“ˆ Parameter Analysis:\")\n",
    "            for param in param_cols[:3]:  # Analyze first 3 parameters\n",
    "                if param in performance_df.columns and performance_df[param].notna().any():\n",
    "                    corr = performance_df[param].corr(performance_df['sharpe_ratio'])\n",
    "                    param_display = param.replace('param_', '')\n",
    "                    print(f\"Correlation between {param_display} and Sharpe: {corr:.3f}\")\n",
    "            \n",
    "            # Group by parameter ranges to find stable regions\n",
    "            if len(param_cols) >= 2 and len(performance_df) > 20:\n",
    "                print(\"\\nðŸŽ¯ Performance by Parameter Ranges:\")\n",
    "                try:\n",
    "                    # Find numeric parameter columns\n",
    "                    numeric_params = []\n",
    "                    for col in param_cols:\n",
    "                        if pd.api.types.is_numeric_dtype(performance_df[col]) and performance_df[col].notna().sum() > 0:\n",
    "                            numeric_params.append(col)\n",
    "                    \n",
    "                    if len(numeric_params) >= 2:\n",
    "                        # Create bins for numeric parameters\n",
    "                        param1_groups = pd.cut(performance_df[numeric_params[0]], bins=5)\n",
    "                        param2_groups = pd.cut(performance_df[numeric_params[1]], bins=5)\n",
    "                        \n",
    "                        param_summary = performance_df.groupby([param1_groups, param2_groups])['sharpe_ratio'].agg(['mean', 'std', 'count'])\n",
    "                        param_summary = param_summary[param_summary['count'] > 0].sort_values('mean', ascending=False)\n",
    "                        \n",
    "                        # Display with clean parameter names\n",
    "                        param1_name = numeric_params[0].replace('param_', '')\n",
    "                        param2_name = numeric_params[1].replace('param_', '')\n",
    "                        print(f\"\\nTop performing {param1_name} x {param2_name} ranges:\")\n",
    "                        print(param_summary.head(10))\n",
    "                    else:\n",
    "                        print(\"Not enough numeric parameters for range analysis\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not create parameter range analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1be53f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Correlation Analysis for Ensemble Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fc886",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_strategy_correlations(strategies_df, market_data, run_dir):\n",
    "    \"\"\"Calculate correlation matrix between strategies\"\"\"\n",
    "    returns_dict = {}\n",
    "    \n",
    "    for idx, row in strategies_df.iterrows():\n",
    "        try:\n",
    "            # Use the global run_dir\n",
    "            signals_path = run_dir / row['trace_path']\n",
    "            signals = pd.read_parquet(signals_path)\n",
    "            signals['ts'] = pd.to_datetime(signals['ts'])\n",
    "            \n",
    "            # Merge and calculate returns\n",
    "            df = market_data.merge(signals[['ts', 'val']], left_on='timestamp', right_on='ts', how='left')\n",
    "            df['signal'] = df['val'].ffill().fillna(0)\n",
    "            df['returns'] = df['close'].pct_change()\n",
    "            df['strategy_returns'] = df['returns'] * df['signal'].shift(1)\n",
    "            \n",
    "            returns_dict[row['strategy_hash']] = df['strategy_returns']\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create returns DataFrame and calculate correlation\n",
    "    if returns_dict:\n",
    "        returns_df = pd.DataFrame(returns_dict)\n",
    "        return returns_df.corr()\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ae954",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Enhanced Analysis: Stop Loss Impact & Trade Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0514c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop Loss Analysis for Top Strategies\n",
    "if analyze_stop_losses and len(performance_df) > 0 and len(top_overall) > 0:\n",
    "    print(\"\\nðŸ“Š Stop Loss Impact Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    stop_loss_results = {}\n",
    "    \n",
    "    # Analyze top 5 strategies\n",
    "    for idx, (_, strategy) in enumerate(top_overall.head(5).iterrows()):\n",
    "        print(f\"\\nAnalyzing stop losses for strategy {idx+1}: {strategy['strategy_type']} - {strategy['strategy_hash'][:8]}\")\n",
    "        \n",
    "        # Extract trades for this strategy\n",
    "        trades = extract_trades(strategy['strategy_hash'], strategy['trace_path'], market_data, execution_cost_bps)\n",
    "        \n",
    "        if len(trades) > 0:\n",
    "            # Calculate stop loss impact\n",
    "            sl_impact = calculate_stop_loss_impact(trades, stop_loss_levels)\n",
    "            stop_loss_results[strategy['strategy_hash']] = sl_impact\n",
    "            \n",
    "            # Find optimal stop loss\n",
    "            optimal_sl = sl_impact.loc[sl_impact['total_return'].idxmax()]\n",
    "            current_return = trades['net_return'].sum()\n",
    "            \n",
    "            print(f\"  Current total return: {current_return*100:.2f}%\")\n",
    "            print(f\"  Optimal stop loss: {optimal_sl['stop_loss_pct']:.2f}% â†’ Return: {optimal_sl['total_return']*100:.2f}%\")\n",
    "            print(f\"  Improvement: {(optimal_sl['total_return'] - current_return)*100:.2f}%\")\n",
    "            print(f\"  Trades stopped out: {optimal_sl['stopped_out_count']} ({optimal_sl['stopped_out_rate']*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize stop loss impact\n",
    "    if stop_loss_results:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot for each strategy\n",
    "        for i, (hash_id, sl_df) in enumerate(stop_loss_results.items()):\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(sl_df['stop_loss_pct'], sl_df['total_return'] * 100, \n",
    "                    label=f'Strategy {i+1}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Stop Loss (%)')\n",
    "        plt.ylabel('Total Return (%)')\n",
    "        plt.title('Stop Loss Impact on Total Returns')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Win rate impact\n",
    "        plt.subplot(2, 1, 2)\n",
    "        for i, (hash_id, sl_df) in enumerate(stop_loss_results.items()):\n",
    "            plt.plot(sl_df['stop_loss_pct'], sl_df['win_rate'] * 100, \n",
    "                    label=f'Strategy {i+1}', marker='o', markersize=4)\n",
    "        \n",
    "        plt.xlabel('Stop Loss (%)')\n",
    "        plt.ylabel('Win Rate (%)')\n",
    "        plt.title('Stop Loss Impact on Win Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary recommendations\n",
    "        print(\"\\nðŸŽ¯ Stop Loss Recommendations:\")\n",
    "        print(\"Based on the analysis across top strategies:\")\n",
    "        print(\"â€¢ Conservative (minimize large losses): 0.25% stop loss\")\n",
    "        print(\"â€¢ Balanced (optimize return/risk): 0.50% stop loss\")\n",
    "        print(\"â€¢ Aggressive (maximize returns): 1.00% stop loss or no stop\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Skipping stop loss analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5028fb15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Intraday Constraint Verification\n",
    "if verify_intraday and len(performance_df) > 0 and len(top_overall) > 0:\n",
    "    print(\"\\nâ° Intraday Constraint Verification\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Market timezone: {market_timezone}\")\n",
    "    \n",
    "    constraint_violations = []\n",
    "    \n",
    "    # Check top 10 strategies\n",
    "    for idx, (_, strategy) in enumerate(top_overall.head(10).iterrows()):\n",
    "        # Extract trades\n",
    "        trades = extract_trades(strategy['strategy_hash'], strategy['trace_path'], market_data, execution_cost_bps)\n",
    "        \n",
    "        if len(trades) > 0:\n",
    "            # Verify constraints\n",
    "            constraints = verify_intraday_constraint(trades, market_timezone)\n",
    "            \n",
    "            if constraints['overnight_positions'] > 0 or constraints['after_hours_entries'] > 0 or constraints['after_hours_exits'] > 0:\n",
    "                constraint_violations.append({\n",
    "                    'strategy': f\"{strategy['strategy_type']} - {strategy['strategy_hash'][:8]}\",\n",
    "                    'overnight': constraints['overnight_positions'],\n",
    "                    'overnight_pct': constraints['overnight_position_pct'],\n",
    "                    'after_hours_entries': constraints['after_hours_entries'],\n",
    "                    'after_hours_exits': constraints['after_hours_exits']\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nâš ï¸ Strategy {idx+1} has constraint violations:\")\n",
    "                print(f\"   Overnight positions: {constraints['overnight_positions']} ({constraints['overnight_position_pct']:.1f}%)\")\n",
    "                print(f\"   After-hours entries: {constraints['after_hours_entries']}\")\n",
    "                print(f\"   After-hours exits: {constraints['after_hours_exits']}\")\n",
    "            else:\n",
    "                print(f\"\\nâœ… Strategy {idx+1}: All trades respect intraday constraints\")\n",
    "            \n",
    "            # Show trade duration statistics\n",
    "            print(f\"   Avg duration: {constraints['avg_trade_duration_minutes']:.1f} minutes\")\n",
    "            print(f\"   Max duration: {constraints['max_trade_duration_minutes']:.1f} minutes\")\n",
    "            print(f\"   Trades > 390 min: {constraints['trades_over_390_minutes']}\")\n",
    "    \n",
    "    # Summary\n",
    "    if constraint_violations:\n",
    "        print(f\"\\nâš ï¸ Found {len(constraint_violations)} strategies with constraint violations\")\n",
    "        violations_df = pd.DataFrame(constraint_violations)\n",
    "        print(\"\\nViolation Summary:\")\n",
    "        print(violations_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nâœ… All top 10 strategies respect intraday constraints!\")\n",
    "    \n",
    "    # Visualize entry/exit times for a sample strategy\n",
    "    if len(top_overall) > 0:\n",
    "        sample_strategy = top_overall.iloc[0]\n",
    "        sample_trades = extract_trades(sample_strategy['strategy_hash'], sample_strategy['trace_path'], market_data, execution_cost_bps)\n",
    "        \n",
    "        if len(sample_trades) > 0:\n",
    "            sample_constraints = verify_intraday_constraint(sample_trades, market_timezone)\n",
    "            \n",
    "            if 'entries_by_hour' in sample_constraints and sample_constraints['entries_by_hour']:\n",
    "                plt.figure(figsize=(12, 5))\n",
    "                \n",
    "                # Entry times\n",
    "                plt.subplot(1, 2, 1)\n",
    "                hours = sorted(sample_constraints['entries_by_hour'].keys())\n",
    "                counts = [sample_constraints['entries_by_hour'][h] for h in hours]\n",
    "                plt.bar(hours, counts)\n",
    "                plt.axvline(9.5, color='red', linestyle='--', alpha=0.5, label='Market Open')\n",
    "                plt.axvline(16, color='red', linestyle='--', alpha=0.5, label='Market Close')\n",
    "                plt.xlabel('Hour of Day')\n",
    "                plt.ylabel('Number of Entries')\n",
    "                plt.title('Trade Entry Times (Top Strategy)')\n",
    "                plt.legend()\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Exit times\n",
    "                plt.subplot(1, 2, 2)\n",
    "                if 'exits_by_hour' in sample_constraints and sample_constraints['exits_by_hour']:\n",
    "                    hours = sorted(sample_constraints['exits_by_hour'].keys())\n",
    "                    counts = [sample_constraints['exits_by_hour'][h] for h in hours]\n",
    "                    plt.bar(hours, counts)\n",
    "                    plt.axvline(9.5, color='red', linestyle='--', alpha=0.5, label='Market Open')\n",
    "                    plt.axvline(16, color='red', linestyle='--', alpha=0.5, label='Market Close')\n",
    "                    plt.xlabel('Hour of Day')\n",
    "                    plt.ylabel('Number of Exits')\n",
    "                    plt.title('Trade Exit Times (Top Strategy)')\n",
    "                    plt.legend()\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Skipping intraday constraint verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fcdc9f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimized correlation calculation with progress tracking\n",
    "if len(performance_df) > 0 and len(top_overall) > 1:\n",
    "    print(\"\\nðŸ”— Calculating correlations among top strategies...\")\n",
    "    print(f\"Processing {len(top_overall)} strategies...\")\n",
    "    \n",
    "    # First, load all returns data in one pass\n",
    "    returns_dict = {}\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_overall.iterrows()):\n",
    "        if idx % 5 == 0:\n",
    "            print(f\"  Loading signals: {idx}/{len(top_overall)}\")\n",
    "            \n",
    "        try:\n",
    "            signals_path = run_dir / row['trace_path']\n",
    "            \n",
    "            # Load signals\n",
    "            signals = pd.read_parquet(signals_path)\n",
    "            signals['ts'] = pd.to_datetime(signals['ts'])\n",
    "            \n",
    "            # Merge with market data (already in memory)\n",
    "            df = market_data.merge(\n",
    "                signals[['ts', 'val']], \n",
    "                left_on='timestamp', \n",
    "                right_on='ts', \n",
    "                how='left'\n",
    "            )\n",
    "            df['signal'] = df['val'].ffill().fillna(0)\n",
    "            \n",
    "            # Calculate strategy returns only once\n",
    "            df['returns'] = df['close'].pct_change()\n",
    "            df['strategy_returns'] = df['returns'] * df['signal'].shift(1)\n",
    "            \n",
    "            returns_dict[row['strategy_hash']] = df['strategy_returns'].values\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not load {row['strategy_hash'][:8]}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… Loaded returns for {len(returns_dict)} strategies\")\n",
    "    \n",
    "    if len(returns_dict) >= 2:\n",
    "        # Convert to DataFrame for correlation calculation\n",
    "        returns_df = pd.DataFrame(returns_dict)\n",
    "        \n",
    "        # Calculate correlation matrix (this is fast once data is loaded)\n",
    "        print(\"Calculating correlation matrix...\")\n",
    "        corr_matrix = returns_df.corr()\n",
    "        \n",
    "        # Find uncorrelated pairs\n",
    "        uncorrelated_pairs = []\n",
    "        n = len(corr_matrix)\n",
    "        total_pairs = n * (n - 1) // 2\n",
    "        \n",
    "        pair_count = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                pair_count += 1\n",
    "                    \n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) < correlation_threshold:\n",
    "                    uncorrelated_pairs.append({\n",
    "                        'strategy1': corr_matrix.index[i],\n",
    "                        'strategy2': corr_matrix.columns[j],\n",
    "                        'correlation': corr_val\n",
    "                    })\n",
    "        \n",
    "        print(f\"âœ… Found {len(uncorrelated_pairs)} uncorrelated pairs (correlation < {correlation_threshold})\")\n",
    "        \n",
    "        # Visualize correlation matrix\n",
    "        if len(corr_matrix) <= 20:\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            # Only show annotations if matrix is small enough\n",
    "            show_annot = len(corr_matrix) <= 10\n",
    "            sns.heatmap(corr_matrix, cmap='coolwarm', center=0, vmin=-1, vmax=1, \n",
    "                       xticklabels=[h[:8] for h in corr_matrix.columns],\n",
    "                       yticklabels=[h[:8] for h in corr_matrix.index],\n",
    "                       annot=show_annot, fmt='.2f' if show_annot else None)\n",
    "            plt.title('Strategy Correlation Matrix')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Show correlation statistics\n",
    "            corr_values = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]\n",
    "            print(f\"\\nCorrelation Statistics:\")\n",
    "            print(f\"  Mean correlation: {np.mean(corr_values):.3f}\")\n",
    "            print(f\"  Median correlation: {np.median(corr_values):.3f}\")\n",
    "            print(f\"  Min correlation: {np.min(corr_values):.3f}\")\n",
    "            print(f\"  Max correlation: {np.max(corr_values):.3f}\")\n",
    "        else:\n",
    "            print(f\"Skipping heatmap visualization (too many strategies: {len(corr_matrix)})\")\n",
    "    else:\n",
    "        print(\"âŒ Not enough strategies loaded for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38e46e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de2256",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build optimal ensemble\n",
    "if len(performance_df) > 0 and 'corr_matrix' in locals() and not corr_matrix.empty:\n",
    "    # Start with best strategy\n",
    "    ensemble = [top_overall.iloc[0]['strategy_hash']]\n",
    "    ensemble_data = [top_overall.iloc[0]]\n",
    "    \n",
    "    # Add uncorrelated strategies\n",
    "    for idx, candidate in top_overall.iloc[1:].iterrows():\n",
    "        if len(ensemble) >= ensemble_size:\n",
    "            break\n",
    "            \n",
    "        # Check correlation with existing ensemble members\n",
    "        candidate_hash = candidate['strategy_hash']\n",
    "        if candidate_hash in corr_matrix.columns:\n",
    "            max_corr = 0\n",
    "            for existing in ensemble:\n",
    "                if existing in corr_matrix.index:\n",
    "                    corr = abs(corr_matrix.loc[existing, candidate_hash])\n",
    "                    max_corr = max(max_corr, corr)\n",
    "            \n",
    "            if max_corr < correlation_threshold:\n",
    "                ensemble.append(candidate_hash)\n",
    "                ensemble_data.append(candidate)\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Recommended Ensemble ({len(ensemble)} strategies):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ensemble_df = pd.DataFrame(ensemble_data)\n",
    "    for idx, row in ensemble_df.iterrows():\n",
    "        print(f\"\\n{idx+1}. {row['strategy_type']} - {row['strategy_hash'][:8]}\")\n",
    "        print(f\"   Sharpe: {row['sharpe_ratio']:.2f} | Return: {row['total_return']:.1%}\")\n",
    "    \n",
    "    # Calculate ensemble metrics\n",
    "    print(f\"\\nEnsemble Statistics:\")\n",
    "    print(f\"  Average Sharpe: {ensemble_df['sharpe_ratio'].mean():.2f}\")\n",
    "    print(f\"  Average Return: {ensemble_df['total_return'].mean():.1%}\")\n",
    "    print(f\"  Strategy Types: {', '.join(ensemble_df['strategy_type'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e83bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export recommendations with enhanced metrics\n",
    "if len(performance_df) > 0:\n",
    "    recommendations = {\n",
    "        'run_info': {\n",
    "            'run_id': run_dir.name,\n",
    "            'config_name': config_name,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'total_strategies': len(strategy_index) if strategy_index is not None else 0,\n",
    "            'strategies_analyzed': len(performance_df),\n",
    "            'execution_cost_bps': execution_cost_bps\n",
    "        },\n",
    "        'best_individual': {},\n",
    "        'best_by_type': {},\n",
    "        'ensemble': [],\n",
    "        'stop_loss_recommendations': {\n",
    "            'conservative': 0.25,\n",
    "            'balanced': 0.50,\n",
    "            'aggressive': 1.00\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Best overall\n",
    "    if len(top_overall) > 0:\n",
    "        best = top_overall.iloc[0]\n",
    "        recommendations['best_individual'] = {\n",
    "            'strategy_hash': best['strategy_hash'],\n",
    "            'strategy_type': best['strategy_type'],\n",
    "            'sharpe_ratio': float(best['sharpe_ratio']),\n",
    "            'total_return': float(best['total_return']),\n",
    "            'max_drawdown': float(best['max_drawdown']),\n",
    "            'win_rate': float(best.get('win_rate', 0)),\n",
    "            'avg_return_per_trade': float(best.get('avg_return_per_trade', 0)),\n",
    "            'num_trades': int(best.get('num_trades', 0)),\n",
    "            'total_execution_cost': float(best.get('total_execution_cost', 0)),\n",
    "            'parameters': {col.replace('param_', ''): best[col] \n",
    "                          for col in best.index if (col.startswith('param_') or col in ['period', 'std_dev', 'fast_period', 'slow_period', 'multiplier']) \n",
    "                          and pd.notna(best[col])}\n",
    "        }\n",
    "    \n",
    "    # Best by type\n",
    "    for stype in performance_df['strategy_type'].unique():\n",
    "        type_best = performance_df[performance_df['strategy_type'] == stype].nlargest(1, 'sharpe_ratio')\n",
    "        if len(type_best) > 0:\n",
    "            row = type_best.iloc[0]\n",
    "            recommendations['best_by_type'][stype] = {\n",
    "                'strategy_hash': row['strategy_hash'],\n",
    "                'sharpe_ratio': float(row['sharpe_ratio']),\n",
    "                'total_return': float(row['total_return']),\n",
    "                'win_rate': float(row.get('win_rate', 0)),\n",
    "                'avg_return_per_trade': float(row.get('avg_return_per_trade', 0))\n",
    "            }\n",
    "    \n",
    "    # Ensemble\n",
    "    if 'ensemble_df' in locals():\n",
    "        for idx, row in ensemble_df.iterrows():\n",
    "            recommendations['ensemble'].append({\n",
    "                'strategy_hash': row['strategy_hash'],\n",
    "                'strategy_type': row['strategy_type'],\n",
    "                'sharpe_ratio': float(row['sharpe_ratio']),\n",
    "                'win_rate': float(row.get('win_rate', 0)),\n",
    "                'weight': 1.0 / len(ensemble_df)  # Equal weight for now\n",
    "            })\n",
    "    \n",
    "    # Add stop loss analysis results if available\n",
    "    if 'stop_loss_results' in locals() and stop_loss_results:\n",
    "        recommendations['stop_loss_analysis'] = {}\n",
    "        for hash_id, sl_df in list(stop_loss_results.items())[:3]:  # Top 3 strategies\n",
    "            optimal_idx = sl_df['total_return'].idxmax()\n",
    "            recommendations['stop_loss_analysis'][hash_id[:8]] = {\n",
    "                'optimal_stop_loss_pct': float(sl_df.loc[optimal_idx, 'stop_loss_pct']),\n",
    "                'optimal_total_return': float(sl_df.loc[optimal_idx, 'total_return']),\n",
    "                'improvement_pct': float((sl_df.loc[optimal_idx, 'total_return'] - sl_df.loc[0, 'total_return']) * 100)\n",
    "            }\n",
    "    \n",
    "    # Save files\n",
    "    with open(run_dir / 'recommendations.json', 'w') as f:\n",
    "        json.dump(recommendations, f, indent=2)\n",
    "    \n",
    "    performance_df.to_csv(run_dir / 'performance_analysis.csv', index=False)\n",
    "    \n",
    "    # Also save enhanced metrics\n",
    "    enhanced_metrics_df = performance_df[['strategy_hash', 'strategy_type', 'sharpe_ratio', 'total_return', \n",
    "                                         'win_rate', 'avg_return_per_trade', 'num_trades', 'total_execution_cost']].copy()\n",
    "    enhanced_metrics_df.to_csv(run_dir / 'enhanced_metrics.csv', index=False)\n",
    "    \n",
    "    print(\"\\nâœ… Results exported:\")\n",
    "    print(f\"  - recommendations.json (with enhanced metrics)\")\n",
    "    print(f\"  - performance_analysis.csv\")\n",
    "    print(f\"  - enhanced_metrics.csv\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results to export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0f8b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export recommendations\n",
    "if len(performance_df) > 0:\n",
    "    recommendations = {\n",
    "        'run_info': {\n",
    "            'run_id': run_dir.name,\n",
    "            'config_name': config_name,\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'total_strategies': len(strategy_index) if strategy_index is not None else 0,\n",
    "            'strategies_analyzed': len(performance_df)\n",
    "        },\n",
    "        'best_individual': {},\n",
    "        'best_by_type': {},\n",
    "        'ensemble': []\n",
    "    }\n",
    "    \n",
    "    # Best overall\n",
    "    if len(top_overall) > 0:\n",
    "        best = top_overall.iloc[0]\n",
    "        recommendations['best_individual'] = {\n",
    "            'strategy_hash': best['strategy_hash'],\n",
    "            'strategy_type': best['strategy_type'],\n",
    "            'sharpe_ratio': float(best['sharpe_ratio']),\n",
    "            'total_return': float(best['total_return']),\n",
    "            'max_drawdown': float(best['max_drawdown']),\n",
    "            'parameters': {col.replace('param_', ''): best[col] \n",
    "                          for col in best.index if col.startswith('param_') and pd.notna(best[col])}\n",
    "        }\n",
    "    \n",
    "    # Best by type\n",
    "    for stype in performance_df['strategy_type'].unique():\n",
    "        type_best = performance_df[performance_df['strategy_type'] == stype].nlargest(1, 'sharpe_ratio')\n",
    "        if len(type_best) > 0:\n",
    "            row = type_best.iloc[0]\n",
    "            recommendations['best_by_type'][stype] = {\n",
    "                'strategy_hash': row['strategy_hash'],\n",
    "                'sharpe_ratio': float(row['sharpe_ratio']),\n",
    "                'total_return': float(row['total_return'])\n",
    "            }\n",
    "    \n",
    "    # Ensemble\n",
    "    if 'ensemble_df' in locals():\n",
    "        for idx, row in ensemble_df.iterrows():\n",
    "            recommendations['ensemble'].append({\n",
    "                'strategy_hash': row['strategy_hash'],\n",
    "                'strategy_type': row['strategy_type'],\n",
    "                'sharpe_ratio': float(row['sharpe_ratio']),\n",
    "                'weight': 1.0 / len(ensemble_df)  # Equal weight for now\n",
    "            })\n",
    "    \n",
    "    # Save files\n",
    "    with open(run_dir / 'recommendations.json', 'w') as f:\n",
    "        json.dump(recommendations, f, indent=2)\n",
    "    \n",
    "    performance_df.to_csv(run_dir / 'performance_analysis.csv', index=False)\n",
    "    \n",
    "    print(\"\\nâœ… Results exported:\")\n",
    "    print(f\"  - recommendations.json\")\n",
    "    print(f\"  - performance_analysis.csv\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb7661",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Summary\n",
    "\n",
    "Analysis complete! Key files generated:\n",
    "- `recommendations.json` - Best strategies and ensemble recommendations with enhanced metrics\n",
    "- `performance_analysis.csv` - Full performance data for all strategies\n",
    "- `enhanced_metrics.csv` - Win rates, execution costs, and trade-level metrics\n",
    "\n",
    "### Key Findings:\n",
    "1. **Execution Costs**: Analysis includes {execution_cost_bps} basis points round-trip costs\n",
    "2. **Stop Loss Analysis**: Optimal stop loss levels identified for top strategies\n",
    "3. **Intraday Constraints**: Verification of overnight positions and after-hours trading\n",
    "\n",
    "### Next Steps:\n",
    "1. Review stop loss recommendations for your risk tolerance\n",
    "2. Verify all selected strategies respect intraday constraints\n",
    "3. Use the recommended ensemble for live trading with appropriate position sizing\n",
    "4. Monitor execution costs and adjust if actual costs differ from assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be340c65",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "Analysis complete! Key files generated:\n",
    "- `recommendations.json` - Best strategies and ensemble recommendations\n",
    "- `performance_analysis.csv` - Full performance data for all strategies\n",
    "\n",
    "Next steps:\n",
    "1. Use the recommended ensemble for live trading\n",
    "2. Deep dive into specific strategy types if needed\n",
    "3. Run regime-specific analysis to understand performance drivers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.243395,
   "end_time": "2025-06-25T19:47:51.060047",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/daws/ADMF-PC/src/analytics/templates/universal_analysis.ipynb",
   "output_path": "/Users/daws/ADMF-PC/config/bollinger/results/20250625_124543/analysis_20250625_124748.ipynb",
   "parameters": {
    "calculate_all_performance": true,
    "config_name": "bollinger",
    "correlation_threshold": 0.7,
    "ensemble_size": 5,
    "min_strategies_to_analyze": 20,
    "performance_limit": 100,
    "run_dir": "/Users/daws/ADMF-PC/config/bollinger/results/20250625_124543",
    "sharpe_threshold": 1.0,
    "symbols": [
     "SPY_5m"
    ],
    "timeframe": "5m",
    "top_n_strategies": 10
   },
   "start_time": "2025-06-25T19:47:48.816652",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}