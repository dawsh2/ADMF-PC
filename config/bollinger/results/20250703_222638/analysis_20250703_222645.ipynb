{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245e5dbc",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa086e",
   "metadata": {
    "papermill": {
     "duration": 0.009487,
     "end_time": "2025-07-04T05:26:46.710241",
     "exception": false,
     "start_time": "2025-07-04T05:26:46.700754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Trading System Analysis\n",
    "\n",
    "This notebook provides complete analysis of the full trading system including signals, portfolio, and execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Signal generation analysis\n",
    "- Trade execution analysis\n",
    "- Portfolio performance metrics\n",
    "- Risk analysis\n",
    "- Execution cost analysis\n",
    "- Position and fill analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2da9e04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:46.720048Z",
     "iopub.status.busy": "2025-07-04T05:26:46.719721Z",
     "iopub.status.idle": "2025-07-04T05:26:46.724711Z",
     "shell.execute_reply": "2025-07-04T05:26:46.724244Z"
    },
    "papermill": {
     "duration": 0.0103,
     "end_time": "2025-07-04T05:26:46.725920",
     "exception": false,
     "start_time": "2025-07-04T05:26:46.715620",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters will be injected here by papermill\n",
    "# This cell is tagged with 'parameters' for papermill to recognize it\n",
    "run_dir = \".\"\n",
    "config_name = \"config\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n",
    "\n",
    "# Analysis parameters\n",
    "execution_cost_bps = 1.0  # Round-trip execution cost in basis points\n",
    "analyze_slippage = True\n",
    "analyze_intraday_patterns = True\n",
    "market_timezone = \"America/New_York\"\n",
    "\n",
    "# Performance thresholds\n",
    "min_sharpe_ratio = 1.0\n",
    "max_acceptable_drawdown = 0.20  # 20%\n",
    "min_win_rate = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c94b3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:46.731725Z",
     "iopub.status.busy": "2025-07-04T05:26:46.731595Z",
     "iopub.status.idle": "2025-07-04T05:26:46.733738Z",
     "shell.execute_reply": "2025-07-04T05:26:46.733405Z"
    },
    "papermill": {
     "duration": 0.006051,
     "end_time": "2025-07-04T05:26:46.734587",
     "exception": false,
     "start_time": "2025-07-04T05:26:46.728536",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_dir = \"/Users/daws/ADMF-PC/config/bollinger/results/20250703_222638\"\n",
    "config_name = \"bollinger\"\n",
    "symbols = [\"SPY\"]\n",
    "timeframe = \"5m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1792ab",
   "metadata": {
    "papermill": {
     "duration": 0.002423,
     "end_time": "2025-07-04T05:26:46.739197",
     "exception": false,
     "start_time": "2025-07-04T05:26:46.736774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ec0ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:46.744169Z",
     "iopub.status.busy": "2025-07-04T05:26:46.744028Z",
     "iopub.status.idle": "2025-07-04T05:26:47.181187Z",
     "shell.execute_reply": "2025-07-04T05:26:47.180953Z"
    },
    "papermill": {
     "duration": 0.440449,
     "end_time": "2025-07-04T05:26:47.181871",
     "exception": false,
     "start_time": "2025-07-04T05:26:46.741422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing run: 20250703_222638\n",
      "Full path: /Users/daws/ADMF-PC/config/bollinger/results/20250703_222638\n",
      "Config: bollinger\n",
      "Symbol(s): ['SPY']\n",
      "Timeframe: 5m\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, time\n",
    "import pytz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Convert run_dir to Path\n",
    "run_dir = Path(run_dir).resolve()\n",
    "print(f\"Analyzing run: {run_dir.name}\")\n",
    "print(f\"Full path: {run_dir}\")\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"Symbol(s): {symbols}\")\n",
    "print(f\"Timeframe: {timeframe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcadb4d",
   "metadata": {
    "papermill": {
     "duration": 0.001441,
     "end_time": "2025-07-04T05:26:47.185009",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.183568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Metadata and Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2fad7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:47.188702Z",
     "iopub.status.busy": "2025-07-04T05:26:47.188563Z",
     "iopub.status.idle": "2025-07-04T05:26:47.193168Z",
     "shell.execute_reply": "2025-07-04T05:26:47.192954Z"
    },
    "papermill": {
     "duration": 0.00713,
     "end_time": "2025-07-04T05:26:47.193796",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.186666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Run metadata loaded\n",
      "   Total bars: 16614\n",
      "   Total signals: 16601\n",
      "   Total orders: 4132\n",
      "   Total fills: 4132\n",
      "   Total positions: 4132\n",
      "\n",
      "📁 Global traces path: /Users/daws/ADMF-PC/traces\n",
      "\n",
      "📊 Available traces:\n",
      "   Global signals: ✅\n",
      "   Local signals: ❌\n",
      "   Portfolio: ❌\n",
      "   Execution: ❌\n"
     ]
    }
   ],
   "source": [
    "# Load run metadata\n",
    "metadata_path = run_dir / 'metadata.json'\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"✅ Run metadata loaded\")\n",
    "    print(f\"   Total bars: {metadata.get('total_bars', 'N/A')}\")\n",
    "    print(f\"   Total signals: {metadata.get('total_signals', 'N/A')}\")\n",
    "    print(f\"   Total orders: {metadata.get('total_orders', 'N/A')}\")\n",
    "    print(f\"   Total fills: {metadata.get('total_fills', 'N/A')}\")\n",
    "    print(f\"   Total positions: {metadata.get('total_positions', 'N/A')}\")\n",
    "    \n",
    "    # Get global traces path\n",
    "    global_traces_path = Path(metadata.get('global_traces_path', '/Users/daws/ADMF-PC/traces'))\n",
    "    print(f\"\\n📁 Global traces path: {global_traces_path}\")\n",
    "else:\n",
    "    print(\"❌ No metadata.json found\")\n",
    "    metadata = {}\n",
    "    global_traces_path = Path('/Users/daws/ADMF-PC/traces')\n",
    "\n",
    "# Check what traces are available in global store\n",
    "store_path = global_traces_path / 'store'\n",
    "has_global_signals = store_path.exists() and any(store_path.glob('*.parquet'))\n",
    "\n",
    "# For backward compatibility, also check run directory\n",
    "traces_dir = run_dir / 'traces'\n",
    "has_local_signals = (traces_dir / 'signals').exists() if traces_dir.exists() else False\n",
    "has_portfolio = (traces_dir / 'portfolio').exists() if traces_dir.exists() else False\n",
    "has_execution = (traces_dir / 'execution').exists() if traces_dir.exists() else False\n",
    "\n",
    "print(f\"\\n📊 Available traces:\")\n",
    "print(f\"   Global signals: {'✅' if has_global_signals else '❌'}\")\n",
    "print(f\"   Local signals: {'✅' if has_local_signals else '❌'}\")\n",
    "print(f\"   Portfolio: {'✅' if has_portfolio else '❌'}\")\n",
    "print(f\"   Execution: {'✅' if has_execution else '❌'}\")\n",
    "\n",
    "# Determine trace location\n",
    "use_global_store = has_global_signals and not has_local_signals\n",
    "is_full_system = metadata.get('total_orders', 0) > 0 or has_portfolio or has_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5b084",
   "metadata": {
    "papermill": {
     "duration": 0.001529,
     "end_time": "2025-07-04T05:26:47.196825",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.195296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da2b9e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:47.200244Z",
     "iopub.status.busy": "2025-07-04T05:26:47.200061Z",
     "iopub.status.idle": "2025-07-04T05:26:47.232153Z",
     "shell.execute_reply": "2025-07-04T05:26:47.231929Z"
    },
    "papermill": {
     "duration": 0.034538,
     "end_time": "2025-07-04T05:26:47.232781",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.198243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded market data from: /Users/daws/ADMF-PC/data/SPY_5m.csv\n",
      "   Date range: 2024-03-26 13:30:00+00:00 to 2025-04-02 19:20:00+00:00\n",
      "   Total bars: 20769\n"
     ]
    }
   ],
   "source": [
    "# Load market data\n",
    "market_data = None\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Try different possible locations\n",
    "        data_paths = [\n",
    "            run_dir / f'data/{symbol}_{timeframe}.csv',\n",
    "            run_dir / f'{symbol}_{timeframe}.csv',\n",
    "            run_dir.parent / f'data/{symbol}_{timeframe}.csv',\n",
    "            Path(f'/Users/daws/ADMF-PC/data/{symbol}_{timeframe}.csv')\n",
    "        ]\n",
    "        \n",
    "        for data_path in data_paths:\n",
    "            if data_path.exists():\n",
    "                market_data = pd.read_csv(data_path)\n",
    "                market_data['timestamp'] = pd.to_datetime(market_data['timestamp'])\n",
    "                market_data = market_data.sort_values('timestamp')\n",
    "                \n",
    "                # Add derived fields\n",
    "                market_data['returns'] = market_data['close'].pct_change()\n",
    "                market_data['log_returns'] = np.log(market_data['close'] / market_data['close'].shift(1))\n",
    "                market_data['hour'] = market_data['timestamp'].dt.hour\n",
    "                market_data['minute'] = market_data['timestamp'].dt.minute\n",
    "                market_data['day_of_week'] = market_data['timestamp'].dt.dayofweek\n",
    "                \n",
    "                print(f\"✅ Loaded market data from: {data_path}\")\n",
    "                print(f\"   Date range: {market_data['timestamp'].min()} to {market_data['timestamp'].max()}\")\n",
    "                print(f\"   Total bars: {len(market_data)}\")\n",
    "                break\n",
    "        \n",
    "        if market_data is not None:\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for {symbol}: {e}\")\n",
    "\n",
    "if market_data is None:\n",
    "    print(\"❌ Could not load market data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d690f4a",
   "metadata": {
    "papermill": {
     "duration": 0.001508,
     "end_time": "2025-07-04T05:26:47.235923",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.234415",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Signal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "441dff64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:47.239468Z",
     "iopub.status.busy": "2025-07-04T05:26:47.239354Z",
     "iopub.status.idle": "2025-07-04T05:26:47.243990Z",
     "shell.execute_reply": "2025-07-04T05:26:47.243786Z"
    },
    "papermill": {
     "duration": 0.007164,
     "end_time": "2025-07-04T05:26:47.244560",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.237396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 SIGNAL ANALYSIS\n",
      "================================================================================\n",
      "No strategy index found in run directory\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze signals if available\n",
    "if has_global_signals or has_local_signals:\n",
    "    print(\"\\n📊 SIGNAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load strategy index\n",
    "    strategy_index_path = run_dir / 'strategy_index.parquet'\n",
    "    if strategy_index_path.exists():\n",
    "        strategy_index = pd.read_parquet(strategy_index_path)\n",
    "        print(f\"Loaded {len(strategy_index)} strategies from run index\")\n",
    "        \n",
    "        # Show strategy distribution\n",
    "        by_type = strategy_index['strategy_type'].value_counts()\n",
    "        print(\"\\nStrategies by type:\")\n",
    "        for stype, count in by_type.items():\n",
    "            print(f\"  {stype}: {count}\")\n",
    "    else:\n",
    "        print(\"No strategy index found in run directory\")\n",
    "        strategy_index = pd.DataFrame()\n",
    "    \n",
    "    # Analyze signal patterns from global store\n",
    "    if use_global_store and len(strategy_index) > 0:\n",
    "        print(\"\\n📊 Analyzing signals from global store...\")\n",
    "        \n",
    "        # Get trace paths from metadata components\n",
    "        signal_counts = []\n",
    "        components = metadata.get('components', {})\n",
    "        \n",
    "        for comp_name, comp_data in components.items():\n",
    "            if comp_data.get('type') == 'strategy' and 'trace_path' in comp_data:\n",
    "                trace_path = Path(comp_data['trace_path'])\n",
    "                if trace_path.exists():\n",
    "                    signals = pd.read_parquet(trace_path)\n",
    "                    \n",
    "                    # Count actual signal changes (non-zero values)\n",
    "                    signal_changes = signals[signals['val'] != 0]\n",
    "                    \n",
    "                    signal_counts.append({\n",
    "                        'strategy_type': comp_data.get('strategy_type'),\n",
    "                        'strategy_hash': comp_data.get('strategy_hash'),\n",
    "                        'total_signals': len(signals),\n",
    "                        'signal_changes': len(signal_changes),\n",
    "                        'long_signals': (signal_changes['val'] > 0).sum(),\n",
    "                        'short_signals': (signal_changes['val'] < 0).sum(),\n",
    "                        'signals_per_1000_bars': len(signal_changes) / (metadata.get('total_bars', 1000) / 1000)\n",
    "                    })\n",
    "                    \n",
    "                    # Show sample signals\n",
    "                    if len(signal_changes) > 0:\n",
    "                        print(f\"\\n  Strategy: {comp_data.get('strategy_type')} ({comp_data.get('strategy_hash', '')[:8]})\")\n",
    "                        print(f\"    Signal changes: {len(signal_changes)}\")\n",
    "                        print(f\"    First signal: {signal_changes.iloc[0]['ts']} -> {signal_changes.iloc[0]['val']}\")\n",
    "                        print(f\"    Last signal: {signal_changes.iloc[-1]['ts']} -> {signal_changes.iloc[-1]['val']}\")\n",
    "        \n",
    "        if signal_counts:\n",
    "            signal_df = pd.DataFrame(signal_counts)\n",
    "            print(\"\\n📊 Signal frequency analysis:\")\n",
    "            print(signal_df.to_string(index=False))\n",
    "            \n",
    "            # Check if signals were generated but no trades\n",
    "            if metadata.get('total_signals', 0) > 0 and metadata.get('total_orders', 0) == 0:\n",
    "                print(\"\\n⚠️ WARNING: Signals were generated but no orders were created!\")\n",
    "                print(\"Possible reasons:\")\n",
    "                print(\"  - Risk constraints (stop loss/take profit) may be too tight\")\n",
    "                print(\"  - Position sizing returned 0 shares\")\n",
    "                print(\"  - Intraday constraints prevented trades\")\n",
    "                print(\"  - Check the execution logs for more details\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ No signal traces found in global store\")\n",
    "            \n",
    "    # Analyze from local traces (backward compatibility)\n",
    "    elif has_local_signals:\n",
    "        print(\"\\n📊 Analyzing signals from local traces...\")\n",
    "        # Original local trace analysis code here\n",
    "else:\n",
    "    print(\"\\n⚠️ No signal traces available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c6df9",
   "metadata": {
    "papermill": {
     "duration": 0.001565,
     "end_time": "2025-07-04T05:26:47.247774",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.246209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Portfolio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a91a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:47.252214Z",
     "iopub.status.busy": "2025-07-04T05:26:47.252032Z",
     "iopub.status.idle": "2025-07-04T05:26:47.405637Z",
     "shell.execute_reply": "2025-07-04T05:26:47.405398Z"
    },
    "papermill": {
     "duration": 0.156213,
     "end_time": "2025-07-04T05:26:47.406349",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.250136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💼 PORTFOLIO ANALYSIS\n",
      "================================================================================\n",
      "Found 27 trades file(s) in global store\n",
      "✅ Loaded 1033 trades from Ta405f5fbdee7.parquet\n",
      "✅ Loaded 1033 trades from T2bfbfc99f8b5.parquet\n",
      "✅ Loaded 4 trades from T93b7e27d534b.parquet\n",
      "✅ Loaded 2 trades from T5d20cf8187db.parquet\n",
      "✅ Loaded 1033 trades from Ta9997e10a4e3.parquet\n",
      "✅ Loaded 1033 trades from T40df3c9e1459.parquet\n",
      "✅ Loaded 2 trades from Ta781416fd9e4.parquet\n",
      "✅ Loaded 2 trades from Tf120fa29a212.parquet\n",
      "✅ Loaded 1033 trades from T2a95646ad129.parquet\n",
      "✅ Loaded 4 trades from T0afff94cd392.parquet\n",
      "✅ Loaded 9 trades from T2e7fbce3bf82.parquet\n",
      "✅ Loaded 1033 trades from T9b9b28f3ffa9.parquet\n",
      "✅ Loaded 4 trades from Tdb3a6f6d0fa9.parquet\n",
      "✅ Loaded 4 trades from Td619c3141148.parquet\n",
      "✅ Loaded 4 trades from T174d05c1f713.parquet\n",
      "✅ Loaded 1033 trades from Tae329989009e.parquet\n",
      "✅ Loaded 1033 trades from T36bd18061b3a.parquet\n",
      "✅ Loaded 9 trades from T71e430ef2a6e.parquet\n",
      "✅ Loaded 1033 trades from T077f7715d2e5.parquet\n",
      "✅ Loaded 1033 trades from Tb00c26431149.parquet\n",
      "✅ Loaded 1033 trades from T0b01c74c847d.parquet\n",
      "✅ Loaded 1033 trades from Tb052b88aa35c.parquet\n",
      "✅ Loaded 1033 trades from T10966321165c.parquet\n",
      "✅ Loaded 4 trades from T5e50ecc0c55c.parquet\n",
      "✅ Loaded 4 trades from Td3468538d525.parquet\n",
      "✅ Loaded 1033 trades from T864299be09c1.parquet\n",
      "✅ Loaded 4 trades from T45a145caf923.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total trades loaded: 14518\n",
      "\n",
      "🔍 DEBUG: Trades DataFrame Analysis\n",
      "Shape: (14518, 31)\n",
      "\n",
      "Columns: ['trade_id', 'position_id', 'symbol', 'strategy_id', 'signal_hash', 'entry_bar_idx', 'entry_time', 'entry_signal_strength', 'direction', 'entry_order_id', 'entry_order_price', 'entry_order_time', 'entry_fill_id', 'entry_fill_price', 'entry_fill_time', 'exit_bar_idx', 'exit_time', 'exit_reason', 'exit_signal_strength', 'pnl', 'duration_bars', 'exit_order_id', 'exit_order_price', 'exit_order_time', 'exit_fill_id', 'exit_fill_price', 'exit_fill_time', 'commission', 'slippage_entry', 'slippage_exit', 'duration_time']\n",
      "\n",
      "📊 Price field analysis:\n",
      "entry_fill_price non-null: 4164 / 14518\n",
      "exit_fill_price non-null: 4144 / 14518\n",
      "entry_order_price non-null: 4166 / 14518\n",
      "exit_order_price non-null: 4144 / 14518\n",
      "\n",
      "entry_fill_price unique values (first 5): [   nan 521.11 521.4  520.21 519.19]\n",
      "exit_fill_price unique values (first 5): [        nan 521.4       521.3969    519.8198425 519.1114   ]\n",
      "\n",
      "📋 Sample trade (first row):\n",
      "  trade_id: T000001\n",
      "  position_id: unknown\n",
      "  symbol: SPY_5m\n",
      "  strategy_id: SPY_5m_strategy_0\n",
      "  signal_hash: d63509043abb\n",
      "  entry_bar_idx: 27\n",
      "  entry_time: 2024-03-26 15:45:00+00:00\n",
      "  entry_signal_strength: 0.0\n",
      "  direction: unknown\n",
      "  entry_order_id: None\n",
      "  entry_order_price: nan\n",
      "  entry_order_time: NaT\n",
      "  entry_fill_id: None\n",
      "  entry_fill_price: nan\n",
      "  entry_fill_time: NaT\n",
      "  exit_bar_idx: 28\n",
      "  exit_time: 2024-03-26 15:50:00+00:00\n",
      "  exit_reason: Strategy reversal signal (short)\n",
      "  exit_signal_strength: 0\n",
      "  pnl: 0.0\n",
      "  duration_bars: 1\n",
      "  exit_order_id: None\n",
      "  exit_order_price: nan\n",
      "  exit_order_time: NaT\n",
      "  exit_fill_id: None\n",
      "  exit_fill_price: nan\n",
      "  exit_fill_time: NaT\n",
      "  commission: 0\n",
      "  slippage_entry: 0.0\n",
      "  slippage_exit: 0.0\n",
      "  duration_time: 0:05:00\n",
      "\n",
      "✅ Trades with complete price data: 4144 / 14518\n",
      "\n",
      "📊 Trade Statistics:\n",
      "  Total trades: 14518\n",
      "  Unique strategies: 1\n",
      "  Win rate: 11.3%\n",
      "  Average PnL: $0.00\n",
      "  Total PnL: $53.46\n",
      "  Average duration: 1.1 bars\n",
      "  Average return: 0.003%\n",
      "\n",
      "📋 Sample trades:\n",
      "trade_id symbol direction                entry_time                 exit_time  entry_fill_price  exit_fill_price  pnl\n",
      " T000001 SPY_5m   unknown 2024-03-26 15:45:00+00:00 2024-03-26 15:50:00+00:00               NaN              NaN  0.0\n",
      " T000002 SPY_5m   unknown 2024-03-26 15:50:00+00:00 2024-03-26 15:55:00+00:00               NaN              NaN  0.0\n",
      " T000003 SPY_5m   unknown 2024-03-26 19:25:00+00:00 2024-03-26 19:30:00+00:00               NaN              NaN  0.0\n",
      " T000004 SPY_5m   unknown 2024-03-26 19:40:00+00:00 2024-03-26 19:45:00+00:00               NaN              NaN  0.0\n",
      " T000005 SPY_5m   unknown 2024-03-27 13:30:00+00:00 2024-03-27 13:35:00+00:00               NaN              NaN  0.0\n"
     ]
    }
   ],
   "source": [
    "# Load and analyze portfolio data\n",
    "if is_full_system:\n",
    "    print(\"\\n💼 PORTFOLIO ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    trades_df = pd.DataFrame()\n",
    "    portfolio_traces_found = False\n",
    "    \n",
    "    # First check for unified trades file in global store\n",
    "    if use_global_store:\n",
    "        # Look for trades files (T{hash}.parquet pattern)\n",
    "        trades_files = list(store_path.glob('T*.parquet'))\n",
    "        \n",
    "        if trades_files:\n",
    "            print(f\"Found {len(trades_files)} trades file(s) in global store\")\n",
    "            \n",
    "            # Load the most recent trades file (they have timestamps in the hash)\n",
    "            # or load all and concatenate if multiple runs\n",
    "            all_trades = []\n",
    "            for trades_file in trades_files:\n",
    "                try:\n",
    "                    df = pd.read_parquet(trades_file)\n",
    "                    print(f\"✅ Loaded {len(df)} trades from {trades_file.name}\")\n",
    "                    all_trades.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {trades_file}: {e}\")\n",
    "            \n",
    "            if all_trades:\n",
    "                trades_df = pd.concat(all_trades, ignore_index=True)\n",
    "                \n",
    "                # Convert timestamp columns\n",
    "                for col in ['entry_time', 'exit_time', 'entry_order_time', 'exit_order_time', \n",
    "                           'entry_fill_time', 'exit_fill_time']:\n",
    "                    if col in trades_df.columns:\n",
    "                        trades_df[col] = pd.to_datetime(trades_df[col])\n",
    "                \n",
    "                # CRITICAL: Convert price columns to numeric to handle string prices\n",
    "                price_columns = ['entry_fill_price', 'exit_fill_price', 'entry_order_price', 'exit_order_price']\n",
    "                for col in price_columns:\n",
    "                    if col in trades_df.columns:\n",
    "                        trades_df[col] = pd.to_numeric(trades_df[col], errors='coerce')\n",
    "                \n",
    "                portfolio_traces_found = True\n",
    "                print(f\"✅ Total trades loaded: {len(trades_df)}\")\n",
    "                \n",
    "                # DEBUG: Examine trades data structure\n",
    "                print(\"\\n🔍 DEBUG: Trades DataFrame Analysis\")\n",
    "                print(f\"Shape: {trades_df.shape}\")\n",
    "                print(f\"\\nColumns: {list(trades_df.columns)}\")\n",
    "                \n",
    "                # Check price fields\n",
    "                print(f\"\\n📊 Price field analysis:\")\n",
    "                print(f\"entry_fill_price non-null: {trades_df['entry_fill_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"exit_fill_price non-null: {trades_df['exit_fill_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"entry_order_price non-null: {trades_df['entry_order_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                print(f\"exit_order_price non-null: {trades_df['exit_order_price'].notna().sum()} / {len(trades_df)}\")\n",
    "                \n",
    "                # Show unique values\n",
    "                print(f\"\\nentry_fill_price unique values (first 5): {trades_df['entry_fill_price'].unique()[:5]}\")\n",
    "                print(f\"exit_fill_price unique values (first 5): {trades_df['exit_fill_price'].unique()[:5]}\")\n",
    "                \n",
    "                # Show a sample trade with all fields\n",
    "                if len(trades_df) > 0:\n",
    "                    print(f\"\\n📋 Sample trade (first row):\")\n",
    "                    sample = trades_df.iloc[0]\n",
    "                    for col in trades_df.columns:\n",
    "                        print(f\"  {col}: {sample[col]}\")\n",
    "                \n",
    "                # Check for trades with complete price data\n",
    "                complete_price_trades = trades_df[\n",
    "                    trades_df['entry_fill_price'].notna() & \n",
    "                    trades_df['exit_fill_price'].notna()\n",
    "                ]\n",
    "                print(f\"\\n✅ Trades with complete price data: {len(complete_price_trades)} / {len(trades_df)}\")\n",
    "                \n",
    "                # Show trade statistics\n",
    "                if len(trades_df) > 0:\n",
    "                    print(\"\\n📊 Trade Statistics:\")\n",
    "                    print(f\"  Total trades: {len(trades_df)}\")\n",
    "                    print(f\"  Unique strategies: {trades_df['strategy_id'].nunique() if 'strategy_id' in trades_df else 'N/A'}\")\n",
    "                    \n",
    "                    if 'pnl' in trades_df:\n",
    "                        winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "                        print(f\"  Win rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "                        print(f\"  Average PnL: ${trades_df['pnl'].mean():.2f}\")\n",
    "                        print(f\"  Total PnL: ${trades_df['pnl'].sum():.2f}\")\n",
    "                    \n",
    "                    if 'duration_bars' in trades_df:\n",
    "                        print(f\"  Average duration: {trades_df['duration_bars'].mean():.1f} bars\")\n",
    "                    \n",
    "                    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "                        # Calculate returns from fill prices ONLY for trades with valid numeric prices\n",
    "                        valid_trades = trades_df[\n",
    "                            trades_df['entry_fill_price'].notna() & \n",
    "                            trades_df['exit_fill_price'].notna() &\n",
    "                            (trades_df['entry_fill_price'] > 0)\n",
    "                        ].copy()\n",
    "                        \n",
    "                        if len(valid_trades) > 0:\n",
    "                            valid_trades['return'] = (valid_trades['exit_fill_price'] - valid_trades['entry_fill_price']) / valid_trades['entry_fill_price']\n",
    "                            # Adjust for short trades\n",
    "                            short_mask = valid_trades['direction'] == 'short'\n",
    "                            valid_trades.loc[short_mask, 'return'] = -valid_trades.loc[short_mask, 'return']\n",
    "                            print(f\"  Average return: {valid_trades['return'].mean()*100:.3f}%\")\n",
    "                            \n",
    "                            # Copy return column back to main dataframe\n",
    "                            trades_df['return'] = np.nan\n",
    "                            trades_df.loc[valid_trades.index, 'return'] = valid_trades['return']\n",
    "                    \n",
    "                    # Show sample trades\n",
    "                    print(\"\\n📋 Sample trades:\")\n",
    "                    display_cols = ['trade_id', 'symbol', 'direction', 'entry_time', 'exit_time', \n",
    "                                   'entry_fill_price', 'exit_fill_price', 'pnl']\n",
    "                    display_cols = [col for col in display_cols if col in trades_df.columns]\n",
    "                    if display_cols:\n",
    "                        print(trades_df[display_cols].head(5).to_string(index=False))\n",
    "    \n",
    "    # Fallback to old format if no unified trades file\n",
    "    if not portfolio_traces_found:\n",
    "        print(\"\\n⚠️ No unified trades file found, checking for legacy portfolio traces...\")\n",
    "        \n",
    "        # Check both global and local for individual order/position files\n",
    "        orders = pd.DataFrame()\n",
    "        positions_opened = pd.DataFrame()\n",
    "        positions_closed = pd.DataFrame()\n",
    "        \n",
    "        # [Previous code for loading individual order/position files remains as fallback]\n",
    "        # ... (keeping the existing fallback code)\n",
    "        \n",
    "    # If still no trades found\n",
    "    if len(trades_df) == 0:\n",
    "        print(\"\\n⚠️ No trades found. Possible reasons:\")\n",
    "        print(\"  - Signals didn't trigger any trades due to risk constraints\")\n",
    "        print(\"  - Position sizing returned 0 shares\")\n",
    "        print(\"  - Intraday constraints prevented trades\")\n",
    "        print(\"  - Check console output for execution warnings\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No portfolio data to analyze (metadata shows 0 orders/fills/positions)\")\n",
    "    trades_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be6510",
   "metadata": {
    "papermill": {
     "duration": 0.001652,
     "end_time": "2025-07-04T05:26:47.409853",
     "exception": false,
     "start_time": "2025-07-04T05:26:47.408201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Execution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2eb10d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48fd150c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T05:26:47.413809Z",
     "iopub.status.busy": "2025-07-04T05:26:47.413686Z",
     "iopub.status.idle": "2025-07-04T05:26:47.746303Z",
     "shell.execute_reply": "2025-07-04T05:26:47.745762Z"
    },
    "papermill": {
     "duration": 0.335273,
     "end_time": "2025-07-04T05:26:47.746948",
     "exception": true,
     "start_time": "2025-07-04T05:26:47.411675",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚡ EXECUTION ANALYSIS\n",
      "================================================================================\n",
      "📊 Analyzing execution quality from trades data...\n",
      "\n",
      "Fill Statistics:\n",
      "  Total fills: 29036 (entry + exit fills)\n",
      "  Average fill price: $557.50\n",
      "  Fill price range: $495.54 - $609.20\n",
      "\n",
      "💸 Slippage Analysis:\n",
      "  Entry slippage: 0.0 bps (avg), 0.0 bps (std)\n",
      "  Exit slippage: 0.0 bps (avg), 0.0 bps (std)\n",
      "  Total slippage cost: $0.00\n",
      "\n",
      "⏱️ Execution Timing:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot subtract tz-naive and tz-aware datetime-like objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py:1188\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._sub_datetimelike\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assert_tzawareness_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arrays/datetimes.py:784\u001b[39m, in \u001b[36mDatetimeArray._assert_tzawareness_compat\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m other_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    785\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot compare tz-naive and tz-aware datetime-like objects.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m         )\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m other_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: Cannot compare tz-naive and tz-aware datetime-like objects.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m⏱️ Execution Timing:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mentry_order_time\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trades_df \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mentry_fill_time\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trades_df:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Calculate time to fill\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     trades_df[\u001b[33m'\u001b[39m\u001b[33mentry_time_to_fill\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[43mtrades_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_fill_time\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrades_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentry_order_time\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m).dt.total_seconds()\n\u001b[32m     89\u001b[39m     valid_times = trades_df[\u001b[33m'\u001b[39m\u001b[33mentry_time_to_fill\u001b[39m\u001b[33m'\u001b[39m].dropna()\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_times) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:194\u001b[39m, in \u001b[36mOpsMixin.__sub__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__sub__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/series.py:6146\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6145\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other)\n\u001b[32m-> \u001b[39m\u001b[32m6146\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/base.py:1391\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1388\u001b[39m     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:273\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# NB: We assume that extract_array and ensure_wrapped_if_datetimelike\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;66;03m#  have already been called on `left` and `right`,\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;66;03m#  and `maybe_prepare_scalar_for_op` has already been called on `right`\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# We need to special-case datetime64/timedelta64 dtypes (e.g. because numpy\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# casts integer dtypes to timedelta64 when operating with timedelta64 - GH#22390)\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    267\u001b[39m     should_extension_dispatch(left, right)\n\u001b[32m    268\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, (Timedelta, BaseOffset, Timestamp))\n\u001b[32m   (...)\u001b[39m\u001b[32m    271\u001b[39m     \u001b[38;5;66;03m# Timedelta/Timestamp and other custom scalars are included in the check\u001b[39;00m\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# because numexpr will fail on it, see GH#31457\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# TODO we should handle EAs consistently and move this check before the if/else\u001b[39;00m\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# (https://github.com/pandas-dev/pandas/issues/41165)\u001b[39;00m\n\u001b[32m    277\u001b[39m     \u001b[38;5;66;03m# error: Argument 2 to \"_bool_arith_check\" has incompatible type\u001b[39;00m\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py:1482\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__sub__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   1477\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._addsub_object_array(other, operator.sub)\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lib.is_np_dtype(other_dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m   1479\u001b[39m     other_dtype, DatetimeTZDtype\n\u001b[32m   1480\u001b[39m ):\n\u001b[32m   1481\u001b[39m     \u001b[38;5;66;03m# DatetimeIndex, ndarray[datetime64]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sub_datetime_arraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other_dtype, PeriodDtype):\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# PeriodIndex\u001b[39;00m\n\u001b[32m   1485\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sub_periodlike(other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py:1179\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._sub_datetime_arraylike\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28mself\u001b[39m = cast(\u001b[33m\"\u001b[39m\u001b[33mDatetimeArray\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1178\u001b[39m \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._ensure_matching_resos(other)\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sub_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ADMF-PC/venv/lib/python3.13/site-packages/pandas/core/arrays/datetimelike.py:1191\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._sub_datetimelike\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1190\u001b[39m     new_message = \u001b[38;5;28mstr\u001b[39m(err).replace(\u001b[33m\"\u001b[39m\u001b[33mcompare\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msubtract\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(new_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1193\u001b[39m other_i8, o_mask = \u001b[38;5;28mself\u001b[39m._get_i8_values_and_mask(other)\n\u001b[32m   1194\u001b[39m res_values = add_overflowsafe(\u001b[38;5;28mself\u001b[39m.asi8, np.asarray(-other_i8, dtype=\u001b[33m\"\u001b[39m\u001b[33mi8\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mTypeError\u001b[39m: Cannot subtract tz-naive and tz-aware datetime-like objects."
     ]
    }
   ],
   "source": [
    "# Load and analyze execution data\n",
    "if is_full_system and len(trades_df) > 0:\n",
    "    print(\"\\n⚡ EXECUTION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract execution metrics from trades dataframe\n",
    "    print(\"📊 Analyzing execution quality from trades data...\")\n",
    "    \n",
    "    # Fill statistics\n",
    "    total_fills = len(trades_df) * 2  # Entry and exit for each trade\n",
    "    print(f\"\\nFill Statistics:\")\n",
    "    print(f\"  Total fills: {total_fills} (entry + exit fills)\")\n",
    "    \n",
    "    if 'entry_fill_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "        # Filter out non-numeric values\n",
    "        entry_prices = trades_df['entry_fill_price'].dropna()\n",
    "        exit_prices = trades_df['exit_fill_price'].dropna()\n",
    "        \n",
    "        if len(entry_prices) > 0 and len(exit_prices) > 0:\n",
    "            all_fill_prices = pd.concat([entry_prices, exit_prices])\n",
    "            print(f\"  Average fill price: ${all_fill_prices.mean():.2f}\")\n",
    "            print(f\"  Fill price range: ${all_fill_prices.min():.2f} - ${all_fill_prices.max():.2f}\")\n",
    "    \n",
    "    # Slippage analysis\n",
    "    if analyze_slippage:\n",
    "        print(\"\\n💸 Slippage Analysis:\")\n",
    "        \n",
    "        # Entry slippage\n",
    "        if 'slippage_entry' in trades_df:\n",
    "            # Convert to numeric if needed\n",
    "            trades_df['slippage_entry'] = pd.to_numeric(trades_df['slippage_entry'], errors='coerce')\n",
    "            valid_slippage = trades_df[trades_df['slippage_entry'].notna() & trades_df['entry_fill_price'].notna()]\n",
    "            if len(valid_slippage) > 0:\n",
    "                entry_slippage_bps = valid_slippage['slippage_entry'] / valid_slippage['entry_fill_price'] * 10000\n",
    "                print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n",
    "        elif 'entry_order_price' in trades_df and 'entry_fill_price' in trades_df:\n",
    "            # Calculate if not pre-computed (only for valid numeric data)\n",
    "            valid_entry = trades_df[\n",
    "                trades_df['entry_order_price'].notna() & \n",
    "                trades_df['entry_fill_price'].notna() &\n",
    "                (trades_df['entry_order_price'] > 0) &\n",
    "                (trades_df['entry_fill_price'] > 0)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(valid_entry) > 0:\n",
    "                valid_entry['slippage_entry'] = abs(valid_entry['entry_fill_price'] - valid_entry['entry_order_price'])\n",
    "                entry_slippage_bps = valid_entry['slippage_entry'] / valid_entry['entry_fill_price'] * 10000\n",
    "                print(f\"  Entry slippage: {entry_slippage_bps.mean():.1f} bps (avg), {entry_slippage_bps.std():.1f} bps (std)\")\n",
    "        \n",
    "        # Exit slippage\n",
    "        if 'slippage_exit' in trades_df:\n",
    "            # Convert to numeric if needed\n",
    "            trades_df['slippage_exit'] = pd.to_numeric(trades_df['slippage_exit'], errors='coerce')\n",
    "            valid_slippage = trades_df[trades_df['slippage_exit'].notna() & trades_df['exit_fill_price'].notna()]\n",
    "            if len(valid_slippage) > 0:\n",
    "                exit_slippage_bps = valid_slippage['slippage_exit'] / valid_slippage['exit_fill_price'] * 10000\n",
    "                print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n",
    "        elif 'exit_order_price' in trades_df and 'exit_fill_price' in trades_df:\n",
    "            # Calculate if not pre-computed (only for valid numeric data)\n",
    "            valid_exit = trades_df[\n",
    "                trades_df['exit_order_price'].notna() & \n",
    "                trades_df['exit_fill_price'].notna() &\n",
    "                (trades_df['exit_order_price'] > 0) &\n",
    "                (trades_df['exit_fill_price'] > 0)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(valid_exit) > 0:\n",
    "                valid_exit['slippage_exit'] = abs(valid_exit['exit_fill_price'] - valid_exit['exit_order_price'])\n",
    "                exit_slippage_bps = valid_exit['slippage_exit'] / valid_exit['exit_fill_price'] * 10000\n",
    "                print(f\"  Exit slippage: {exit_slippage_bps.mean():.1f} bps (avg), {exit_slippage_bps.std():.1f} bps (std)\")\n",
    "        \n",
    "        # Total slippage cost\n",
    "        if 'slippage_entry' in trades_df and 'slippage_exit' in trades_df:\n",
    "            # Ensure numeric\n",
    "            trades_df['slippage_entry'] = pd.to_numeric(trades_df['slippage_entry'], errors='coerce')\n",
    "            trades_df['slippage_exit'] = pd.to_numeric(trades_df['slippage_exit'], errors='coerce')\n",
    "            \n",
    "            total_slippage_cost = (\n",
    "                trades_df['slippage_entry'].fillna(0) + \n",
    "                trades_df['slippage_exit'].fillna(0)\n",
    "            ).sum()\n",
    "            print(f\"  Total slippage cost: ${total_slippage_cost:.2f}\")\n",
    "    \n",
    "    # Execution timing analysis\n",
    "    print(\"\\n⏱️ Execution Timing:\")\n",
    "    if 'entry_order_time' in trades_df and 'entry_fill_time' in trades_df:\n",
    "        # Calculate time to fill\n",
    "        trades_df['entry_time_to_fill'] = (trades_df['entry_fill_time'] - trades_df['entry_order_time']).dt.total_seconds()\n",
    "        valid_times = trades_df['entry_time_to_fill'].dropna()\n",
    "        if len(valid_times) > 0:\n",
    "            print(f\"  Entry order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n",
    "    \n",
    "    if 'exit_order_time' in trades_df and 'exit_fill_time' in trades_df:\n",
    "        trades_df['exit_time_to_fill'] = (trades_df['exit_fill_time'] - trades_df['exit_order_time']).dt.total_seconds()\n",
    "        valid_times = trades_df['exit_time_to_fill'].dropna()\n",
    "        if len(valid_times) > 0:\n",
    "            print(f\"  Exit order to fill: {valid_times.mean():.1f}s (avg), {valid_times.max():.1f}s (max)\")\n",
    "    \n",
    "    # Commission analysis\n",
    "    if 'commission' in trades_df:\n",
    "        # Ensure numeric\n",
    "        trades_df['commission'] = pd.to_numeric(trades_df['commission'], errors='coerce')\n",
    "        print(\"\\n💰 Execution Costs:\")\n",
    "        print(f\"  Total commissions: ${trades_df['commission'].sum():.2f}\")\n",
    "        print(f\"  Average commission per trade: ${trades_df['commission'].mean():.2f}\")\n",
    "        \n",
    "        if 'pnl' in trades_df and trades_df['pnl'].sum() != 0:\n",
    "            print(f\"  Commission as % of PnL: {abs(trades_df['commission'].sum() / trades_df['pnl'].sum() * 100):.2f}%\")\n",
    "    \n",
    "    # Exit reason analysis\n",
    "    if 'exit_reason' in trades_df:\n",
    "        print(\"\\n🎯 Exit Reasons:\")\n",
    "        exit_reasons = trades_df['exit_reason'].value_counts()\n",
    "        for reason, count in exit_reasons.items():\n",
    "            print(f\"  {reason}: {count} ({count/len(trades_df)*100:.1f}%)\")\n",
    "            \n",
    "elif is_full_system:\n",
    "    print(\"\\n⚠️ No execution data to analyze (no trades found)\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping execution analysis (no trades executed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69a461",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57a030",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate overall performance metrics if we have trades\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n📈 PERFORMANCE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Use PnL data if available\n",
    "    if 'pnl' in trades_df:\n",
    "        # Calculate equity curve from trades\n",
    "        initial_capital = 100000  # Assumed\n",
    "        trades_df = trades_df.sort_values('exit_time')\n",
    "        trades_df['cum_pnl'] = trades_df['pnl'].cumsum()\n",
    "        trades_df['equity'] = initial_capital + trades_df['cum_pnl']\n",
    "        \n",
    "        # Create equity curve dataframe\n",
    "        equity_curve = []\n",
    "        equity_curve.append({'timestamp': trades_df['entry_time'].min(), 'equity': initial_capital})\n",
    "        for _, trade in trades_df.iterrows():\n",
    "            equity_curve.append({'timestamp': trade['exit_time'], 'equity': trade['equity']})\n",
    "        \n",
    "        equity_df = pd.DataFrame(equity_curve)\n",
    "        equity_df['returns'] = equity_df['equity'].pct_change()\n",
    "        \n",
    "        # Performance metrics\n",
    "        total_return = (equity_df['equity'].iloc[-1] / initial_capital - 1)\n",
    "        \n",
    "        # Sharpe ratio (assuming daily returns)\n",
    "        if 'exit_time' in trades_df and len(trades_df) > 1:\n",
    "            daily_returns = trades_df.groupby(trades_df['exit_time'].dt.date)['pnl'].sum() / initial_capital\n",
    "            if len(daily_returns) > 1 and daily_returns.std() > 0:\n",
    "                sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
    "            else:\n",
    "                sharpe_ratio = 0\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "        \n",
    "        # Max drawdown\n",
    "        cummax = equity_df['equity'].expanding().max()\n",
    "        drawdown = (equity_df['equity'] / cummax - 1)\n",
    "        max_drawdown = drawdown.min()\n",
    "        \n",
    "        # Win/loss statistics\n",
    "        winning_trades = trades_df[trades_df['pnl'] > 0]\n",
    "        losing_trades = trades_df[trades_df['pnl'] <= 0]\n",
    "        \n",
    "        print(f\"Total Return: {total_return*100:.2f}%\")\n",
    "        print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "        print(f\"Max Drawdown: {max_drawdown*100:.2f}%\")\n",
    "        print(f\"\\nTrade Statistics:\")\n",
    "        print(f\"  Total Trades: {len(trades_df)}\")\n",
    "        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}%\")\n",
    "        print(f\"  Average Win: ${winning_trades['pnl'].mean():.2f}\" if len(winning_trades) > 0 else \"  Average Win: N/A\")\n",
    "        print(f\"  Average Loss: ${losing_trades['pnl'].mean():.2f}\" if len(losing_trades) > 0 else \"  Average Loss: N/A\")\n",
    "        \n",
    "        if len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0:\n",
    "            profit_factor = winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())\n",
    "            print(f\"  Profit Factor: {profit_factor:.2f}\")\n",
    "        else:\n",
    "            print(f\"  Profit Factor: N/A\")\n",
    "        \n",
    "        # Performance vs thresholds\n",
    "        print(f\"\\n🎯 Performance vs Thresholds:\")\n",
    "        print(f\"  Sharpe Ratio: {sharpe_ratio:.2f} {'✅' if sharpe_ratio >= min_sharpe_ratio else '❌'} (min: {min_sharpe_ratio})\")\n",
    "        print(f\"  Max Drawdown: {abs(max_drawdown)*100:.1f}% {'✅' if abs(max_drawdown) <= max_acceptable_drawdown else '❌'} (max: {max_acceptable_drawdown*100:.0f}%)\")\n",
    "        print(f\"  Win Rate: {len(winning_trades)/len(trades_df)*100:.1f}% {'✅' if len(winning_trades)/len(trades_df) >= min_win_rate else '❌'} (min: {min_win_rate*100:.0f}%)\")\n",
    "        \n",
    "        # Plot equity curve\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(equity_df['timestamp'], equity_df['equity'])\n",
    "        plt.title('Portfolio Equity Curve')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Equity ($)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot drawdown\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.fill_between(equity_df['timestamp'], drawdown * 100, 0, alpha=0.3, color='red')\n",
    "        plt.title('Portfolio Drawdown')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Drawdown (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"⚠️ No PnL data available in trades\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No trades available for performance analysis\")\n",
    "    print(\"\\nPossible reasons why no trades were executed:\")\n",
    "    print(\"1. Risk parameters (stop loss: 0.075%, take profit: 0.1%) may be too tight\")\n",
    "    print(\"2. Position sizing returned 0 shares\")\n",
    "    print(\"3. Intraday constraints prevented trades\") \n",
    "    print(\"4. Signals were generated but didn't meet execution criteria\")\n",
    "    print(\"\\nCheck the signal analysis section above to confirm signals were generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c149f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Intraday Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95722856",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analyze intraday patterns if requested\n",
    "if analyze_intraday_patterns and len(trades_df) > 0:\n",
    "    print(\"\\n⏰ INTRADAY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Extract hour of entry and exit\n",
    "    trades_df['entry_hour'] = trades_df['entry_time'].dt.hour\n",
    "    trades_df['exit_hour'] = trades_df['exit_time'].dt.hour\n",
    "    trades_df['entry_day'] = trades_df['entry_time'].dt.dayofweek\n",
    "    \n",
    "    # Performance by hour of day\n",
    "    hourly_performance = trades_df.groupby('entry_hour').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Win rate by hour\n",
    "    hourly_win_rate = trades_df.groupby('entry_hour').apply(\n",
    "        lambda x: (x['pnl'] > 0).mean() * 100\n",
    "    )\n",
    "    \n",
    "    # Performance by day of week\n",
    "    daily_performance = trades_df.groupby('entry_day').agg({\n",
    "        'pnl': ['count', 'sum', 'mean'],\n",
    "        'return': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Trades by hour\n",
    "    ax = axes[0, 0]\n",
    "    hourly_performance['pnl']['count'].plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Number of Trades by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Trade Count')\n",
    "    \n",
    "    # Win rate by hour\n",
    "    ax = axes[0, 1]\n",
    "    hourly_win_rate.plot(kind='bar', ax=ax, color='green')\n",
    "    ax.axhline(50, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Win Rate by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Win Rate (%)')\n",
    "    \n",
    "    # Average PnL by hour\n",
    "    ax = axes[1, 0]\n",
    "    hourly_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='blue')\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Hour')\n",
    "    ax.set_xlabel('Hour of Day')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    # Performance by day of week\n",
    "    ax = axes[1, 1]\n",
    "    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']\n",
    "    daily_performance['pnl']['mean'].plot(kind='bar', ax=ax, color='purple')\n",
    "    ax.set_xticklabels(days[:len(daily_performance)], rotation=0)\n",
    "    ax.axhline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average PnL by Day of Week')\n",
    "    ax.set_xlabel('Day of Week')\n",
    "    ax.set_ylabel('Average PnL ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best and worst times\n",
    "    print(\"\\n🕐 Best Trading Hours:\")\n",
    "    best_hours = hourly_performance['pnl']['mean'].nlargest(3)\n",
    "    for hour, avg_pnl in best_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")\n",
    "    \n",
    "    print(\"\\n🕐 Worst Trading Hours:\")\n",
    "    worst_hours = hourly_performance['pnl']['mean'].nsmallest(3)\n",
    "    for hour, avg_pnl in worst_hours.items():\n",
    "        count = hourly_performance.loc[hour, ('pnl', 'count')]\n",
    "        win_rate = hourly_win_rate.loc[hour]\n",
    "        print(f\"  {hour}:00 - Avg PnL: ${avg_pnl:.2f}, Win Rate: {win_rate:.1f}%, Trades: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d560b8f4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfaf9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comprehensive risk analysis\n",
    "if len(trades_df) > 0:\n",
    "    print(\"\\n⚠️ RISK ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Trade duration analysis\n",
    "    if 'duration' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics:\")\n",
    "        print(f\"  Average: {trades_df['duration'].mean():.1f} minutes\")\n",
    "        print(f\"  Median: {trades_df['duration'].median():.1f} minutes\")\n",
    "        print(f\"  Shortest: {trades_df['duration'].min():.1f} minutes\")\n",
    "        print(f\"  Longest: {trades_df['duration'].max():.1f} minutes\")\n",
    "    elif 'duration_bars' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics (in bars):\")\n",
    "        print(f\"  Average: {trades_df['duration_bars'].mean():.1f} bars\")\n",
    "        print(f\"  Median: {trades_df['duration_bars'].median():.1f} bars\")\n",
    "        print(f\"  Shortest: {trades_df['duration_bars'].min():.0f} bars\")\n",
    "        print(f\"  Longest: {trades_df['duration_bars'].max():.0f} bars\")\n",
    "    elif 'duration_time' in trades_df.columns:\n",
    "        print(\"Trade Duration Statistics:\")\n",
    "        print(f\"  Duration times available in 'duration_time' column\")\n",
    "    \n",
    "    # Consecutive wins/losses\n",
    "    if 'pnl' in trades_df.columns:\n",
    "        trades_df['is_win'] = trades_df['pnl'] > 0\n",
    "        trades_df['streak'] = (trades_df['is_win'] != trades_df['is_win'].shift()).cumsum()\n",
    "        \n",
    "        win_streaks = trades_df[trades_df['is_win']].groupby('streak').size()\n",
    "        loss_streaks = trades_df[~trades_df['is_win']].groupby('streak').size()\n",
    "        \n",
    "        print(f\"\\nStreak Analysis:\")\n",
    "        print(f\"  Max consecutive wins: {win_streaks.max() if len(win_streaks) > 0 else 0}\")\n",
    "        print(f\"  Max consecutive losses: {loss_streaks.max() if len(loss_streaks) > 0 else 0}\")\n",
    "        print(f\"  Average win streak: {win_streaks.mean():.1f}\" if len(win_streaks) > 0 else \"  Average win streak: N/A\")\n",
    "        print(f\"  Average loss streak: {loss_streaks.mean():.1f}\" if len(loss_streaks) > 0 else \"  Average loss streak: N/A\")\n",
    "    \n",
    "    # Risk-adjusted returns\n",
    "    if 'return' in trades_df.columns and trades_df['return'].std() > 0:\n",
    "        information_ratio = trades_df['return'].mean() / trades_df['return'].std()\n",
    "        print(f\"\\nRisk-Adjusted Metrics:\")\n",
    "        print(f\"  Information Ratio: {information_ratio:.3f}\")\n",
    "        print(f\"  Return/Risk: {trades_df['return'].mean() / trades_df['return'].std():.3f}\")\n",
    "    \n",
    "    # Value at Risk (VaR)\n",
    "    if 'pnl' in trades_df.columns:\n",
    "        var_95 = np.percentile(trades_df['pnl'], 5)\n",
    "        var_99 = np.percentile(trades_df['pnl'], 1)\n",
    "        \n",
    "        print(f\"\\nValue at Risk (VaR):\")\n",
    "        print(f\"  95% VaR: ${var_95:.2f}\")\n",
    "        print(f\"  99% VaR: ${var_99:.2f}\")\n",
    "        \n",
    "        # Plot PnL distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(trades_df['pnl'], bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.axvline(0, color='red', linestyle='--', alpha=0.5, label='Breakeven')\n",
    "        plt.axvline(trades_df['pnl'].mean(), color='green', linestyle='--', alpha=0.5, label='Mean PnL')\n",
    "        plt.axvline(var_95, color='orange', linestyle='--', alpha=0.5, label='95% VaR')\n",
    "        plt.xlabel('PnL ($)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('PnL Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Show what columns are available for further analysis\n",
    "    print(f\"\\nAvailable trade data columns: {list(trades_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f809b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3658c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate summary and recommendations\n",
    "print(\"\\n📋 SUMMARY AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'run_info': {\n",
    "        'run_id': run_dir.name,\n",
    "        'config_name': config_name,\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'is_full_system': is_full_system\n",
    "    },\n",
    "    'data_summary': {\n",
    "        'total_bars': metadata.get('total_bars', 0),\n",
    "        'total_signals': metadata.get('total_signals', 0),\n",
    "        'total_orders': metadata.get('total_orders', 0),\n",
    "        'total_fills': metadata.get('total_fills', 0),\n",
    "        'total_positions': metadata.get('total_positions', 0)\n",
    "    },\n",
    "    'performance_summary': {},\n",
    "    'risk_summary': {},\n",
    "    'recommendations': []\n",
    "}\n",
    "\n",
    "if len(trades_df) > 0:\n",
    "    # Performance summary\n",
    "    summary['performance_summary'] = {\n",
    "        'total_trades': len(trades_df),\n",
    "        'total_return': float(total_return) if 'total_return' in locals() else 0,\n",
    "        'sharpe_ratio': float(sharpe_ratio) if 'sharpe_ratio' in locals() else 0,\n",
    "        'max_drawdown': float(max_drawdown) if 'max_drawdown' in locals() else 0,\n",
    "        'win_rate': float(len(winning_trades)/len(trades_df)) if 'winning_trades' in locals() else 0,\n",
    "        'profit_factor': float(winning_trades['pnl'].sum() / abs(losing_trades['pnl'].sum())) if 'winning_trades' in locals() and 'losing_trades' in locals() and len(losing_trades) > 0 and losing_trades['pnl'].sum() != 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Risk summary\n",
    "    risk_summary = {}\n",
    "    if 'var_95' in locals():\n",
    "        risk_summary['var_95'] = float(var_95)\n",
    "    if 'var_99' in locals():\n",
    "        risk_summary['var_99'] = float(var_99)\n",
    "    if 'loss_streaks' in locals() and len(loss_streaks) > 0:\n",
    "        risk_summary['max_consecutive_losses'] = int(loss_streaks.max())\n",
    "    \n",
    "    # Add trade duration based on available columns\n",
    "    if 'duration' in trades_df.columns:\n",
    "        risk_summary['avg_trade_duration_minutes'] = float(trades_df['duration'].mean())\n",
    "    elif 'duration_bars' in trades_df.columns:\n",
    "        risk_summary['avg_trade_duration_bars'] = float(trades_df['duration_bars'].mean())\n",
    "    \n",
    "    summary['risk_summary'] = risk_summary\n",
    "    \n",
    "    # Generate recommendations\n",
    "    if 'sharpe_ratio' in locals() and sharpe_ratio < min_sharpe_ratio:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'high',\n",
    "            'message': f'Sharpe ratio ({sharpe_ratio:.2f}) below minimum threshold ({min_sharpe_ratio}). Consider parameter optimization.'\n",
    "        })\n",
    "    \n",
    "    if 'max_drawdown' in locals() and abs(max_drawdown) > max_acceptable_drawdown:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'risk',\n",
    "            'severity': 'high',\n",
    "            'message': f'Maximum drawdown ({abs(max_drawdown)*100:.1f}%) exceeds acceptable limit ({max_acceptable_drawdown*100:.0f}%). Implement stricter risk controls.'\n",
    "        })\n",
    "    \n",
    "    if 'winning_trades' in locals() and len(winning_trades)/len(trades_df) < min_win_rate:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'performance',\n",
    "            'severity': 'medium',\n",
    "            'message': f'Win rate ({len(winning_trades)/len(trades_df)*100:.1f}%) below minimum ({min_win_rate*100:.0f}%). Review entry criteria.'\n",
    "        })\n",
    "    \n",
    "    # Execution-specific recommendations\n",
    "    if 'fills' in locals() and 'slippage_bps' in fills.columns and fills['slippage_bps'].mean() > 5:\n",
    "        summary['recommendations'].append({\n",
    "            'type': 'execution',\n",
    "            'severity': 'medium',\n",
    "            'message': f'High average slippage ({fills[\"slippage_bps\"].mean():.1f} bps). Consider limit orders or better execution timing.'\n",
    "        })\n",
    "    \n",
    "    # Intraday pattern recommendations\n",
    "    if analyze_intraday_patterns and 'hourly_performance' in locals():\n",
    "        worst_hour = hourly_performance['pnl']['mean'].idxmin()\n",
    "        if hourly_performance.loc[worst_hour, ('pnl', 'mean')] < -50:\n",
    "            summary['recommendations'].append({\n",
    "                'type': 'timing',\n",
    "                'severity': 'low',\n",
    "                'message': f'Poor performance at {worst_hour}:00. Consider avoiding trades during this hour.'\n",
    "            })\n",
    "\n",
    "# Display recommendations\n",
    "if summary['recommendations']:\n",
    "    print(\"🎯 Recommendations:\")\n",
    "    for rec in sorted(summary['recommendations'], key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']]):\n",
    "        severity_icon = {'high': '🔴', 'medium': '🟡', 'low': '🟢'}[rec['severity']]\n",
    "        print(f\"\\n{severity_icon} [{rec['severity'].upper()}] {rec['type'].title()}\")\n",
    "        print(f\"   {rec['message']}\")\n",
    "else:\n",
    "    print(\"✅ No critical issues identified\")\n",
    "\n",
    "# Save summary\n",
    "with open(run_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\n📄 Analysis summary saved to: analysis_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8eee8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed621c82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export key dataframes for further analysis\n",
    "print(\"\\n💾 EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exports = {}\n",
    "\n",
    "# Export trades if available\n",
    "if len(trades_df) > 0:\n",
    "    trades_export_path = run_dir / 'analyzed_trades.csv'\n",
    "    trades_df.to_csv(trades_export_path, index=False)\n",
    "    exports['trades'] = 'analyzed_trades.csv'\n",
    "    print(f\"✅ Exported {len(trades_df)} trades to {trades_export_path}\")\n",
    "    \n",
    "    # Export performance summary\n",
    "    if 'pnl' in trades_df:\n",
    "        # Convert numpy types to Python native types for JSON serialization\n",
    "        performance_summary = {\n",
    "            'total_trades': int(len(trades_df)),\n",
    "            'total_pnl': float(trades_df['pnl'].sum()),\n",
    "            'win_rate': float((trades_df['pnl'] > 0).mean()),\n",
    "            'avg_pnl': float(trades_df['pnl'].mean()),\n",
    "            'best_trade': float(trades_df['pnl'].max()),\n",
    "            'worst_trade': float(trades_df['pnl'].min()),\n",
    "            'avg_duration_bars': float(trades_df['duration_bars'].mean()) if 'duration_bars' in trades_df else None\n",
    "        }\n",
    "        \n",
    "        perf_summary_path = run_dir / 'performance_summary.json'\n",
    "        with open(perf_summary_path, 'w') as f:\n",
    "            json.dump(performance_summary, f, indent=2)\n",
    "        exports['performance_summary'] = 'performance_summary.json'\n",
    "        print(f\"✅ Exported performance summary\")\n",
    "\n",
    "# Export slippage analysis if available\n",
    "if len(trades_df) > 0 and 'slippage_entry' in trades_df:\n",
    "    slippage_summary = trades_df[['trade_id', 'symbol', 'entry_fill_price', 'exit_fill_price', \n",
    "                                  'slippage_entry', 'slippage_exit']].copy()\n",
    "    slippage_path = run_dir / 'slippage_analysis.csv'\n",
    "    slippage_summary.to_csv(slippage_path, index=False)\n",
    "    exports['slippage'] = 'slippage_analysis.csv'\n",
    "    print(f\"✅ Exported slippage analysis\")\n",
    "\n",
    "# Export equity curve if calculated\n",
    "if 'equity_df' in locals() and len(equity_df) > 0:\n",
    "    equity_path = run_dir / 'equity_curve.csv'\n",
    "    equity_df.to_csv(equity_path, index=False)\n",
    "    exports['equity_curve'] = 'equity_curve.csv'\n",
    "    print(f\"✅ Exported equity curve\")\n",
    "\n",
    "# Create final report\n",
    "report = {\n",
    "    'analysis_complete': True,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'exports': exports,\n",
    "    'summary': summary,\n",
    "    'global_store_used': use_global_store,\n",
    "    'trades_file_location': str(store_path / 'T*.parquet') if use_global_store else None\n",
    "}\n",
    "\n",
    "final_report_path = run_dir / 'final_report.json'\n",
    "with open(final_report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Analysis complete! Results saved to {run_dir}\")\n",
    "\n",
    "if use_global_store:\n",
    "    print(f\"\\n📁 Note: Signal traces are in the global store at: {store_path}\")\n",
    "    print(f\"   Trade data may be in: {store_path}/T*.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.287411,
   "end_time": "2025-07-04T05:26:48.066154",
   "environment_variables": {},
   "exception": true,
   "input_path": "/Users/daws/ADMF-PC/src/analytics/templates/analysis.ipynb",
   "output_path": "config/bollinger/results/20250703_222638/analysis_20250703_222645.ipynb",
   "parameters": {
    "config_name": "bollinger",
    "run_dir": "/Users/daws/ADMF-PC/config/bollinger/results/20250703_222638",
    "symbols": [
     "SPY"
    ],
    "timeframe": "5m"
   },
   "start_time": "2025-07-04T05:26:45.778743",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}