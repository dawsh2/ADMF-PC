# Multi-Process Implementation Guide for ADMF-PC

## Overview

This guide provides a complete implementation plan for adding multi-process capabilities to ADMF-PC while maintaining:
- Protocol + Composition architecture (zero inheritance)
- Complete event tracing across process boundaries
- Backward compatibility with single-process mode
- Clean, testable, incremental implementation

## Architecture Principles

### 1. Protocol-First Design
All multi-process components use protocols, not base classes:

```python
from typing import Protocol, runtime_checkable, Any, Dict, Optional, List
from abc import abstractmethod

@runtime_checkable
class WorkerProtocol(Protocol):
    """Protocol for all worker processes"""
    
    @property
    def worker_id(self) -> str:
        """Unique identifier for this worker"""
        ...
    
    @property
    def is_running(self) -> bool:
        """Check if worker is currently running"""
        ...
    
    def setup(self) -> None:
        """Initialize worker resources"""
        ...
    
    def process_messages(self) -> None:
        """Process incoming messages"""
        ...
    
    def cleanup(self) -> None:
        """Clean up worker resources"""
        ...
    
    def get_status(self) -> Dict[str, Any]:
        """Get worker status and metrics"""
        ...


@runtime_checkable
class IPCProtocol(Protocol):
    """Protocol for inter-process communication"""
    
    def publish(self, topic: str, event: Any) -> None:
        """Publish event to topic"""
        ...
    
    def subscribe(self, topics: List[str]) -> None:
        """Subscribe to topics"""
        ...
    
    def receive(self, timeout: Optional[float] = None) -> Optional[Tuple[str, Any]]:
        """Receive message with optional timeout"""
        ...
    
    def close(self) -> None:
        """Close all connections"""
        ...


@runtime_checkable
class ProcessManagerProtocol(Protocol):
    """Protocol for managing worker processes"""
    
    def start_worker(self, worker_config: Dict[str, Any]) -> str:
        """Start a new worker process"""
        ...
    
    def stop_worker(self, worker_id: str) -> None:
        """Stop a worker process"""
        ...
    
    def get_worker_status(self, worker_id: str) -> Dict[str, Any]:
        """Get status of specific worker"""
        ...
    
    def health_check(self) -> Dict[str, Any]:
        """Check health of all workers"""
        ...
```

### 2. Composition Over Inheritance
Workers are composed of capabilities, not inherited:

```python
@dataclass
class WorkerComponents:
    """Components that can be composed into workers"""
    ipc: IPCProtocol
    event_bus: Optional[EventBus] = None
    tracer: Optional[EventTracerProtocol] = None
    metrics: Optional[MetricsObserver] = None
    barriers: Optional[CompositeBarrier] = None
```

## Phase 1: IPC Foundation

### 1.1 Event Serialization
Extend existing Event class with serialization:

```python
# src/core/events/serialization.py
from typing import Dict, Any, Optional
import msgpack
import json
from datetime import datetime
from decimal import Decimal

from .types import Event, EventType


class EventSerializer:
    """Handles event serialization for IPC"""
    
    @staticmethod
    def to_wire_format(event: Event, format: str = 'msgpack') -> bytes:
        """
        Serialize event for IPC transmission.
        
        Args:
            event: Event to serialize
            format: Serialization format ('msgpack', 'json')
            
        Returns:
            Serialized bytes
        """
        # Convert to dict with proper type handling
        data = {
            'event_id': event.event_id,
            'event_type': event.event_type,
            'timestamp': event.timestamp.isoformat() if event.timestamp else None,
            'source_id': event.source_id,
            'container_id': event.container_id,
            'correlation_id': event.correlation_id,
            'causation_id': event.causation_id,
            'sequence_number': event.sequence_number,
            'payload': EventSerializer._prepare_payload(event.payload),
            'metadata': event.metadata
        }
        
        if format == 'msgpack':
            return msgpack.packb(data, use_bin_type=True)
        else:
            return json.dumps(data, default=str).encode()
    
    @staticmethod
    def from_wire_format(data: bytes, format: str = 'msgpack') -> Event:
        """
        Deserialize event from IPC transmission.
        
        Args:
            data: Serialized bytes
            format: Serialization format
            
        Returns:
            Deserialized Event
        """
        if format == 'msgpack':
            event_dict = msgpack.unpackb(data, raw=False)
        else:
            event_dict = json.loads(data.decode())
        
        # Reconstruct datetime
        if event_dict.get('timestamp'):
            event_dict['timestamp'] = datetime.fromisoformat(event_dict['timestamp'])
        
        # Reconstruct payload types
        event_dict['payload'] = EventSerializer._restore_payload(event_dict.get('payload', {}))
        
        return Event(**event_dict)
    
    @staticmethod
    def _prepare_payload(payload: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare payload for serialization"""
        prepared = {}
        for key, value in payload.items():
            if isinstance(value, Decimal):
                prepared[key] = {'_type': 'Decimal', 'value': str(value)}
            elif isinstance(value, datetime):
                prepared[key] = {'_type': 'datetime', 'value': value.isoformat()}
            else:
                prepared[key] = value
        return prepared
    
    @staticmethod
    def _restore_payload(payload: Dict[str, Any]) -> Dict[str, Any]:
        """Restore payload types after deserialization"""
        restored = {}
        for key, value in payload.items():
            if isinstance(value, dict) and '_type' in value:
                if value['_type'] == 'Decimal':
                    restored[key] = Decimal(value['value'])
                elif value['_type'] == 'datetime':
                    restored[key] = datetime.fromisoformat(value['value'])
            else:
                restored[key] = value
        return restored
```

### 1.2 IPC Implementation
ZeroMQ-based IPC with Protocol compliance:

```python
# src/core/multiprocess/ipc.py
import zmq
from typing import Dict, Any, Optional, List, Tuple
import logging
from dataclasses import dataclass, field

from ..events.serialization import EventSerializer
from ..events.types import Event

logger = logging.getLogger(__name__)


@dataclass
class IPCConfig:
    """Configuration for IPC connections"""
    transport: str = "tcp"  # tcp, ipc, inproc
    host: str = "127.0.0.1"
    base_port: int = 5550
    serialization: str = "msgpack"  # msgpack, json
    
    def get_address(self, channel: str) -> str:
        """Get ZMQ address for channel"""
        port_map = {
            'features': self.base_port,
            'signals': self.base_port + 1,
            'orders': self.base_port + 2,
            'fills': self.base_port + 3,
            'control': self.base_port + 4,
        }
        port = port_map.get(channel, self.base_port + 10)
        
        if self.transport == "tcp":
            return f"tcp://{self.host}:{port}"
        elif self.transport == "ipc":
            return f"ipc:///tmp/admf_{channel}.ipc"
        else:
            return f"inproc://{channel}"


class ZMQPublisher:
    """ZeroMQ publisher implementation"""
    
    def __init__(self, context: zmq.Context, address: str, config: IPCConfig):
        self.socket = context.socket(zmq.PUB)
        self.socket.bind(address)
        self.config = config
        self.published_count = 0
        
    def publish(self, topic: str, event: Event) -> None:
        """Publish event to topic"""
        # Serialize event
        data = EventSerializer.to_wire_format(event, self.config.serialization)
        
        # Send as multipart message [topic, data]
        self.socket.send_multipart([topic.encode(), data])
        self.published_count += 1
        
    def close(self) -> None:
        """Close socket"""
        self.socket.close()


class ZMQSubscriber:
    """ZeroMQ subscriber implementation"""
    
    def __init__(self, context: zmq.Context, address: str, config: IPCConfig):
        self.socket = context.socket(zmq.SUB)
        self.socket.connect(address)
        self.config = config
        self.received_count = 0
        
    def subscribe(self, topics: List[str]) -> None:
        """Subscribe to topics"""
        for topic in topics:
            self.socket.setsockopt(zmq.SUBSCRIBE, topic.encode())
    
    def receive(self, timeout: Optional[float] = None) -> Optional[Tuple[str, Event]]:
        """Receive message with optional timeout"""
        # Set timeout if specified
        if timeout is not None:
            self.socket.setsockopt(zmq.RCVTIMEO, int(timeout * 1000))
        
        try:
            # Receive multipart message
            topic, data = self.socket.recv_multipart()
            
            # Deserialize event
            event = EventSerializer.from_wire_format(data, self.config.serialization)
            self.received_count += 1
            
            return topic.decode(), event
            
        except zmq.Again:
            # Timeout occurred
            return None
        finally:
            # Reset timeout
            if timeout is not None:
                self.socket.setsockopt(zmq.RCVTIMEO, -1)
    
    def close(self) -> None:
        """Close socket"""
        self.socket.close()


class IPCManager:
    """Manages IPC connections - implements IPCProtocol"""
    
    def __init__(self, config: Optional[IPCConfig] = None):
        self.config = config or IPCConfig()
        self.context = zmq.Context()
        self.publishers: Dict[str, ZMQPublisher] = {}
        self.subscribers: Dict[str, ZMQSubscriber] = {}
        
    def create_publisher(self, channel: str) -> ZMQPublisher:
        """Create publisher for channel"""
        if channel not in self.publishers:
            address = self.config.get_address(channel)
            self.publishers[channel] = ZMQPublisher(self.context, address, self.config)
            logger.info(f"Created publisher on {address}")
        return self.publishers[channel]
    
    def create_subscriber(self, channel: str) -> ZMQSubscriber:
        """Create subscriber for channel"""
        if channel not in self.subscribers:
            address = self.config.get_address(channel)
            self.subscribers[channel] = ZMQSubscriber(self.context, address, self.config)
            logger.info(f"Created subscriber on {address}")
        return self.subscribers[channel]
    
    def publish(self, topic: str, event: Event) -> None:
        """Publish event to topic (IPCProtocol compliance)"""
        # Determine channel from topic prefix
        channel = topic.split('.')[0].lower()
        publisher = self.create_publisher(channel)
        publisher.publish(topic, event)
    
    def subscribe(self, topics: List[str]) -> None:
        """Subscribe to topics (IPCProtocol compliance)"""
        # Group topics by channel
        channels = {}
        for topic in topics:
            channel = topic.split('.')[0].lower()
            if channel not in channels:
                channels[channel] = []
            channels[channel].append(topic)
        
        # Subscribe on each channel
        for channel, channel_topics in channels.items():
            subscriber = self.create_subscriber(channel)
            subscriber.subscribe(channel_topics)
    
    def receive(self, timeout: Optional[float] = None) -> Optional[Tuple[str, Event]]:
        """Receive from any subscribed channel (IPCProtocol compliance)"""
        # Poll all subscribers
        poller = zmq.Poller()
        for subscriber in self.subscribers.values():
            poller.register(subscriber.socket, zmq.POLLIN)
        
        # Wait for message
        socks = dict(poller.poll(timeout * 1000 if timeout else None))
        
        # Read from first available
        for subscriber in self.subscribers.values():
            if subscriber.socket in socks:
                return subscriber.receive(timeout=0)  # Non-blocking since we know data is ready
        
        return None
    
    def close(self) -> None:
        """Close all connections"""
        for publisher in self.publishers.values():
            publisher.close()
        for subscriber in self.subscribers.values():
            subscriber.close()
        self.context.term()
    
    def get_stats(self) -> Dict[str, Any]:
        """Get IPC statistics"""
        return {
            'publishers': {
                name: {'published': pub.published_count}
                for name, pub in self.publishers.items()
            },
            'subscribers': {
                name: {'received': sub.received_count}
                for name, sub in self.subscribers.items()
            }
        }
```

## Phase 2: Event Tracing Across Processes

### 2.1 Distributed Tracing
Extend tracing to work across process boundaries:

```python
# src/core/events/tracing/distributed.py
from typing import Dict, Any, Optional
import uuid
from dataclasses import dataclass

from ..types import Event
from .traced_event import TracedEvent


@dataclass
class TraceContext:
    """Context for distributed tracing across processes"""
    trace_id: str
    span_id: str
    parent_span_id: Optional[str] = None
    baggage: Dict[str, Any] = field(default_factory=dict)
    
    def to_headers(self) -> Dict[str, str]:
        """Convert to headers for IPC transmission"""
        headers = {
            'X-Trace-Id': self.trace_id,
            'X-Span-Id': self.span_id,
        }
        if self.parent_span_id:
            headers['X-Parent-Span-Id'] = self.parent_span_id
        
        # Add baggage items
        for key, value in self.baggage.items():
            headers[f'X-Baggage-{key}'] = str(value)
        
        return headers
    
    @classmethod
    def from_headers(cls, headers: Dict[str, str]) -> Optional['TraceContext']:
        """Reconstruct from headers"""
        if 'X-Trace-Id' not in headers:
            return None
        
        # Extract baggage
        baggage = {}
        for key, value in headers.items():
            if key.startswith('X-Baggage-'):
                baggage_key = key[10:]  # Remove 'X-Baggage-' prefix
                baggage[baggage_key] = value
        
        return cls(
            trace_id=headers['X-Trace-Id'],
            span_id=headers['X-Span-Id'],
            parent_span_id=headers.get('X-Parent-Span-Id'),
            baggage=baggage
        )


class DistributedTracer:
    """Handles distributed tracing across processes"""
    
    def __init__(self, process_id: str):
        self.process_id = process_id
        self.active_traces: Dict[str, TraceContext] = {}
        
    def inject_trace_context(self, event: Event) -> Event:
        """Inject trace context into event for IPC"""
        # Check if event already has trace context
        trace_context = self._get_or_create_trace_context(event)
        
        # Add to event metadata
        if 'trace_context' not in event.metadata:
            event.metadata['trace_context'] = trace_context.to_headers()
        
        # Store active trace
        self.active_traces[event.event_id] = trace_context
        
        return event
    
    def extract_trace_context(self, event: Event) -> Optional[TraceContext]:
        """Extract trace context from event after IPC"""
        headers = event.metadata.get('trace_context', {})
        return TraceContext.from_headers(headers)
    
    def create_span(self, event: Event, operation: str) -> TracedEvent:
        """Create a new span for cross-process operation"""
        trace_context = self.extract_trace_context(event)
        
        if not trace_context:
            # Start new trace
            trace_context = TraceContext(
                trace_id=str(uuid.uuid4()),
                span_id=str(uuid.uuid4())
            )
        else:
            # Continue trace with new span
            trace_context = TraceContext(
                trace_id=trace_context.trace_id,
                span_id=str(uuid.uuid4()),
                parent_span_id=trace_context.span_id,
                baggage=trace_context.baggage
            )
        
        # Create traced event
        traced = TracedEvent(
            event=event,
            process_id=self.process_id,
            operation=operation,
            trace_context=trace_context
        )
        
        return traced
    
    def _get_or_create_trace_context(self, event: Event) -> TraceContext:
        """Get existing or create new trace context"""
        # Check if event has correlation_id (existing trace)
        if event.correlation_id and event.correlation_id in self.active_traces:
            return self.active_traces[event.correlation_id]
        
        # Check metadata
        if 'trace_context' in event.metadata:
            context = TraceContext.from_headers(event.metadata['trace_context'])
            if context:
                return context
        
        # Create new trace
        return TraceContext(
            trace_id=event.correlation_id or str(uuid.uuid4()),
            span_id=str(uuid.uuid4())
        )
```

### 2.2 Cross-Process Event Bus
Event bus that works across processes:

```python
# src/core/events/distributed_bus.py
from typing import Dict, Any, Optional, List, Callable
import logging

from .bus import EventBus
from .types import Event, EventType
from ..multiprocess.ipc import IPCManager
from .tracing.distributed import DistributedTracer

logger = logging.getLogger(__name__)


class DistributedEventBus(EventBus):
    """
    Event bus that can publish/subscribe across process boundaries.
    
    Extends the regular EventBus with IPC capabilities while maintaining
    the same interface for backward compatibility.
    """
    
    def __init__(self, bus_id: str, ipc_manager: Optional[IPCManager] = None,
                 process_id: Optional[str] = None):
        super().__init__(bus_id)
        self.ipc_manager = ipc_manager
        self.process_id = process_id or bus_id
        self.distributed_tracer = DistributedTracer(self.process_id)
        
        # Track remote subscriptions
        self.remote_subscriptions: Dict[str, List[str]] = {}
        
    def publish(self, event: Event) -> None:
        """Publish event locally and remotely if configured"""
        # Local publish
        super().publish(event)
        
        # Remote publish if IPC configured
        if self.ipc_manager and self._should_publish_remote(event):
            # Inject trace context
            event = self.distributed_tracer.inject_trace_context(event)
            
            # Publish to IPC
            topic = f"{event.event_type}.{event.source_id}"
            self.ipc_manager.publish(topic, event)
            
            logger.debug(f"Published {event.event_type} to IPC topic {topic}")
    
    def subscribe_remote(self, event_types: List[str], topics: List[str]) -> None:
        """Subscribe to remote events via IPC"""
        if not self.ipc_manager:
            raise RuntimeError("IPC manager required for remote subscriptions")
        
        # Track subscriptions
        for event_type in event_types:
            if event_type not in self.remote_subscriptions:
                self.remote_subscriptions[event_type] = []
            self.remote_subscriptions[event_type].extend(topics)
        
        # Subscribe via IPC
        self.ipc_manager.subscribe(topics)
        
        # Start receiving if not already
        if not hasattr(self, '_receiving'):
            self._start_receiving()
    
    def _should_publish_remote(self, event: Event) -> bool:
        """Determine if event should be published remotely"""
        # Configurable rules for what gets published to IPC
        remote_event_types = {
            EventType.BAR.value,
            EventType.SIGNAL.value,
            EventType.ORDER.value,
            EventType.FILL.value,
        }
        return event.event_type in remote_event_types
    
    def _start_receiving(self) -> None:
        """Start receiving remote events"""
        # In production, this would be in a separate thread
        # For now, it's called explicitly by the worker process
        self._receiving = True
    
    def receive_remote_events(self, timeout: Optional[float] = 0.001) -> int:
        """
        Receive and process remote events.
        
        Returns number of events processed.
        """
        if not self.ipc_manager:
            return 0
        
        count = 0
        while True:
            result = self.ipc_manager.receive(timeout)
            if not result:
                break
            
            topic, event = result
            
            # Extract trace context
            trace_context = self.distributed_tracer.extract_trace_context(event)
            if trace_context:
                # Continue distributed trace
                span = self.distributed_tracer.create_span(event, "receive_remote")
                logger.debug(f"Continuing trace {trace_context.trace_id} in {self.process_id}")
            
            # Publish locally
            super().publish(event)
            count += 1
        
        return count
```

## Phase 3: Worker Implementation

### 3.1 Base Worker Components
Composable worker using protocols:

```python
# src/core/multiprocess/worker.py
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
import logging
import signal
import time

from ..events.distributed_bus import DistributedEventBus
from ..events.tracing import EventTracer
from ..containers import Container, ContainerConfig
from .ipc import IPCManager, IPCConfig

logger = logging.getLogger(__name__)


@dataclass
class WorkerConfig:
    """Configuration for a worker process"""
    worker_id: str
    worker_type: str  # 'producer', 'strategy', 'portfolio', 'execution'
    ipc_config: IPCConfig = field(default_factory=IPCConfig)
    container_config: Optional[ContainerConfig] = None
    subscriptions: List[str] = field(default_factory=list)
    enable_tracing: bool = True
    
    
class BaseWorker:
    """
    Base worker implementation using composition.
    
    Not meant to be inherited - use composition to build specific workers.
    """
    
    def __init__(self, config: WorkerConfig):
        self.config = config
        self.worker_id = config.worker_id
        self.running = False
        
        # Composed components
        self.ipc_manager = IPCManager(config.ipc_config)
        self.event_bus = DistributedEventBus(
            bus_id=f"worker_{self.worker_id}",
            ipc_manager=self.ipc_manager,
            process_id=self.worker_id
        )
        
        # Optional components
        self.container: Optional[Container] = None
        self.tracer: Optional[EventTracer] = None
        
        # Graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
    def setup(self) -> None:
        """Initialize worker components"""
        logger.info(f"Setting up worker {self.worker_id}")
        
        # Setup container if configured
        if self.config.container_config:
            self.container = Container(
                self.config.container_config,
                parent_event_bus=self.event_bus
            )
            self.container.initialize()
        
        # Setup tracing if enabled
        if self.config.enable_tracing:
            self.tracer = self.event_bus.enable_tracing({
                'correlation_id': f"worker_{self.worker_id}",
                'max_events': 10000
            })
        
        # Subscribe to remote topics
        if self.config.subscriptions:
            event_types = list(set(topic.split('.')[0] for topic in self.config.subscriptions))
            self.event_bus.subscribe_remote(event_types, self.config.subscriptions)
        
        self.running = True
        
    def run(self) -> None:
        """Main worker loop"""
        self.setup()
        
        logger.info(f"Worker {self.worker_id} starting main loop")
        
        try:
            while self.running:
                self.process_messages()
                
        except Exception as e:
            logger.error(f"Worker {self.worker_id} failed: {e}", exc_info=True)
            
        finally:
            self.cleanup()
    
    def process_messages(self) -> None:
        """Process incoming messages - override in specific workers"""
        # Receive remote events
        count = self.event_bus.receive_remote_events(timeout=0.1)
        
        # Let container execute if it has executable components
        if self.container and hasattr(self.container, 'execute'):
            self.container.execute()
    
    def cleanup(self) -> None:
        """Clean up worker resources"""
        logger.info(f"Cleaning up worker {self.worker_id}")
        
        if self.container:
            self.container.stop()
            self.container.cleanup()
        
        if self.tracer:
            # Save trace before shutdown
            trace_path = f"./traces/worker_{self.worker_id}_trace.jsonl"
            self.tracer.save_to_file(trace_path)
            logger.info(f"Saved trace to {trace_path}")
        
        self.ipc_manager.close()
        
    def get_status(self) -> Dict[str, Any]:
        """Get worker status and metrics"""
        status = {
            'worker_id': self.worker_id,
            'worker_type': self.config.worker_type,
            'is_running': self.running,
            'ipc_stats': self.ipc_manager.get_stats()
        }
        
        if self.container:
            status['container'] = self.container.get_status()
        
        if self.tracer:
            status['trace_summary'] = self.event_bus.get_tracer_summary()
        
        return status
    
    def _signal_handler(self, signum, frame):
        """Handle shutdown signals"""
        logger.info(f"Worker {self.worker_id} received signal {signum}")
        self.running = False
```

### 3.2 Specific Worker Types
Workers composed for specific roles:

```python
# src/core/multiprocess/workers/portfolio_worker.py
from typing import Dict, Any, List
import logging

from ..worker import BaseWorker, WorkerConfig
from ...events.types import EventType
from ...events.filters import strategy_filter, container_filter

logger = logging.getLogger(__name__)


class PortfolioWorker:
    """
    Portfolio worker that processes signals and generates orders.
    
    Uses composition to combine BaseWorker with portfolio-specific logic.
    """
    
    def __init__(self, portfolio_id: str, strategy_ids: List[str], 
                 portfolio_config: Dict[str, Any], ipc_config: IPCConfig):
        # Create worker config
        config = WorkerConfig(
            worker_id=f"portfolio_{portfolio_id}",
            worker_type='portfolio',
            ipc_config=ipc_config,
            container_config=self._build_container_config(portfolio_id, portfolio_config),
            subscriptions=self._build_subscriptions(strategy_ids)
        )
        
        # Compose base worker
        self.worker = BaseWorker(config)
        self.portfolio_id = portfolio_id
        self.strategy_ids = strategy_ids
        
    def run(self) -> None:
        """Run the portfolio worker"""
        # Customize setup
        self.worker.setup()
        
        # Add portfolio-specific event handlers
        self._setup_event_handlers()
        
        # Start processing
        self.worker.run()
    
    def _build_container_config(self, portfolio_id: str, 
                               portfolio_config: Dict[str, Any]) -> ContainerConfig:
        """Build container configuration for portfolio"""
        return ContainerConfig(
            name=f"portfolio_{portfolio_id}",
            container_id=f"portfolio_{portfolio_id}",
            components=['portfolio_manager', 'position_manager', 'risk_manager'],
            config=portfolio_config,
            container_type='portfolio'
        )
    
    def _build_subscriptions(self, strategy_ids: List[str]) -> List[str]:
        """Build IPC subscriptions for this portfolio"""
        subscriptions = []
        
        # Subscribe to signals from our strategies
        for strategy_id in strategy_ids:
            subscriptions.append(f"SIGNAL.{strategy_id}")
        
        # Subscribe to fills for this portfolio
        subscriptions.append(f"FILL.portfolio_{self.portfolio_id}")
        
        return subscriptions
    
    def _setup_event_handlers(self) -> None:
        """Setup portfolio-specific event filtering"""
        # Filter signals to only our strategies
        signal_filter_func = strategy_filter(self.strategy_ids)
        
        # Filter fills to only our portfolio
        fill_filter_func = container_filter(self.portfolio_id)
        
        # Apply filters via event bus subscriptions
        # These work on top of IPC topic filtering for extra safety
        if self.worker.container:
            portfolio_component = self.worker.container.get_component('portfolio_manager')
            if portfolio_component:
                self.worker.event_bus.subscribe(
                    EventType.SIGNAL.value,
                    portfolio_component.handle_signal,
                    filter_func=signal_filter_func
                )
                
                self.worker.event_bus.subscribe(
                    EventType.FILL.value,
                    portfolio_component.handle_fill,
                    filter_func=fill_filter_func
                )
```

### 3.3 Portfolio Batch Worker
For handling multiple portfolios per process:

```python
# src/core/multiprocess/workers/portfolio_batch_worker.py
from typing import Dict, Any, List
import numpy as np
import logging

from ..worker import BaseWorker, WorkerConfig
from ...containers import Container, ContainerConfig
from ...events.types import Event, EventType

logger = logging.getLogger(__name__)


class PortfolioBatchWorker:
    """
    Worker that handles multiple portfolios in a single process.
    
    Uses vectorized operations for efficiency.
    """
    
    def __init__(self, batch_id: int, portfolio_configs: List[Dict[str, Any]], 
                 ipc_config: IPCConfig):
        self.batch_id = batch_id
        self.portfolio_configs = portfolio_configs
        self.batch_size = len(portfolio_configs)
        
        # Create base worker
        config = WorkerConfig(
            worker_id=f"portfolio_batch_{batch_id}",
            worker_type='portfolio_batch',
            ipc_config=ipc_config,
            subscriptions=self._build_subscriptions()
        )
        
        self.worker = BaseWorker(config)
        
        # Batch processing components
        self.portfolio_states = None
        self.position_matrices = None
        self.strategy_masks = None
        
        # Individual portfolio containers for complex logic
        self.portfolios: List[Container] = []
        
    def setup(self) -> None:
        """Setup batch processing structures"""
        self.worker.setup()
        
        # Initialize numpy arrays for vectorized operations
        self._init_batch_structures()
        
        # Create portfolio containers
        self._create_portfolio_containers()
        
        # Setup vectorized event handler
        self.worker.event_bus.subscribe(
            EventType.SIGNAL.value,
            self._handle_signal_batch
        )
        
    def _init_batch_structures(self) -> None:
        """Initialize numpy arrays for batch processing"""
        # Portfolio states matrix
        self.portfolio_states = np.zeros((self.batch_size, 10))  # 10 state dimensions
        
        # Position tracking
        max_positions = 100
        self.position_matrices = np.zeros((self.batch_size, max_positions, 5))  # 5 fields per position
        
        # Strategy masks for filtering
        unique_strategies = self._get_unique_strategies()
        self.strategy_masks = np.zeros((self.batch_size, len(unique_strategies)), dtype=bool)
        
        # Build masks
        for i, config in enumerate(self.portfolio_configs):
            for j, strategy in enumerate(unique_strategies):
                if strategy in config.get('strategies', []):
                    self.strategy_masks[i, j] = True
    
    def _handle_signal_batch(self, event: Event) -> None:
        """Handle signal across portfolio batch"""
        signal = event.payload
        strategy_id = signal.get('strategy_id')
        
        # Find portfolios that care about this signal
        strategy_idx = self._get_strategy_index(strategy_id)
        if strategy_idx is None:
            return
        
        relevant_mask = self.strategy_masks[:, strategy_idx]
        relevant_indices = np.where(relevant_mask)[0]
        
        if len(relevant_indices) == 0:
            return
        
        # Vectorized risk checks
        risk_scores = self._calculate_risk_batch(relevant_indices, signal)
        allowed_mask = risk_scores < 0.95  # Risk threshold
        
        # Vectorized position sizing
        position_sizes = self._calculate_position_sizes_batch(
            relevant_indices[allowed_mask],
            signal
        )
        
        # Generate orders
        self._generate_orders_batch(relevant_indices[allowed_mask], signal, position_sizes)
    
    def _calculate_risk_batch(self, indices: np.ndarray, signal: Dict[str, Any]) -> np.ndarray:
        """Calculate risk scores for portfolio subset"""
        # Extract relevant portfolio states
        states = self.portfolio_states[indices]
        
        # Vectorized risk calculation
        # Example: position count + volatility estimate
        position_counts = states[:, 0]  # First column is position count
        volatilities = states[:, 1]  # Second column is volatility
        
        risk_scores = (position_counts / 10.0) * 0.5 + (volatilities / 0.02) * 0.5
        
        return risk_scores
    
    def _calculate_position_sizes_batch(self, indices: np.ndarray, 
                                       signal: Dict[str, Any]) -> np.ndarray:
        """Calculate position sizes using vectorized operations"""
        # Get portfolio values
        portfolio_values = self.portfolio_states[indices, 2]  # Third column is portfolio value
        
        # Base position size (e.g., 2% of portfolio)
        base_sizes = portfolio_values * 0.02
        
        # Adjust by signal strength
        signal_strength = signal.get('strength', 1.0)
        adjusted_sizes = base_sizes * signal_strength
        
        return adjusted_sizes
    
    def _generate_orders_batch(self, indices: np.ndarray, signal: Dict[str, Any], 
                              sizes: np.ndarray) -> None:
        """Generate orders for multiple portfolios"""
        for idx, size in zip(indices, sizes):
            portfolio_id = self.portfolio_configs[idx]['id']
            
            # Create order event
            order_event = Event(
                event_type=EventType.ORDER.value,
                source_id=f"portfolio_batch_{self.batch_id}",
                container_id=portfolio_id,
                payload={
                    'symbol': signal['symbol'],
                    'side': signal['direction'],
                    'quantity': int(size / signal.get('price', 100)),  # Convert to shares
                    'order_type': 'MARKET',
                    'portfolio_id': portfolio_id
                }
            )
            
            # Publish order
            self.worker.event_bus.publish(order_event)
```

## Phase 4: Process Management

### 4.1 Process Manager
Manages worker lifecycle:

```python
# src/core/multiprocess/manager.py
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
import multiprocessing as mp
import logging
import time
import psutil

from .worker import WorkerConfig
from .workers import ProducerWorker, StrategyWorker, PortfolioWorker, ExecutionWorker
from .ipc import IPCConfig

logger = logging.getLogger(__name__)


@dataclass
class ProcessInfo:
    """Information about a managed process"""
    process: mp.Process
    worker_config: WorkerConfig
    start_time: float
    restart_count: int = 0
    last_health_check: float = field(default_factory=time.time)


class ProcessManager:
    """
    Manages worker processes with health checking and restart capability.
    
    Implements ProcessManagerProtocol using composition.
    """
    
    def __init__(self, ipc_config: Optional[IPCConfig] = None):
        self.ipc_config = ipc_config or IPCConfig()
        self.processes: Dict[str, ProcessInfo] = {}
        self.running = False
        
        # Health check configuration
        self.health_check_interval = 5.0  # seconds
        self.max_restart_attempts = 3
        
    def start_worker(self, worker_config: Dict[str, Any]) -> str:
        """Start a new worker process"""
        worker_id = worker_config['worker_id']
        worker_type = worker_config['worker_type']
        
        # Create appropriate worker
        worker = self._create_worker(worker_type, worker_config)
        
        # Create process
        process = mp.Process(
            target=worker.run,
            name=f"Worker-{worker_id}"
        )
        
        # Start process
        process.start()
        logger.info(f"Started worker {worker_id} (PID: {process.pid})")
        
        # Track process
        self.processes[worker_id] = ProcessInfo(
            process=process,
            worker_config=WorkerConfig(**worker_config),
            start_time=time.time()
        )
        
        return worker_id
    
    def stop_worker(self, worker_id: str) -> None:
        """Stop a worker process gracefully"""
        if worker_id not in self.processes:
            logger.warning(f"Worker {worker_id} not found")
            return
        
        info = self.processes[worker_id]
        process = info.process
        
        if process.is_alive():
            # Try graceful shutdown
            process.terminate()
            process.join(timeout=5.0)
            
            # Force kill if needed
            if process.is_alive():
                logger.warning(f"Force killing worker {worker_id}")
                process.kill()
                process.join()
        
        # Remove from tracking
        del self.processes[worker_id]
        logger.info(f"Stopped worker {worker_id}")
    
    def get_worker_status(self, worker_id: str) -> Dict[str, Any]:
        """Get status of specific worker"""
        if worker_id not in self.processes:
            return {'error': 'Worker not found'}
        
        info = self.processes[worker_id]
        process = info.process
        
        status = {
            'worker_id': worker_id,
            'pid': process.pid,
            'is_alive': process.is_alive(),
            'uptime': time.time() - info.start_time,
            'restart_count': info.restart_count
        }
        
        # Add process metrics if available
        try:
            proc = psutil.Process(process.pid)
            status['cpu_percent'] = proc.cpu_percent()
            status['memory_mb'] = proc.memory_info().rss / 1024 / 1024
        except:
            pass
        
        return status
    
    def health_check(self) -> Dict[str, Any]:
        """Check health of all workers"""
        now = time.time()
        health_report = {
            'timestamp': now,
            'workers': {}
        }
        
        for worker_id, info in list(self.processes.items()):
            # Check if process is alive
            if not info.process.is_alive():
                logger.error(f"Worker {worker_id} is dead")
                
                # Attempt restart if under limit
                if info.restart_count < self.max_restart_attempts:
                    logger.info(f"Attempting restart of {worker_id}")
                    self._restart_worker(worker_id)
                    health_report['workers'][worker_id] = {'status': 'restarted'}
                else:
                    logger.error(f"Worker {worker_id} exceeded restart limit")
                    health_report['workers'][worker_id] = {'status': 'failed'}
            else:
                health_report['workers'][worker_id] = self.get_worker_status(worker_id)
        
        return health_report
    
    def _create_worker(self, worker_type: str, config: Dict[str, Any]) -> Any:
        """Create appropriate worker instance"""
        # Add IPC config
        config['ipc_config'] = self.ipc_config
        
        if worker_type == 'producer':
            return ProducerWorker(**config)
        elif worker_type == 'strategy':
            return StrategyWorker(**config)
        elif worker_type == 'portfolio':
            return PortfolioWorker(**config)
        elif worker_type == 'portfolio_batch':
            return PortfolioBatchWorker(**config)
        elif worker_type == 'execution':
            return ExecutionWorker(**config)
        else:
            raise ValueError(f"Unknown worker type: {worker_type}")
    
    def _restart_worker(self, worker_id: str) -> None:
        """Restart a failed worker"""
        info = self.processes[worker_id]
        
        # Clean up old process
        if info.process.is_alive():
            info.process.terminate()
        info.process.join()
        
        # Create new process with same config
        worker = self._create_worker(
            info.worker_config.worker_type,
            info.worker_config.__dict__
        )
        
        process = mp.Process(
            target=worker.run,
            name=f"Worker-{worker_id}"
        )
        process.start()
        
        # Update tracking
        info.process = process
        info.restart_count += 1
        info.start_time = time.time()
        
        logger.info(f"Restarted worker {worker_id} (attempt {info.restart_count})")
    
    def start_monitoring(self) -> None:
        """Start health monitoring loop"""
        self.running = True
        
        while self.running:
            time.sleep(self.health_check_interval)
            self.health_check()
    
    def shutdown(self) -> None:
        """Shutdown all workers and cleanup"""
        logger.info("Shutting down process manager")
        self.running = False
        
        # Stop all workers
        for worker_id in list(self.processes.keys()):
            self.stop_worker(worker_id)
        
        logger.info("Process manager shutdown complete")
```

## Phase 5: Integration with Existing Code

### 5.1 Multi-Process Topology Runner
Extends existing topology runner:

```python
# src/core/coordinator/multiprocess_runner.py
from typing import Dict, Any, List, Optional
import logging
import time

from .topology_runner import TopologyRunner
from ..multiprocess.manager import ProcessManager
from ..multiprocess.ipc import IPCConfig

logger = logging.getLogger(__name__)


class MultiProcessTopologyRunner(TopologyRunner):
    """
    Extends TopologyRunner to support multi-process execution.
    
    Uses composition to add multi-process capabilities while
    maintaining backward compatibility.
    """
    
    def __init__(self, topology_builder: Optional[Any] = None):
        super().__init__(topology_builder)
        self.process_manager = None
        
    def run_topology(self, topology_name: str, config: Dict[str, Any],
                    execution_id: Optional[str] = None) -> Dict[str, Any]:
        """Execute topology with optional multi-process support"""
        # Check if multi-process is enabled
        mp_config = config.get('multiprocess', {})
        if mp_config.get('enabled', False):
            return self._run_multiprocess_topology(topology_name, config, execution_id)
        else:
            # Fall back to single process
            return super().run_topology(topology_name, config, execution_id)
    
    def _run_multiprocess_topology(self, topology_name: str, 
                                  config: Dict[str, Any],
                                  execution_id: str) -> Dict[str, Any]:
        """Run topology using multiple processes"""
        logger.info(f"Running {topology_name} in multi-process mode")
        
        # Initialize process manager
        ipc_config = IPCConfig(**config.get('multiprocess', {}).get('ipc', {}))
        self.process_manager = ProcessManager(ipc_config)
        
        try:
            # Start producer process (data + shared features)
            producer_id = self._start_producer(config)
            
            # Start strategy workers
            strategy_workers = self._start_strategy_workers(config)
            
            # Start portfolio workers or batches
            portfolio_workers = self._start_portfolio_workers(config)
            
            # Start execution worker
            execution_id = self._start_execution_worker(config)
            
            # Monitor execution
            result = self._monitor_execution(config)
            
            return result
            
        finally:
            # Cleanup
            if self.process_manager:
                self.process_manager.shutdown()
    
    def _start_producer(self, config: Dict[str, Any]) -> str:
        """Start producer process for data and features"""
        producer_config = {
            'worker_id': 'producer_main',
            'worker_type': 'producer',
            'data_config': config.get('data', {}),
            'feature_config': config.get('features', {})
        }
        
        return self.process_manager.start_worker(producer_config)
    
    def _start_strategy_workers(self, config: Dict[str, Any]) -> List[str]:
        """Start strategy worker processes"""
        workers = []
        strategies = config.get('strategies', [])
        
        # Group strategies by computational requirements
        strategy_groups = self._group_strategies(strategies)
        
        for i, group in enumerate(strategy_groups):
            worker_config = {
                'worker_id': f'strategy_worker_{i}',
                'worker_type': 'strategy',
                'strategies': group
            }
            
            worker_id = self.process_manager.start_worker(worker_config)
            workers.append(worker_id)
        
        return workers
    
    def _start_portfolio_workers(self, config: Dict[str, Any]) -> List[str]:
        """Start portfolio workers based on configuration"""
        workers = []
        portfolios = config.get('portfolios', [])
        mp_config = config.get('multiprocess', {})
        
        # Determine batching strategy
        batch_size = mp_config.get('portfolio_batch_size', 1)
        
        if batch_size > 1:
            # Batch mode
            for i in range(0, len(portfolios), batch_size):
                batch = portfolios[i:i+batch_size]
                worker_config = {
                    'worker_id': f'portfolio_batch_{i//batch_size}',
                    'worker_type': 'portfolio_batch',
                    'portfolio_configs': batch
                }
                worker_id = self.process_manager.start_worker(worker_config)
                workers.append(worker_id)
        else:
            # Individual mode
            for i, portfolio in enumerate(portfolios):
                worker_config = {
                    'worker_id': f'portfolio_{i}',
                    'worker_type': 'portfolio',
                    'portfolio_id': portfolio.get('id', str(i)),
                    'strategy_ids': portfolio.get('strategies', []),
                    'portfolio_config': portfolio
                }
                worker_id = self.process_manager.start_worker(worker_config)
                workers.append(worker_id)
        
        return workers
```

### 5.2 Configuration Schema
Update configuration to support multi-process:

```yaml
# config/multiprocess_example.yaml
# Multi-process configuration example

# Data configuration (runs in producer process)
data:
  source: csv
  symbols: ['AAPL', 'GOOGL', 'MSFT']
  start_date: '2023-01-01'
  end_date: '2023-12-31'
  
# Feature computation (shared in producer)
features:
  - name: sma_20
    type: simple_moving_average
    params: {period: 20}
  - name: rsi_14
    type: relative_strength_index
    params: {period: 14}

# Strategies (distributed across workers)
strategies:
  # ML strategy - needs dedicated worker
  - id: ml_momentum_1
    type: ml_model
    model_path: models/lstm_momentum.pkl
    requires_gpu: true
    features: ['sma_20', 'rsi_14', 'volume']
    
  # Traditional strategies - can share worker
  - id: trend_follow_1
    type: trend_following
    params:
      entry_threshold: 0.02
      exit_threshold: -0.01
      
  - id: mean_revert_1
    type: mean_reversion
    params:
      zscore_entry: 2.0
      zscore_exit: 0.5

# Portfolios with parameter sweeps
portfolios:
  - id: portfolio_conservative
    strategies: ['ml_momentum_1', 'trend_follow_1']
    risk_params:
      max_position_size: 0.02
      max_portfolio_risk: 0.06
      
  - id: portfolio_moderate
    strategies: ['ml_momentum_1', 'mean_revert_1']
    risk_params:
      max_position_size: 0.05
      max_portfolio_risk: 0.15
      
  - id: portfolio_aggressive
    strategies: ['all']
    risk_params:
      max_position_size: 0.10
      max_portfolio_risk: 0.30

# Multi-process configuration
multiprocess:
  enabled: true
  
  # IPC configuration
  ipc:
    transport: tcp  # tcp, ipc, inproc
    host: 127.0.0.1
    base_port: 5550
    serialization: msgpack
    
  # Worker allocation
  strategy_workers: 2  # Number of strategy worker processes
  portfolio_batch_size: 1  # Portfolios per worker (1 = individual)
  
  # Performance tuning
  message_batch_size: 100
  polling_timeout: 0.001  # seconds

# Event tracing configuration
execution:
  enable_event_tracing: true
  trace_settings:
    storage_backend: 'hierarchical'
    trace_dir: './traces/multiprocess'
    max_events: 50000
    enable_distributed_tracing: true
```

## Testing Strategy

### Unit Tests
Test each component in isolation:

```python
# tests/test_multiprocess/test_ipc.py
def test_event_serialization():
    """Test event serialization/deserialization"""
    event = Event(
        event_type=EventType.SIGNAL.value,
        payload={'symbol': 'AAPL', 'price': Decimal('150.25')}
    )
    
    # Serialize
    data = EventSerializer.to_wire_format(event)
    
    # Deserialize
    recovered = EventSerializer.from_wire_format(data)
    
    assert recovered.event_type == event.event_type
    assert recovered.payload['price'] == Decimal('150.25')

def test_ipc_pubsub():
    """Test IPC publish/subscribe"""
    config = IPCConfig(transport='inproc')
    manager = IPCManager(config)
    
    # Publish
    event = create_test_event()
    manager.publish('SIGNAL.test', event)
    
    # Subscribe and receive
    manager.subscribe(['SIGNAL.test'])
    topic, received = manager.receive(timeout=1.0)
    
    assert topic == 'SIGNAL.test'
    assert received.event_id == event.event_id
```

### Integration Tests
Test process interaction:

```python
# tests/test_multiprocess/test_workers.py
def test_portfolio_worker():
    """Test portfolio worker in isolation"""
    worker = PortfolioWorker(
        portfolio_id='test_1',
        strategy_ids=['strat_1'],
        portfolio_config={'initial_capital': 100000},
        ipc_config=IPCConfig(transport='inproc')
    )
    
    # Run for short time
    worker.worker.setup()
    
    # Send test signal
    signal_event = Event(
        event_type=EventType.SIGNAL.value,
        payload={'strategy_id': 'strat_1', 'symbol': 'AAPL'}
    )
    worker.worker.event_bus.publish(signal_event)
    
    # Check order generated
    # ...

def test_distributed_tracing():
    """Test tracing across process boundaries"""
    # Start two workers with tracing
    producer = ProducerWorker(config1)
    consumer = PortfolioWorker(config2)
    
    # Send traced event
    event = create_test_event()
    trace_context = producer.tracer.start_trace(event)
    
    # Verify trace continues in consumer
    # ...
```

### Performance Tests
Verify multi-process benefits:

```python
# tests/test_multiprocess/test_performance.py
def test_multiprocess_scaling():
    """Test performance scaling with multiple processes"""
    configs = [
        {'multiprocess': {'enabled': False}},  # Baseline
        {'multiprocess': {'enabled': True, 'portfolio_batch_size': 1}},
        {'multiprocess': {'enabled': True, 'portfolio_batch_size': 10}},
    ]
    
    for config in configs:
        start = time.time()
        runner = MultiProcessTopologyRunner()
        result = runner.run_topology('backtest', config)
        duration = time.time() - start
        
        print(f"Config: {config}, Duration: {duration:.2f}s")
```

## Rollout Plan

### Week 1-2: Foundation
1. Implement IPC layer with Protocol design
2. Add event serialization
3. Test basic pub/sub

### Week 3-4: Workers
1. Implement BaseWorker with composition
2. Create PortfolioWorker
3. Test single portfolio in separate process

### Week 5-6: Integration
1. Create MultiProcessTopologyRunner
2. Add process management
3. Test with existing configs

### Week 7-8: Optimization
1. Implement batch workers
2. Add distributed tracing
3. Performance testing

### Week 9-10: Production
1. Add health monitoring
2. Implement graceful shutdown
3. Documentation and training

## Key Benefits

1. **Maintains Architecture**: Protocol + Composition throughout
2. **Backward Compatible**: Single-process mode still works
3. **Event Tracing**: Complete tracing across processes
4. **Incremental**: Can implement phase by phase
5. **Testable**: Each component can be tested in isolation

## Conclusion

This implementation plan provides a clear path to add multi-process capabilities while maintaining your clean architecture principles. The use of protocols and composition ensures no inheritance is needed, and the distributed event bus maintains complete event tracing across process boundaries.

The phased approach allows you to see benefits early (Phase 2) while building toward the complete solution. Each phase is independently valuable and can be deployed to production.