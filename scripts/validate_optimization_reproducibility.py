#!/usr/bin/env python3\n\"\"\"\nOptimization Reproducibility Validation Script\n\nThis script validates that optimization results can be exactly reproduced\nby running the final optimized configuration on the test dataset.\n\nUsage:\n    python scripts/validate_optimization_reproducibility.py step1\n    python scripts/validate_optimization_reproducibility.py step2\n    python scripts/validate_optimization_reproducibility.py phase1\n\"\"\"\n\nimport sys\nimport os\nimport json\nimport argparse\nfrom pathlib import Path\nfrom typing import Dict, Any, Tuple\nimport logging\n\n# Add project root to path\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\nfrom src.core.logging import ComponentLogger\n\n\nclass OptimizationValidationFramework:\n    \"\"\"Framework for validating optimization result reproducibility.\"\"\"\n    \n    def __init__(self, step_name: str):\n        self.step_name = step_name\n        self.logger = ComponentLogger(\"optimization_validator\", f\"validation_{step_name}\")\n        self.config_dir = project_root / \"config\"\n        self.results_dir = project_root / \"results\" / \"validation\"\n        self.results_dir.mkdir(parents=True, exist_ok=True)\n    \n    def validate_step_reproducibility(self) -> Dict[str, Any]:\n        \"\"\"Validate that optimization results are reproducible.\"\"\"\n        \n        # 1. Check that optimization was completed\n        optimization_results = self._load_optimization_results()\n        if not optimization_results:\n            return {\n                'success': False,\n                'error': 'No optimization results found',\n                'step': self.step_name\n            }\n        \n        # 2. Check that final config was saved\n        final_config_path = self.config_dir / f\"{self.step_name}_final.yaml\"\n        if not final_config_path.exists():\n            return {\n                'success': False,\n                'error': f'Final config not found: {final_config_path}',\n                'step': self.step_name\n            }\n        \n        # 3. Run validation backtest\n        validation_results = self._run_validation_backtest(final_config_path)\n        if not validation_results:\n            return {\n                'success': False,\n                'error': 'Validation backtest failed',\n                'step': self.step_name\n            }\n        \n        # 4. Compare results\n        comparison_results = self._compare_results(\n            optimization_results['test_performance'],\n            validation_results\n        )\n        \n        # 5. Generate validation report\n        validation_report = {\n            'success': comparison_results['exact_match'],\n            'step': self.step_name,\n            'optimization_results': optimization_results['test_performance'],\n            'validation_results': validation_results,\n            'comparison': comparison_results,\n            'timestamp': self._get_timestamp()\n        }\n        \n        # 6. Save validation report\n        self._save_validation_report(validation_report)\n        \n        # 7. Log results\n        self._log_validation_results(validation_report)\n        \n        return validation_report\n    \n    def _load_optimization_results(self) -> Dict[str, Any]:\n        \"\"\"Load optimization results from file.\"\"\"\n        results_path = self.results_dir / f\"{self.step_name}_optimization_results.json\"\n        \n        if not results_path.exists():\n            self.logger.log_validation_result(\n                \"load_optimization_results\",\n                False,\n                f\"Optimization results file not found: {results_path}\"\n            )\n            return {}\n        \n        try:\n            with open(results_path, 'r') as f:\n                results = json.load(f)\n            \n            self.logger.log_validation_result(\n                \"load_optimization_results\",\n                True,\n                f\"Loaded optimization results from {results_path}\"\n            )\n            return results\n            \n        except Exception as e:\n            self.logger.log_validation_result(\n                \"load_optimization_results\",\n                False,\n                f\"Error loading optimization results: {e}\"\n            )\n            return {}\n    \n    def _run_validation_backtest(self, config_path: Path) -> Dict[str, Any]:\n        \"\"\"Run validation backtest using the final optimized configuration.\"\"\"\n        try:\n            # Import main execution module\n            from src.main import run_backtest\n            \n            # Run backtest on test dataset\n            results = run_backtest(\n                config_path=str(config_path),\n                dataset='test',\n                mode='validation'\n            )\n            \n            self.logger.log_validation_result(\n                \"run_validation_backtest\",\n                True,\n                f\"Validation backtest completed successfully\"\n            )\n            \n            return results\n            \n        except Exception as e:\n            self.logger.log_validation_result(\n                \"run_validation_backtest\",\n                False,\n                f\"Validation backtest failed: {e}\"\n            )\n            return {}\n    \n    def _compare_results(self, optimization_results: Dict[str, Any], validation_results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Compare optimization and validation results for exact match.\"\"\"\n        \n        comparison = {\n            'exact_match': True,\n            'differences': {},\n            'tolerance_checks': {},\n            'trade_count_match': True,\n            'performance_metrics': {}\n        }\n        \n        # Define critical metrics that must match exactly\n        critical_metrics = [\n            'total_return',\n            'sharpe_ratio',\n            'max_drawdown',\n            'trade_count',\n            'win_rate',\n            'profit_factor'\n        ]\n        \n        # Define tolerance for floating point comparisons\n        tolerance = 1e-10\n        \n        for metric in critical_metrics:\n            opt_value = optimization_results.get(metric)\n            val_value = validation_results.get(metric)\n            \n            if opt_value is None or val_value is None:\n                comparison['differences'][metric] = {\n                    'optimization': opt_value,\n                    'validation': val_value,\n                    'status': 'missing_data'\n                }\n                comparison['exact_match'] = False\n                continue\n            \n            # For trade count, require exact integer match\n            if metric == 'trade_count':\n                if opt_value != val_value:\n                    comparison['differences'][metric] = {\n                        'optimization': opt_value,\n                        'validation': val_value,\n                        'difference': abs(opt_value - val_value),\n                        'status': 'exact_mismatch'\n                    }\n                    comparison['exact_match'] = False\n                    comparison['trade_count_match'] = False\n            else:\n                # For floating point metrics, use tolerance\n                difference = abs(float(opt_value) - float(val_value))\n                tolerance_passed = difference <= tolerance\n                \n                comparison['tolerance_checks'][metric] = {\n                    'optimization': opt_value,\n                    'validation': val_value,\n                    'difference': difference,\n                    'tolerance': tolerance,\n                    'passed': tolerance_passed\n                }\n                \n                if not tolerance_passed:\n                    comparison['differences'][metric] = {\n                        'optimization': opt_value,\n                        'validation': val_value,\n                        'difference': difference,\n                        'status': 'tolerance_exceeded'\n                    }\n                    comparison['exact_match'] = False\n        \n        # Calculate performance metrics comparison\n        if comparison['exact_match']:\n            comparison['performance_metrics']['match_quality'] = 'perfect'\n        elif len(comparison['differences']) == 0:\n            comparison['performance_metrics']['match_quality'] = 'excellent'\n        elif len(comparison['differences']) <= 2:\n            comparison['performance_metrics']['match_quality'] = 'acceptable'\n        else:\n            comparison['performance_metrics']['match_quality'] = 'poor'\n        \n        comparison['performance_metrics']['difference_count'] = len(comparison['differences'])\n        comparison['performance_metrics']['total_metrics_compared'] = len(critical_metrics)\n        \n        return comparison\n    \n    def _save_validation_report(self, report: Dict[str, Any]):\n        \"\"\"Save validation report to file.\"\"\"\n        report_path = self.results_dir / f\"{self.step_name}_validation_report.json\"\n        \n        try:\n            with open(report_path, 'w') as f:\n                json.dump(report, f, indent=2, default=str)\n            \n            self.logger.log_validation_result(\n                \"save_validation_report\",\n                True,\n                f\"Validation report saved to {report_path}\"\n            )\n            \n        except Exception as e:\n            self.logger.log_validation_result(\n                \"save_validation_report\",\n                False,\n                f\"Error saving validation report: {e}\"\n            )\n    \n    def _log_validation_results(self, report: Dict[str, Any]):\n        \"\"\"Log validation results with appropriate level.\"\"\"\n        \n        if report['success']:\n            self.logger.log_validation_result(\n                f\"{self.step_name}_optimization_reproducibility\",\n                True,\n                \"Optimization results exactly reproduced in validation\"\n            )\n            print(f\"✅ {self.step_name}: Optimization results validated successfully\")\n        else:\n            self.logger.log_validation_result(\n                f\"{self.step_name}_optimization_reproducibility\",\n                False,\n                f\"Optimization results validation failed: {report.get('error', 'Unknown error')}\"\n            )\n            print(f\"❌ {self.step_name}: Optimization results validation FAILED\")\n            \n            if 'comparison' in report and 'differences' in report['comparison']:\n                print(\"\\n🔍 Differences found:\")\n                for metric, diff in report['comparison']['differences'].items():\n                    print(f\"  • {metric}: optimization={diff['optimization']}, validation={diff['validation']}\")\n    \n    def _get_timestamp(self) -> str:\n        \"\"\"Get current timestamp for reporting.\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n\n\ndef main():\n    \"\"\"Main validation function.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Validate optimization result reproducibility\"\n    )\n    parser.add_argument(\n        \"step_name\",\n        help=\"Name of the step to validate (e.g., step1, step2, phase1)\"\n    )\n    parser.add_argument(\n        \"--verbose\", \"-v\",\n        action=\"store_true\",\n        help=\"Enable verbose logging\"\n    )\n    \n    args = parser.parse_args()\n    \n    # Configure logging\n    logging.basicConfig(\n        level=logging.DEBUG if args.verbose else logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n    \n    print(f\"🔍 Validating optimization reproducibility for {args.step_name}\")\n    print(\"=\" * 60)\n    \n    # Run validation\n    validator = OptimizationValidationFramework(args.step_name)\n    results = validator.validate_step_reproducibility()\n    \n    # Print summary\n    print(f\"\\n📊 Validation Summary:\")\n    print(f\"  Step: {results['step']}\")\n    print(f\"  Success: {results['success']}\")\n    \n    if results['success']:\n        print(f\"  Match Quality: {results['comparison']['performance_metrics']['match_quality']}\")\n        print(f\"  Metrics Compared: {results['comparison']['performance_metrics']['total_metrics_compared']}\")\n        print(f\"  Differences Found: {results['comparison']['performance_metrics']['difference_count']}\")\n    else:\n        print(f\"  Error: {results.get('error', 'Unknown error')}\")\n    \n    # Exit with appropriate code\n    sys.exit(0 if results['success'] else 1)\n\n\nif __name__ == \"__main__\":\n    main()\n