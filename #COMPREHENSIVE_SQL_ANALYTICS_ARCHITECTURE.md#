# ADMF-PC SQL Analytics Architecture
## Comprehensive Design for Scalable Trading System Analysis

### Executive Summary

This document outlines a hybrid SQL + Parquet architecture for ADMF-PC analytics that scales from hundreds to tens of thousands of strategies. The approach combines the query power of SQL with the efficiency of sparse Parquet storage, providing both interactive exploration capabilities and programmatic interfaces for systematic analysis.

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Data Storage Strategy](#data-storage-strategy)
3. [SQL Schema Design](#sql-schema-design)
4. [Interactive Interfaces](#interactive-interfaces)
5. [Performance & Scalability](#performance--scalability)
6. [Implementation Roadmap](#implementation-roadmap)
7. [Migration Strategy](#migration-strategy)
8. [Use Case Examples](#use-case-examples)
9. [Future Extensions](#future-extensions)

---

## Architecture Overview

### Core Philosophy

**SQL as Smart Index, Parquet as Data Store**

- **SQL Database**: Stores metadata, performance metrics, and file pointers
- **Parquet Files**: Store actual signal data, event traces, and classifier states
- **DuckDB**: Embedded database with excellent Parquet integration
- **Hybrid Queries**: SQL catalogs what to analyze, Parquet stores how to analyze it

### System Components

```
┌─────────────────────────────────────────────────────────────┐
│                    ADMF-PC Analytics Architecture            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Interactive Layer                                          │
│  ├── Raw SQL Interface (DuckDB CLI)                        │
│  ├── ADMF SQL Mode (admf analytics sql)                    │
│  ├── Jupyter Notebooks                                     │
│  └── Future: Web Dashboard                                 │
│                                                             │
│  Query Layer                                               │
│  ├── SQL Templates (common queries)                        │
│  ├── Custom Functions (trading-specific)                   │
│  ├── Query Builder (programmatic)                          │
│  └── Visualization Pipeline                                │
│                                                             │
│  Storage Layer                                             │
│  ├── SQL Catalog (DuckDB)                                 │
│  │   ├── Strategy metadata                                 │
│  │   ├── Performance metrics                               │
│  │   ├── File pointers                                     │
│  │   └── Pre-computed analytics                            │
│  │                                                         │
│  └── Parquet Data Store                                    │
│      ├── Signal files (sparse storage)                     │
│      ├── Classifier states (sparse storage)                │
│      ├── Event archives (full traces)                      │
│      └── Market data (optimized source data)               │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## Data Storage Strategy

### File Organization

```
workspace/
├── analytics.duckdb                    # SQL catalog database
├── data/
│   ├── SPY_1m.parquet                 # Market data (converted from CSV)
│   ├── QQQ_1m.parquet
│   └── sector_etfs.parquet
├── signals/
│   ├── momentum/
│   │   ├── mom_10_20_30_a1b2c3.parquet    # Individual strategy signals
│   │   ├── mom_20_30_70_b2c3d4.parquet
│   │   └── index.json                      # Strategy type metadata
│   ├── ma_crossover/
│   │   ├── ma_5_20_sl_1.0_c3d4e5.parquet
│   │   └── ma_10_50_sl_2.0_d4e5f6.parquet
│   └── mean_reversion/
│       └── mr_bb_20_2.0_e5f6g7.parquet
├── classifiers/
│   ├── regime/
│   │   ├── hmm_3state_f6g7h8.parquet      # Classifier state changes
│   │   └── trend_20_50_g7h8i9.parquet
│   └── volatility/
│       └── vol_garch_h8i9j0.parquet
├── events/
│   ├── run_abc123/
│   │   ├── strategy_events_part_001.parquet  # Event traces
│   │   └── portfolio_events_part_001.parquet
│   └── run_def456/
│       └── full_trace_events.parquet
└── analytics/
    ├── correlations.parquet              # Pre-computed strategy correlations
    ├── regime_performance.parquet        # Strategy × Classifier performance
    └── parameter_sensitivity.parquet     # Parameter sweep results
```

### Storage Principles

1. **Sparse Storage**: Only store changes (signals, regime transitions, events)
2. **Columnar Format**: Parquet for efficient analytics
3. **Partitioning**: By strategy type, date, or other logical boundaries
4. **Compression**: Automatic compression with snappy/zstd
5. **Type Safety**: Proper data types preserved across storage/retrieval

---

## SQL Schema Design

### Core Catalog Tables

```sql
-- =============================================
-- WORKSPACE AND RUN MANAGEMENT
-- =============================================

CREATE TABLE runs (
    run_id VARCHAR PRIMARY KEY,
    created_at TIMESTAMP,
    workflow_type VARCHAR,              -- 'grid_search', 'optimization', 'backtest'
    
    -- Data characteristics
    symbols VARCHAR[],
    timeframes VARCHAR[],
    start_date DATE,
    end_date DATE,
    total_bars INTEGER,
    
    -- Configuration
    config_file VARCHAR,
    config_hash VARCHAR,
    
    -- Execution details
    total_strategies INTEGER,
    total_classifiers INTEGER,
    total_combinations INTEGER,         -- strategies × classifiers
    
    -- Status and performance
    status VARCHAR,                     -- 'running', 'completed', 'failed'
    duration_seconds REAL,
    peak_memory_mb REAL,
    
    -- Storage details
    workspace_path VARCHAR,
    total_size_mb REAL,
    compression_ratio REAL
);

-- =============================================
-- STRATEGY CATALOG AND PERFORMANCE
-- =============================================

CREATE TABLE strategies (
    strategy_id VARCHAR PRIMARY KEY,
    run_id VARCHAR REFERENCES runs(run_id),
    
    -- Strategy definition
    strategy_type VARCHAR,              -- 'momentum', 'ma_crossover', 'mean_reversion'
    strategy_name VARCHAR,
    parameters JSONB,                   -- {sma_period: 20, rsi_threshold: 30, ...}
    
    -- File references
    signal_file_path VARCHAR,           -- 'signals/momentum/mom_20_30_a1b2c3.parquet'
    config_hash VARCHAR,                -- Hash of parameters for deduplication
    
    -- Signal characteristics
    total_bars INTEGER,
    signal_changes INTEGER,
    compression_ratio REAL,
    signal_frequency REAL,              -- signal_changes / total_bars
    
    -- Performance metrics (post-execution costs)
    total_return REAL,
    annualized_return REAL,
    volatility REAL,
    sharpe_ratio REAL,
    sortino_ratio REAL,
    calmar_ratio REAL,
    max_drawdown REAL,
    max_drawdown_duration INTEGER,
    
    -- Trade statistics
    total_trades INTEGER,
    winning_trades INTEGER,
    losing_trades INTEGER,
    win_rate REAL,
    profit_factor REAL,
    avg_win REAL,
    avg_loss REAL,
    largest_win REAL,
    largest_loss REAL,
    avg_trade_duration REAL,
    
    -- Risk metrics
    value_at_risk REAL,
    expected_shortfall REAL,
    beta REAL,
    
    -- Execution costs impact
    gross_return REAL,
    net_return REAL,
    gross_sharpe REAL,
    net_sharpe REAL,
    total_commission REAL,
    total_slippage REAL,
    cost_per_trade REAL,
    
    -- Timestamps
    created_at TIMESTAMP,
    processed_at TIMESTAMP,
    
    -- Computed flags
    is_profitable BOOLEAN GENERATED ALWAYS AS (total_return > 0) STORED,
    is_high_sharpe BOOLEAN GENERATED ALWAYS AS (sharpe_ratio > 1.5) STORED,
    is_low_drawdown BOOLEAN GENERATED ALWAYS AS (ABS(max_drawdown) < 0.15) STORED
);

-- =============================================
-- CLASSIFIER CATALOG
-- =============================================

CREATE TABLE classifiers (
    classifier_id VARCHAR PRIMARY KEY,
    run_id VARCHAR REFERENCES runs(run_id),
    
    -- Classifier definition
    classifier_type VARCHAR,            -- 'momentum_regime', 'volatility', 'trend'
    classifier_name VARCHAR,
    parameters JSONB,
    
    -- File references
    states_file_path VARCHAR,           -- 'classifiers/regime/hmm_3state_f6g7h8.parquet'
    config_hash VARCHAR,
    
    -- Classification characteristics
    total_bars INTEGER,
    regime_changes INTEGER,
    compression_ratio REAL,
    change_frequency REAL,              -- regime_changes / total_bars
    
    -- Regime statistics
    regime_counts JSONB,                -- {'TRENDING': 5000, 'VOLATILE': 2000, 'NEUTRAL': 3000}
    regime_durations JSONB,             -- {'TRENDING': 45.2, 'VOLATILE': 23.1, 'NEUTRAL': 67.3}
    transition_matrix JSONB,            -- Regime transition probabilities
    
    -- Quality metrics
    regime_stability REAL,             -- Average regime duration / total bars
    entropy REAL,                       -- Information content of regimes
    predictability REAL,               -- How predictable transitions are
    
    -- Timestamps
    created_at TIMESTAMP,
    processed_at TIMESTAMP
);

-- =============================================
-- STRATEGY-CLASSIFIER COMBINATIONS
-- =============================================

CREATE TABLE regime_performance (
    id SERIAL PRIMARY KEY,
    strategy_id VARCHAR REFERENCES strategies(strategy_id),
    classifier_id VARCHAR REFERENCES classifiers(classifier_id),
    regime VARCHAR,
    
    -- Regime-specific performance
    regime_return REAL,
    regime_sharpe REAL,
    regime_win_rate REAL,
    regime_trade_count INTEGER,
    regime_avg_duration REAL,
    regime_max_drawdown REAL,
    
    -- Time allocation
    regime_bar_count INTEGER,
    regime_time_pct REAL,              -- Percentage of time in this regime
    
    -- Combined scoring
    regime_weighted_return REAL,       -- return * time_pct
    regime_score REAL,                 -- Custom scoring function
    
    UNIQUE(strategy_id, classifier_id, regime)
);

-- =============================================
-- EVENT ARCHIVE CATALOG
-- =============================================

CREATE TABLE event_archives (
    archive_id VARCHAR PRIMARY KEY,
    run_id VARCHAR REFERENCES runs(run_id),
    container_id VARCHAR,
    container_type VARCHAR,             -- 'strategy', 'portfolio', 'execution'
    
    -- File references
    events_file_path VARCHAR,           -- 'events/run_abc/strategy_events_part_001.parquet'
    
    -- Event characteristics
    event_types VARCHAR[],              -- ['SIGNAL', 'ORDER', 'FILL']
    event_count_by_type JSONB,          -- {'SIGNAL': 1250, 'ORDER': 400, 'FILL': 380}
    total_events INTEGER,
    
    -- Time range
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    duration_seconds REAL,
    
    -- Storage details
    file_size_mb REAL,
    compression_ratio REAL,
    
    created_at TIMESTAMP
);

-- =============================================
-- PRE-COMPUTED ANALYTICS
-- =============================================

CREATE TABLE parameter_analysis (
    id SERIAL PRIMARY KEY,
    run_id VARCHAR REFERENCES runs(run_id),
    strategy_type VARCHAR,
    parameter_name VARCHAR,
    parameter_value REAL,
    
    -- Aggregated metrics across all strategies with this parameter value
    strategy_count INTEGER,
    avg_sharpe REAL,
    std_sharpe REAL,
    min_sharpe REAL,
    max_sharpe REAL,
    median_sharpe REAL,
    
    avg_return REAL,
    avg_drawdown REAL,
    avg_win_rate REAL,
    
    -- Rankings
    sharpe_rank INTEGER,               -- Rank by average Sharpe
    stability_rank INTEGER,            -- Rank by Sharpe stability (low std)
    
    computed_at TIMESTAMP,
    
    UNIQUE(run_id, strategy_type, parameter_name, parameter_value)
);

CREATE TABLE strategy_correlations (
    id SERIAL PRIMARY KEY,
    run_id VARCHAR REFERENCES runs(run_id),
    strategy_a_id VARCHAR REFERENCES strategies(strategy_id),
    strategy_b_id VARCHAR REFERENCES strategies(strategy_id),
    
    -- Correlation metrics
    signal_correlation REAL,           -- Correlation of signals
    return_correlation REAL,           -- Correlation of returns
    
    -- Diversification metrics
    diversification_ratio REAL,        -- Portfolio benefit of combining
    combined_sharpe REAL,              -- Sharpe if combined equally
    
    computed_at TIMESTAMP,
    
    UNIQUE(run_id, strategy_a_id, strategy_b_id),
    CHECK(strategy_a_id < strategy_b_id)  -- Avoid duplicates
);

-- =============================================
-- PERFORMANCE INDEXES
-- =============================================

-- Fast strategy lookup
CREATE INDEX idx_strategies_performance ON strategies(sharpe_ratio DESC, max_drawdown ASC);
CREATE INDEX idx_strategies_type_sharpe ON strategies(strategy_type, sharpe_ratio DESC);
CREATE INDEX idx_strategies_run ON strategies(run_id);
CREATE INDEX idx_strategies_profitable ON strategies(run_id) WHERE is_profitable = true;

-- Parameter analysis
CREATE INDEX idx_strategies_parameters ON strategies USING GIN (parameters);
CREATE INDEX idx_param_analysis_lookup ON parameter_analysis(run_id, strategy_type, parameter_name);

-- Regime performance
CREATE INDEX idx_regime_perf_strategy ON regime_performance(strategy_id, regime_score DESC);
CREATE INDEX idx_regime_perf_classifier ON regime_performance(classifier_id, regime_score DESC);

-- Event archives
CREATE INDEX idx_events_run_type ON event_archives(run_id, container_type);
CREATE INDEX idx_events_time_range ON event_archives(start_time, end_time);

-- Correlations
CREATE INDEX idx_correlations_run ON strategy_correlations(run_id, signal_correlation DESC);
```

---

## Interactive Interfaces

### 1. Raw SQL Interface (DuckDB CLI)

**Direct DuckDB access for power users:**

```bash
# Connect to workspace database
duckdb workspace/analytics.duckdb

# Interactive exploration
D .tables
D .schema strategies
D SELECT COUNT(*) FROM strategies WHERE sharpe_ratio > 1.5;
D SELECT * FROM 'signals/momentum/*.parquet' LIMIT 10;
```

### 2. ADMF SQL Mode

**Enhanced CLI with trading-specific helpers:**

```bash
# Start enhanced SQL mode
admf analytics sql --workspace 20250611_grid_search_SPY

# Meta commands
ADMF SQL> .strategies                    # Quick strategy overview
ADMF SQL> .classifiers                  # Classifier summary  
ADMF SQL> .signals                      # Available signal files
ADMF SQL> .performance momentum          # Performance by strategy type
ADMF SQL> .correlations --min 0.7       # High correlations

# Standard SQL
ADMF SQL> SELECT * FROM strategies WHERE sharpe_ratio > 2.0;
ADMF SQL> SELECT AVG(sharpe_ratio) FROM strategies GROUP BY strategy_type;
```

### 3. Query Templates

**Pre-built queries for common analysis:**

```python
# Built-in templates
templates = {
    'top_performers': """
        SELECT strategy_id, strategy_type, sharpe_ratio, max_drawdown, win_rate,
               signal_file_path
        FROM strategies 
        WHERE sharpe_ratio > {min_sharpe} 
          AND ABS(max_drawdown) < {max_drawdown}
        ORDER BY sharpe_ratio DESC 
        LIMIT {limit}
    """,
    
    'parameter_sweep': """
        SELECT 
            CAST(parameters->>{parameter} AS REAL) as param_value,
            COUNT(*) as strategy_count,
            AVG(sharpe_ratio) as avg_sharpe,
            STDDEV(sharpe_ratio) as sharpe_std,
            MAX(sharpe_ratio) as best_sharpe
        FROM strategies 
        WHERE strategy_type = '{strategy_type}'
        GROUP BY CAST(parameters->>{parameter} AS REAL)
        ORDER BY param_value
    """,
    
    'regime_winners': """
        SELECT 
            s.strategy_id,
            s.strategy_type,
            rp.regime,
            rp.regime_sharpe,
            rp.regime_time_pct
        FROM strategies s
        JOIN regime_performance rp ON s.strategy_id = rp.strategy_id
        WHERE rp.regime = '{regime}'
          AND rp.regime_sharpe > {min_sharpe}
        ORDER BY rp.regime_sharpe DESC
        LIMIT {limit}
    """,
    
    'signal_characteristics': """
        SELECT 
            strategy_type,
            AVG(signal_frequency) as avg_signal_freq,
            AVG(compression_ratio) as avg_compression,
            COUNT(*) as strategy_count
        FROM strategies
        GROUP BY strategy_type
        ORDER BY avg_signal_freq DESC
    """
}

# Usage
admf analytics template top_performers --min-sharpe 1.5 --max-drawdown 0.15 --limit 20
admf analytics template parameter_sweep --strategy-type momentum --parameter sma_period
```

### 4. Jupyter Integration

```python
# analytics_exploration.ipynb

import duckdb
import pandas as pd
import plotly.express as px
from pathlib import Path

# Connect to workspace
workspace = Path('workspaces/20250611_grid_search_SPY')
conn = duckdb.connect(str(workspace / 'analytics.duckdb'))

# Enable workspace context for Parquet files
conn.execute(f"SET workspace_root = '{workspace.absolute()}'")

# Interactive analysis
strategies = conn.execute("""
    SELECT strategy_type, parameters, sharpe_ratio, max_drawdown, signal_file_path
    FROM strategies 
    WHERE sharpe_ratio > 1.5
    ORDER BY sharpe_ratio DESC
    LIMIT 50
""").df()

# Visualize results
fig = px.scatter(strategies, 
                x='max_drawdown', 
                y='sharpe_ratio',
                color='strategy_type',
                hover_data=['strategy_id'])
fig.show()

# Load actual signal data for top performer
top_strategy = strategies.iloc[0]
signals = pd.read_parquet(workspace / top_strategy['signal_file_path'])
print(f"Signal count: {len(signals)}")
print(f"Signal frequency: {len(signals) / signals.attrs.get('total_bars', 1):.4f}")
```

---

## Performance & Scalability

### Benchmark Scenarios

| Scale | Strategies | SQL Query Time | Memory Usage | Storage Size |
|-------|------------|----------------|--------------|--------------|
| Small | 100        | <1ms          | 10MB         | 50MB         |
| Medium| 1,000      | 1-5ms         | 50MB         | 500MB        |
| Large | 5,000      | 5-20ms        | 200MB        | 2.5GB        |
| XLarge| 25,000     | 20-100ms      | 1GB          | 12GB         |

### Optimization Strategies

#### 1. **Indexing Strategy**
```sql
-- Primary performance indexes
CREATE INDEX idx_perf_lookup ON strategies(run_id, sharpe_ratio DESC) 
    WHERE sharpe_ratio > 1.0;

-- Parameter-specific indexes  
CREATE INDEX idx_momentum_sma ON strategies(strategy_type, (parameters->>'sma_period'))
    WHERE strategy_type = 'momentum';

-- Composite indexes for common queries
CREATE INDEX idx_profitable_strategies ON strategies(strategy_type, sharpe_ratio DESC, max_drawdown)
    WHERE is_profitable = true;
```

#### 2. **Partitioning for Large Runs**
```
signals/
├── date_partition=2023-01/
│   ├── strategy_type=momentum/
│   └── strategy_type=ma_crossover/
├── date_partition=2023-02/
│   └── strategy_type=momentum/
└── date_partition=2023-03/
```

#### 3. **Parallel Processing**
```python
# Parallel signal loading
def load_signals_parallel(strategy_files, n_workers=4):
    with ThreadPoolExecutor(max_workers=n_workers) as executor:
        futures = {
            executor.submit(pd.read_parquet, f): f 
            for f in strategy_files
        }
        results = {}
        for future in as_completed(futures):
            file_path = futures[future]
            results[file_path] = future.result()
    return results

# SQL identifies files to load in parallel
strategy_files = conn.execute("""
    SELECT signal_file_path FROM strategies 
    WHERE strategy_type = 'momentum' AND sharpe_ratio > 1.5
""").fetchall()

signals = load_signals_parallel([f[0] for f in strategy_files])
```

#### 4. **Caching Strategy**
```python
# Pre-compute expensive analytics
def precompute_analytics(run_id):
    # Parameter sensitivity
    conn.execute("""
        INSERT INTO parameter_analysis (run_id, strategy_type, parameter_name, ...)
        SELECT run_id, strategy_type, param_name, AVG(sharpe_ratio), ...
        FROM strategies_expanded_params
        WHERE run_id = ?
        GROUP BY run_id, strategy_type, param_name, param_value
    """, [run_id])
    
    # Strategy correlations
    conn.execute("""
        INSERT INTO strategy_correlations (run_id, strategy_a_id, strategy_b_id, signal_correlation)
        SELECT ... FROM signal_correlation_calculation(?)
    """, [run_id])
```

---

## Implementation Roadmap

### Phase 1: Foundation (Week 1-2)
- [ ] Create DuckDB schema
- [ ] Implement basic SQL backend  
- [ ] Convert existing JSON workspaces to SQL catalog
- [ ] Basic ADMF SQL CLI mode
- [ ] Core query templates

### Phase 2: Data Integration (Week 3-4)  
- [ ] Convert CSV market data to Parquet
- [ ] Migrate signal storage to Parquet format
- [ ] Implement sparse storage utilities
- [ ] Event archive integration
- [ ] Classifier state storage

### Phase 3: Analytics Engine (Week 5-6)
- [ ] Pre-computed analytics pipeline
- [ ] Parameter sensitivity analysis
- [ ] Strategy correlation calculation
- [ ] Regime performance analysis
- [ ] Performance attribution

### Phase 4: Advanced Features (Week 7-8)
- [ ] Jupyter notebook integration
- [ ] Visualization pipeline
- [ ] Export/import utilities
- [ ] Performance optimization
- [ ] Large-scale testing (5000+ strategies)

### Phase 5: Production Features (Week 9-10)
- [ ] Automated analytics workflow
- [ ] Alerting and monitoring
- [ ] Multi-workspace analysis
- [ ] Advanced visualizations
- [ ] Documentation and training

---

## Migration Strategy

### Existing Workspace Migration

```python
# Migration utility
class WorkspaceMigrator:
    def migrate_to_sql(self, old_workspace_path, new_workspace_path):
        # 1. Create new SQL database
        self.init_sql_database(new_workspace_path)
        
        # 2. Analyze old workspace structure
        workspace_info = self.analyze_old_workspace(old_workspace_path)
        
        # 3. Migrate strategy files
        self.migrate_strategy_files(workspace_info, new_workspace_path)
        
        # 4. Populate SQL catalog
        self.populate_sql_catalog(workspace_info, new_workspace_path)
        
        # 5. Pre-compute analytics
        self.precompute_analytics(workspace_info['run_id'])

# CLI command
admf analytics migrate-to-sql \
    --source ./workspaces/fc4bb91c-2cea-441b-85e4-10d83a0e1580 \
    --destination ./workspaces/20250611_grid_search_SPY_migrated
```

### Backward Compatibility

```python
# Dual-mode operation during transition
class AnalyticsBackend:
    def __init__(self, workspace_path):
        self.workspace_path = Path(workspace_path)
        
        # Try SQL first, fall back to JSON
        sql_db = self.workspace_path / 'analytics.duckdb'
        if sql_db.exists():
            self.backend = SQLBackend(sql_db)
        else:
            self.backend = JSONBackend(workspace_path)
    
    def scan_strategies(self, **filters):
        return self.backend.scan_strategies(**filters)
```

---

## Use Case Examples

### 1. Finding Top Performing Strategies

```sql
-- Interactive SQL exploration
SELECT 
    strategy_id,
    strategy_type,
    parameters->>'sma_period' as sma_period,
    parameters->>'rsi_threshold' as rsi_threshold,
    sharpe_ratio,
    max_drawdown,
    win_rate,
    signal_changes
FROM strategies 
WHERE strategy_type = 'momentum'
  AND sharpe_ratio > 1.5
  AND ABS(max_drawdown) < 0.15
  AND total_trades > 50
ORDER BY sharpe_ratio DESC
LIMIT 20;
```

### 2. Parameter Sensitivity Analysis

```sql
-- Momentum strategy SMA period sensitivity
SELECT 
    CAST(parameters->>'sma_period' AS INT) as sma_period,
    COUNT(*) as strategy_count,
    AVG(sharpe_ratio) as avg_sharpe,
    STDDEV(sharpe_ratio) as sharpe_volatility,
    MIN(sharpe_ratio) as worst_sharpe,
    MAX(sharpe_ratio) as best_sharpe,
    AVG(total_trades) as avg_trades
FROM strategies 
WHERE strategy_type = 'momentum'
  AND parameters->>'sma_period' IS NOT NULL
GROUP BY CAST(parameters->>'sma_period' AS INT)
ORDER BY sma_period;
```

### 3. Cross-Strategy Correlation Analysis

```sql
-- Find uncorrelated high-performing strategies
WITH top_strategies AS (
    SELECT strategy_id, sharpe_ratio
    FROM strategies 
    WHERE sharpe_ratio > 1.2
    ORDER BY sharpe_ratio DESC
    LIMIT 50
),
low_correlations AS (
    SELECT 
        sc.strategy_a_id,
        sc.strategy_b_id,
        sc.signal_correlation,
        ts1.sharpe_ratio as sharpe_a,
        ts2.sharpe_ratio as sharpe_b
    FROM strategy_correlations sc
    JOIN top_strategies ts1 ON sc.strategy_a_id = ts1.strategy_id
    JOIN top_strategies ts2 ON sc.strategy_b_id = ts2.strategy_id
    WHERE ABS(sc.signal_correlation) < 0.3
)
SELECT * FROM low_correlations
ORDER BY (sharpe_a + sharpe_b) DESC
LIMIT 20;
```

### 4. Regime-Aware Performance Analysis

```sql
-- Best strategies by market regime
SELECT 
    s.strategy_type,
    s.strategy_id,
    rp.regime,
    rp.regime_sharpe,
    rp.regime_time_pct,
    rp.regime_weighted_return
FROM strategies s
JOIN regime_performance rp ON s.strategy_id = rp.strategy_id
WHERE rp.regime IN ('TRENDING', 'VOLATILE', 'NEUTRAL')
  AND rp.regime_sharpe > 1.0
ORDER BY rp.regime, rp.regime_sharpe DESC;
```

### 5. Signal Characteristics Analysis

```sql
-- Compare signal generation patterns across strategy types
SELECT 
    strategy_type,
    COUNT(*) as strategy_count,
    AVG(signal_frequency) as avg_signal_freq,
    STDDEV(signal_frequency) as signal_freq_std,
    AVG(compression_ratio) as avg_compression,
    AVG(total_trades) as avg_trades,
    AVG(sharpe_ratio) as avg_sharpe
FROM strategies
GROUP BY strategy_type
ORDER BY avg_sharpe DESC;
```

### 6. Loading Signal Data for Detailed Analysis

```python
# After SQL identifies interesting strategies
top_momentum = conn.execute("""
    SELECT strategy_id, signal_file_path, parameters
    FROM strategies 
    WHERE strategy_type = 'momentum' 
      AND sharpe_ratio > 1.8
    ORDER BY sharpe_ratio DESC
    LIMIT 5
""").df()

# Load actual signal data
for _, row in top_momentum.iterrows():
    strategy_id = row['strategy_id']
    file_path = workspace / row['signal_file_path']
    
    # Load sparse signals
    signals = pd.read_parquet(file_path)
    
    # Analyze signal timing
    print(f"\n{strategy_id}:")
    print(f"  Total signals: {len(signals)}")
    print(f"  Signal frequency: {len(signals) / signals.attrs['total_bars']:.4f}")
    print(f"  Average signal strength: {signals['signal'].abs().mean():.3f}")
    
    # Signal distribution by time of day
    signals['hour'] = signals['timestamp'].dt.hour
    hourly_signals = signals.groupby('hour').size()
    print(f"  Most active hour: {hourly_signals.idxmax()} ({hourly_signals.max()} signals)")
```

---

## Future Extensions

### 1. Advanced Analytics

```sql
-- Machine learning model performance tracking
CREATE TABLE ml_model_performance (
    model_id VARCHAR PRIMARY KEY,
    strategy_id VARCHAR REFERENCES strategies(strategy_id),
    model_type VARCHAR,                 -- 'random_forest', 'lstm', 'transformer'
    feature_set JSONB,
    hyperparameters JSONB,
    
    -- Training metrics
    training_accuracy REAL,
    validation_accuracy REAL,
    training_loss REAL,
    validation_loss REAL,
    
    -- Live performance
    live_accuracy REAL,
    model_drift_score REAL,
    
    -- File references
    model_file_path VARCHAR,
    prediction_file_path VARCHAR
);
```

### 2. Multi-Asset Analysis

```sql
-- Cross-asset strategy performance
CREATE TABLE multi_asset_strategies (
    strategy_id VARCHAR PRIMARY KEY,
    asset_universe VARCHAR[],           -- ['SPY', 'QQQ', 'IWM', 'GLD']
    correlation_matrix JSONB,
    
    -- Portfolio-level metrics
    portfolio_sharpe REAL,
    diversification_ratio REAL,
    max_component_weight REAL,
    
    -- Risk metrics
    portfolio_var REAL,
    portfolio_cvar REAL,
    max_sector_exposure REAL
);
```

### 3. Real-Time Monitoring

```sql
-- Live strategy monitoring
CREATE TABLE live_strategy_monitoring (
    strategy_id VARCHAR REFERENCES strategies(strategy_id),
    timestamp TIMESTAMP,
    
    -- Current positions
    current_signals JSONB,
    active_positions JSONB,
    
    -- Performance tracking
    live_pnl REAL,
    live_sharpe REAL,
    live_drawdown REAL,
    
    -- Alerts
    performance_alert BOOLEAN,
    risk_alert BOOLEAN,
    correlation_alert BOOLEAN
);
```

### 4. Ensemble Optimization

```sql
-- Strategy ensemble definitions
CREATE TABLE strategy_ensembles (
    ensemble_id VARCHAR PRIMARY KEY,
    ensemble_name VARCHAR,
    strategy_weights JSONB,             -- {strategy_id: weight, ...}
    rebalance_frequency VARCHAR,        -- 'daily', 'weekly', 'on_regime_change'
    
    -- Constraints
    max_strategy_weight REAL,
    max_correlation REAL,
    min_strategies INTEGER,
    max_strategies INTEGER,
    
    -- Performance
    ensemble_sharpe REAL,
    ensemble_max_drawdown REAL,
    diversification_benefit REAL
);
```

---

## Conclusion

This SQL + Parquet hybrid architecture provides:

1. **Scalability**: Handles 5,000+ strategies efficiently
2. **Flexibility**: Raw SQL access for power users, templates for common use cases  
3. **Performance**: Indexed queries, sparse storage, parallel processing
4. **Maintainability**: Clean separation between metadata and data storage
5. **Future-Proof**: Foundation for advanced analytics, ML integration, real-time monitoring

The architecture starts simple (SQL catalog + Parquet files) but scales to sophisticated multi-asset, multi-timeframe, ensemble-optimized trading system analysis.

**Key Benefits:**
- **Query Performance**: Sub-second response for most analytics queries
- **Storage Efficiency**: 90%+ compression for sparse signal data
- **Development Speed**: SQL enables rapid prototyping of new analytics
- **Integration Ready**: Foundation for Jupyter, web dashboards, APIs
- **Production Scale**: Tested architecture patterns from financial industry

This foundation enables both immediate tactical analysis and long-term strategic development of the ADMF-PC analytics capabilities.